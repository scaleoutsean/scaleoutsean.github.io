<!doctype html>
<html lang="en">
  <head>
        <meta charset="UTF-8">
    <meta http-equiv="content-language" content="en">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="googlebot" content="noarchive">
    <meta name="googlebot" content="max-snippet:60">
    <meta name="googlebot" content="max-image-preview:small">
    <meta name="bingbot" content="noarchive">
    <meta name="bingbot" content="max-snippet:60">
    <meta name="bingbot" content="max-image-preview:small">
    <meta name="robots" content="noarchive">
    <meta name="robots" content="max-snippet:60">
    <meta name="google-site-verification" content="F6q7vIwQ2G0j8tk-KL9rAXOMLXMDMMUkEz4fRs1Nnnc" />
    <title>
        
            ZFS with NetApp E-Series | Acting Technologist
      
    </title>
    <meta name="description" content="
     Practical use of NetApp E-Series-backed ZFS from Incus/LXD
     ">

    <!-- LINK TO ATOM FEED FOR SEO -->
    <link rel="alternate" type="application/atom+xml" href="https://scaleoutsean.github.io/feed.xml" />

    <!-- FAVICON -->
    <link rel="icon" href="/assets/favicon.ico" type="image/x-icon">
    <link rel="shortcut icon" href="/assets/favicon.ico" type="image/x-icon">

    <!-- CSS -->
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="stylesheet" href="/assets/css/syntax.css">

    <!-- Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css">

    <link href="/_pagefind/pagefind-ui.css" rel="stylesheet">
    <script src="/_pagefind/pagefind-ui.js" type="text/javascript"></script>
    <div id="search"></div>
    <script src="https://cdn.counter.dev/script.js" data-id="83dc700a-b821-4e57-9425-02b8336a9456" data-utcoffset="0"></script>
    <script>
        window.addEventListener('DOMContentLoaded', (event) => {
            new PagefindUI({ element: "#search" });
        });
    </script>

    <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>ZFS with NetApp E-Series | Acting Technologist</title>
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="ZFS with NetApp E-Series" />
<meta name="author" content="scaleoutSean" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Practical use of NetApp E-Series-backed ZFS from Incus/LXD" />
<meta property="og:description" content="Practical use of NetApp E-Series-backed ZFS from Incus/LXD" />
<link rel="canonical" href="https://scaleoutsean.github.io/2024/02/28/incus-zfs-netapp-eseries.html" />
<meta property="og:url" content="https://scaleoutsean.github.io/2024/02/28/incus-zfs-netapp-eseries.html" />
<meta property="og:site_name" content="Acting Technologist" />
<meta property="og:image" content="https://scaleoutsean.github.io/assets/images/incus-zfs-eseries-01-controller-pool-groups-volumes-hosts.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-02-28T00:00:00+08:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"scaleoutSean"},"description":"Practical use of NetApp E-Series-backed ZFS from Incus/LXD","@type":"BlogPosting","headline":"ZFS with NetApp E-Series","dateModified":"2024-02-28T00:00:00+08:00","datePublished":"2024-02-28T00:00:00+08:00","image":"https://scaleoutsean.github.io/assets/images/incus-zfs-eseries-01-controller-pool-groups-volumes-hosts.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://scaleoutsean.github.io/2024/02/28/incus-zfs-netapp-eseries.html"},"url":"https://scaleoutsean.github.io/2024/02/28/incus-zfs-netapp-eseries.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  </head>

<body>

  <div class="container">
    <header id="header">
	<div id = site_title>
		<a href="https://scaleoutsean.github.io/">
			<h2 style="font-size:1.8em;">Acting Technologist</h2>
		</a>
		
		<h3>
			civilizations are created by individuals
		</h3>
		
	</div>

	<div id="subheader">
		
		<nav class="pages">
<a href="/about.html">About</a>

<a href="/archive.html">Archive</a>

<a href="/categories/">Categories</a>

<a href="/projects.html">Projects</a>
</nav>
		
		
		<nav class="social">
			
  
    <a href="https://www.github.com/scaleoutsean" target="_blank" id="github"><i class="fab fa-github" aria-hidden="true"></i></a>
  
   

  
    <a href="https://twitter.com/scaleoutsean" target="_blank" id="twitter"><i class="fab fa-twitter" aria-hidden="true"></i></a>
  
   

  
  
    <a href="/feed.xml" target="_blank" id="rss"><i class="fas fa-rss" aria-hidden="true"></i></a>
   

		</nav>
		
	</div>
</header>


    <div class="post-container">
      <article id = "post">
        <h1 id = "post-title">ZFS with NetApp E-Series</h1>

  
  <div class = "post-info">
    <span>
        <i class="far fa-calendar" aria-hidden="true"></i> <span>28 Feb 2024</span> - <i class="far fa-clock"></i> 


  
  
    5 minute read
  

    </span>
  </div>
  
        <ul>
  <li><a href="#introduction">Introduction</a></li>
  <li><a href="#what-is-e-series-anyway">What is E-Series anyway</a></li>
  <li><a href="#easy-storage-management">Easy storage management</a></li>
  <li><a href="#conclusion">Conclusion</a></li>
</ul>

<h2 id="introduction">Introduction</h2>

<p>This posts is for Incus (or LXD) or ZFS users interested in offloading some of the work to NetApp E-Series.</p>

<p>I’ve written about the reasons why one may want to do that in the post on <a href="/2024/02/26/zfs-deduplication-netapp-eseries.html">ZFS deduplication</a>.</p>

<p>This posts gives an example of a setup with Incus or LXD hosting VMs or containers on a ZFS pool that resides on a E-Series disk array.</p>

<h2 id="what-is-e-series-anyway">What is E-Series anyway</h2>

<p>It’s a NetApp product line of classic dual-controller storage arrays.</p>

<p>The <a href="/2024/02/26/zfs-deduplication-netapp-eseries.html">deduplication post</a> explains some benefits of not using JBOD with ZFS, so I’ll skip that part.</p>

<p>We are using a 2U shelf with dual controllers and 24 disks.</p>
<ul>
  <li>Some disks are allocated to a “pool” (<a href="/2022/09/12/new-ddp-and-e-series-santricity-web-restful-api.html">DDP</a> which is similar to a multi-disk zpool). These can be very wide (e.g. 100s of disks) and on top of a DDP we create RAID 1 or 6-type LUNs for ZFS or other hosts</li>
  <li>Some disks are grouped in traditional RAID (0/1/10/5/6). RAID0 is not protected, so it doesn’t make sense to stripe it - single-disk RAID0 would usually have just 1 LUN on top of each and be presented to ZFS hosts verbatim. Protected RAID always has more than one disk, so it makes sense to create multiple LUNs for one or more ZFS hosts</li>
  <li>This screenshot shows 1 DDP (pool), 6 volume groups, with a total of 11 volumes (LUNs)</li>
</ul>

<p><img src="/assets/images/incus-zfs-eseries-01-controller-pool-groups-volumes-hosts.png" alt="" /></p>

<p>The DDP has 16 disks an can be grown in small increments of a single disk. On it, stripes in RAID10 LUNs are written to random 10 disks at once (128kB each strip) and the same goes for RAID6 LUNs.</p>

<p>The first RAID 0 (sean_raid0) made of two disks is unusual (we’d commonly create RAID 1 out of two disks), but the point is RAID 1 or 10 can be used as the base for a any number of LUNs for whatever purposes (cache, etc.) without having to dedicate a physical disk to each LUN.</p>

<p>RAID 0 named <code class="language-plaintext highlighter-rouge">Z(number)</code> are individual disks as RAID 0 - same as JBOD. You can present them to hosts that are happy with that granularity (1 SSD) and protection (none).</p>

<p><img src="/assets/images/incus-zfs-eseries-02-pools.png" alt="" /></p>

<p>On this array I’ve created a bunch of LUNs (11), most of them on the pool (sean_ddp). We’ll use just one protected LUN (ddp_r1_04) which is a RAID 1-style volume on sean_ddp.</p>

<p><img src="/assets/images/incus-zfs-eseries-03-ddp-lun.png" alt="" /></p>

<p>Incus (or LXD) can create a protected single volume zpool on that disk. We see it in action here. Normally - with JBOD - you’d use a bunch of disks but here we use just one LUN of arbitrary size (here: 200 GiB), stored in hundreds of RAID 10-style stripes on the first DDP. If you move from another platform or want to allocate LUNs, rather than a lot of physical disks, to different hosts or groups of hosts, this is very convenient.</p>

<p><img src="/assets/images/incus-zfs-eseries-04-zfs-pool.png" alt="" /></p>

<p>As you create instances - whether it’s VMs or containers - ZFS works as it normally does. But you don’t have to deal with physical media protection, rebuilds, and security - E-Series does that for you.</p>

<p><img src="/assets/images/incus-zfs-eseries-05-zfs-instance-create.png" alt="" /></p>

<p>ZFS snapshots work fine, of course, and take a fraction of a second. By the way - when you’re upgrading Incus (or LXD) you can take a short-lived snapshot of the E-Series volume ddp_r1_04, just in the case upgrade fails. If you have multiple disks in your Incus pool, you can snapshot them as a group using group snapshots. Anyway, most of the time you’d use ZFS snapshots that are accessible to Incus users, rather than storage administrators.</p>

<p><img src="/assets/images/incus-zfs-eseries-06-zfs-snapshot-create.png" alt="" /></p>

<p>A snapshot has been created.</p>

<p><img src="/assets/images/incus-zfs-eseries-07-zfs-snapshot-restore.png" alt="" /></p>

<p>ZFS volumes also look as usual, with one snapshot visible in the column Snapshots.</p>

<p><img src="/assets/images/incus-zfs-eseries-08-zfs-pool-overview.png" alt="" /></p>

<p>Host view is as you might expect from Linux - DevMapper multi-pathing has been configured, but ddp_r1_04 is “missing”. Why? It’s created and used by Incus, not by the host. When initializing zpool for Incus we told Incus to use that device (/dev/mapper/DEVID) and that was it.</p>

<p><img src="/assets/images/incus-zfs-eseries-09-zfs-overview.png" alt="" /></p>

<p>Above you can see that other LUNs from sean_ddp (ddp_r1_01 and ddp_r1_02, for example) are exposed to host and used for other filesystems (XFS, for example). (There’s another “host-side” zpool created on this system - it’s called <code class="language-plaintext highlighter-rouge">default</code> and mounted at <code class="language-plaintext highlighter-rouge">/default</code>, but it’s not used by Incus).</p>

<p>This particular host is connected to E-Series with direct-attach Infiniband (and uses iSER), but iSCSI, FC and other protocols are available. With iSCSI, as there are 4 x 25GigE ports per controller, so you can connect up to 4 ZFS hosts (2 HA pairs) to an array without having dedicated storage switches.</p>

<p>Compression, when enabled and used on ZFS with compressible data, can save capacity and lower flash disk wear.</p>

<p><img src="/assets/images/incus-zfs-eseries-10-zfs-compression.png" alt="" /></p>

<p>If you’re also interested in deduplication, please see the ZFS deduplication post linked at the top.</p>

<p>For general VMs with non-compressed data, you may save 30-60% that way (that is, 1.50x to 2.00x is common).</p>

<h2 id="easy-storage-management">Easy storage management</h2>

<p>From Incus, all you need to “know” about storage is you can set quotas, create or restore snapshots.</p>

<p>If Incus pool starts running low on capacity, we’d simply create another zpool from one of other disks exposed to the host and add it to Incus.</p>

<p>Because you can create LUNs of arbitrary sizes, you can granularly provision zpools to different Incus projects, so that each team or department has own zpool or zpools.</p>

<p>Because zpool is already protected by RAID 10 or RAID 6 on E-Series LUNs that live on DDP pools, there’s no need to create complex configurations on Incus. While one could certainly create volumes on traditional RAID disk groups or even give LUNs made of an entire single-disk RAID 0 to Incus, but those seem like niche use cases as ZFS administrator would have to protect them by host-side replication or ZRAID.</p>

<p>My primary choices would be:</p>
<ul>
  <li>general use LUNs for ZFS: RAID 6 LUNs on DDP</li>
  <li>faster general use LUNs for ZFS: RAID 10 LUNs on DDP</li>
  <li>fastest, specialty LUNs for ZFS (ARC, etc): LUNs on classic RAID 10 disk groups</li>
</ul>

<h2 id="conclusion">Conclusion</h2>

<p>If you use multiple filesystems or don’t want to create large ZFS configurations, using protected storage saves you effort and time.</p>

<p>Whereas experimenting with production-grade storage may take you multiple servers with 10-20 internal disks each, E-Series lets you start with mere gigabytes and offload the hard part of data protection from ZFS to E-Series while at the same time other filesystems and applications (Windows, VMware, etc.) can keep running fine using the same E-Series array.</p>

      </article>
    </div>
    
      <div class="categories">
    <span><p>Categories:</p>
    
    
      <a href="
      /categories/#storage">storage</a>
       
    
  </span>
</div>
    

  
    <div>
      <h3>Related Posts</h3>
      <ul>
      
        <li><a href="/2022/09/02/lxd-containers-vms-on-beegfs.html">LXD containers and VMs on BeeGFS with NetApp E-Series</a></li>
      
        <li><a href="/2024/02/26/zfs-deduplication-netapp-eseries.html">ZFS deduplication with NetApp E-Series</a></li>
      
        <li><a href="/2023/07/28/thanos-with-minio-eseries-or-ontap-s3.html">Thanos with different S3 backends - StorageGRID, ONTAP S3 and MinIO on E-Series</a></li>
      
        <li><a href="/2022/08/09/nomad-beegfs-minio-s3.html">Simple S3 service endpoint for BeeGFS using Hashicorp Nomad and stand-alone MinIO</a></li>
      
        <li><a href="/2024/02/29/ubuntu-2404-lts-with-netapp-solidfire.html">Ubuntu 24.04 LTS with NetApp SolidFire</a></li>
      
      </ul>
    </div>
  

    
  </div><footer class= "footer">
    <p>2025-06-28 17:29 </p>
    <p>Copyright © 2025 scaleoutSean. Content is released under the CC BY license. Design: Alessio Franceschi</p>
</footer>

</body>
</html>
