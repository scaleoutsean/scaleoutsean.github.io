<!doctype html>
<html lang="en">
  <head>
        <meta charset="UTF-8">
    <meta http-equiv="content-language" content="en">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="googlebot" content="noarchive">
    <meta name="googlebot" content="max-snippet:60">
    <meta name="googlebot" content="max-image-preview:small">
    <meta name="bingbot" content="noarchive">
    <meta name="bingbot" content="max-snippet:60">
    <meta name="bingbot" content="max-image-preview:small">
    <meta name="robots" content="noarchive">
    <meta name="robots" content="max-snippet:60">
    <meta name="google-site-verification" content="F6q7vIwQ2G0j8tk-KL9rAXOMLXMDMMUkEz4fRs1Nnnc" />
    <title>
        
            Velero v1.13 metadata, hooks with NetApp Trident v24.02 | Acting Technologist
      
    </title>
    <meta name="description" content="
     A look at the backup job details with Velero v1.13 CSI and NetApp Trident with SolidFire
     ">

    <!-- LINK TO ATOM FEED FOR SEO -->
    <link rel="alternate" type="application/atom+xml" href="https://scaleoutsean.github.io/feed.xml" />

    <!-- FAVICON -->
    <link rel="icon" href="/assets/favicon.ico" type="image/x-icon">
    <link rel="shortcut icon" href="/assets/favicon.ico" type="image/x-icon">

    <!-- CSS -->
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="stylesheet" href="/assets/css/syntax.css">

    <!-- Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css">

    <link href="/_pagefind/pagefind-ui.css" rel="stylesheet">
    <script src="/_pagefind/pagefind-ui.js" type="text/javascript"></script>
    <div id="search"></div>
    <script src="https://cdn.counter.dev/script.js" data-id="83dc700a-b821-4e57-9425-02b8336a9456" data-utcoffset="0"></script>
    <script>
        window.addEventListener('DOMContentLoaded', (event) => {
            new PagefindUI({ element: "#search" });
        });
    </script>

    <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Velero v1.13 metadata, hooks with NetApp Trident v24.02 | Acting Technologist</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Velero v1.13 metadata, hooks with NetApp Trident v24.02" />
<meta name="author" content="scaleoutSean" />
<meta property="og:locale" content="en" />
<meta name="description" content="A look at the backup job details with Velero v1.13 CSI and NetApp Trident with SolidFire" />
<meta property="og:description" content="A look at the backup job details with Velero v1.13 CSI and NetApp Trident with SolidFire" />
<link rel="canonical" href="https://scaleoutsean.github.io/2024/03/22/velero-trident-backup-job-details.html" />
<meta property="og:url" content="https://scaleoutsean.github.io/2024/03/22/velero-trident-backup-job-details.html" />
<meta property="og:site_name" content="Acting Technologist" />
<meta property="og:image" content="https://scaleoutsean.github.io/assets/images/velero-trident-solidfire-description-00.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-03-22T00:00:00+08:00" />
<script type="application/ld+json">
{"headline":"Velero v1.13 metadata, hooks with NetApp Trident v24.02","dateModified":"2024-03-22T00:00:00+08:00","datePublished":"2024-03-22T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://scaleoutsean.github.io/2024/03/22/velero-trident-backup-job-details.html"},"image":"https://scaleoutsean.github.io/assets/images/velero-trident-solidfire-description-00.png","author":{"@type":"Person","name":"scaleoutSean"},"@type":"BlogPosting","url":"https://scaleoutsean.github.io/2024/03/22/velero-trident-backup-job-details.html","description":"A look at the backup job details with Velero v1.13 CSI and NetApp Trident with SolidFire","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  </head>

<body>

  <div class="container">
    <header id="header">
	<div id = site_title>
		<a href="https://scaleoutsean.github.io/">
			<h2 style="font-size:1.8em;">Acting Technologist</h2>
		</a>
		
		<h3>
			civilizations are created by individuals
		</h3>
		
	</div>

	<div id="subheader">
		
		<nav class="pages">
<a href="/about.html">About</a>

<a href="/archive.html">Archive</a>

<a href="/categories/">Categories</a>

<a href="/projects.html">Projects</a>
</nav>
		
		
		<nav class="social">
			
  
    <a href="https://www.github.com/scaleoutsean" target="_blank" id="github"><i class="fab fa-github" aria-hidden="true"></i></a>
  
   

  
    <a href="https://twitter.com/scaleoutsean" target="_blank" id="twitter"><i class="fab fa-twitter" aria-hidden="true"></i></a>
  
   

  
  
    <a href="/feed.xml" target="_blank" id="rss"><i class="fas fa-rss" aria-hidden="true"></i></a>
   

		</nav>
		
	</div>
</header>


    <div class="post-container">
      <article id = "post">
        <h1 id = "post-title">Velero v1.13 metadata, hooks with NetApp Trident v24.02</h1>

  
  <div class = "post-info">
    <span>
        <i class="far fa-calendar" aria-hidden="true"></i> <span>22 Mar 2024</span> - <i class="far fa-clock"></i> 


  
  
    22 minute read
  

    </span>
  </div>
  
        <ul>
  <li><a href="#introduction">Introduction</a></li>
  <li><a href="#backup-detail-enhancements-in-velero-v113">Backup detail enhancements in Velero v1.13</a>
    <ul>
      <li><a href="#mapping-velero-jobs-to-solidfire-volume-and-snapshot-objects">Mapping Velero jobs to SolidFire volume and snapshot objects</a></li>
    </ul>
  </li>
  <li><a href="#how-many-velero-instances-for-two-sites-with-a-solidfire-cluster-on-each">How many Velero instances for two sites with a SolidFire cluster on each</a></li>
  <li><a href="#conclusion">Conclusion</a></li>
  <li><a href="#appendix-a---configuration-details">Appendix A - configuration details</a></li>
  <li><a href="#appendix-b---backup-and-details">Appendix B - backup and details</a></li>
  <li><a href="#appendix-c---restore-and-delete">Appendix C - restore and delete</a></li>
  <li><a href="#appendix-d---using-velero-hooks">Appendix D - using Velero hooks</a></li>
</ul>

<h2 id="introduction">Introduction</h2>

<p>I’ve been blogging on Velero with NetApp Trident for years (more than three, for now) ever <a href="/2021/02/08/use-velero-with-netapp-solidfire-and-trident-csi.html">since Velero v1.5.3</a>. But because I don’t use it day to day, I am not very good at it.</p>

<p>Still -or because of that - I keep an eye on what’s going on with Velero.</p>

<p>Six-seven weeks ago Velero <a href="https://github.com/vmware-tanzu/velero/releases/tag/v1.13.0">v1.13</a> came out so I wanted to take some notes on its changes.</p>

<p>I’ll focus on one particular change that’s most relevant to me but there are other nice improvements so I encourage you to check the release page.</p>

<h2 id="backup-detail-enhancements-in-velero-v113">Backup detail enhancements in Velero v1.13</h2>

<p>Velero backup description now supports showing information of CSI snapshot data movements, which was not supported in v1.12 and earlier.</p>

<p>When v1.12 came out I blogged about Velero’s CSI snapshot data movement <a href="/2023/09/15/velero-csi-snapshot-data-movement-with-netapp-solidfire.html">here</a> and I think it’s an underestimated feature, especially for SolidFire users (because it is more limited due to the maximum number of snapshots being 32, whereas ONTAP can take and retain thousands).</p>

<p>In v1.13 those and other details are available in backup job details, making data management easier and more transparent.</p>

<p>If we run <code class="language-plaintext highlighter-rouge">velero backup</code> and run then <code class="language-plaintext highlighter-rouge">backup describe</code> on the backup we just created, backup job details describes information for all the volumes and snapshots included in the backup of various backup types, such as whether it’s a native (SolidFire) snapshot, CSI snapshot or CSI snapshot data movement.</p>

<p>Originally I had the entire flow in the main body of this post, but I moved that to appendices because there’s too much text in it.</p>

<p>This screenshot - repeated in Appendix B as well - is hopefully enough to illustrate the main details.</p>

<p><img src="/assets/images/velero-trident-solidfire-description-00.png" alt="Mapping from Velero to Trident to SolidFire" /></p>

<h3 id="mapping-velero-jobs-to-solidfire-volume-and-snapshot-objects">Mapping Velero jobs to SolidFire volume and snapshot objects</h3>

<p>Going further “down” the stack, we get to SolidFire where PVC names are decided by Trident.</p>

<p>Snapshot names are <code class="language-plaintext highlighter-rouge">snapshot-ID</code> and tied to volume names, but snapshot UUIDs are completely random. Still, the SolidFire API is very easy to use, so once we get a hold of one of those, the rest are very easy to find.</p>

<p>I wrote about that in various posts, but I’ll highlight two.</p>

<p><strong>OpenStack Xena with Cinder CSI</strong> talks about the <a href="/2022/03/02/openstack-solidfire-part-2.html#appendix-a---map-kubernetes-pvc-and-pv-to-openstack-volume-name-to-solidfire-volume-name">mapping</a> between OpenStack and SolidFire storage objects. Note that we aren’t using NetApp Trident here, but rather Cinder CSI and it works great!</p>

<p><a href="/2021/03/20/kubernetes-solidfire-failover-failback.html#solidfire-volume-names-and-ids">SolidFire site failover and failback with Trident clients</a> explains how things get tricky during storage cluster failover and failback because volume names can’t be human friendly, and the reason is Trident isn’t conductive to SolidFire cluster failover.</p>

<p>Anyway, that’s not a problem as long as you decide to ignore volume names and decide to automate. It’s not easy to give up the idea of human-friendly names, but giving up on it gives you the energy to move on and simply automate for scale and care-free storage cluster failover.</p>

<h2 id="how-many-velero-instances-for-two-sites-with-a-solidfire-cluster-on-each">How many Velero instances for two sites with a SolidFire cluster on each</h2>

<p>I don’t know. But I wonder about that, because I mentioned that due to the way Trident works (at least with <code class="language-plaintext highlighter-rouge">solidfire-san</code>), the easiest (and still not easy) way to fail over to another site involves re-installing Trident and importing data from a configuration file.</p>

<p>I did that once in PowerShell, and it worked great (see the Trident failover/failback post - it took a minute to failover from one SolidFire to another, and back).</p>

<p>But now I wonder how Velero backup details can help us restore S3 backups at a different site. If we purely rely on namespaces, then it’s probably easy as we can ignore storage names.</p>

<p>But, as I mentioned in those other posts, you may have large volumes that are replicated asynchronously by SolidFire.</p>

<p>Say you have an 8 TB volume that’s replicated that way, by copying over SolidFire snapshot deltas - one every 5 minutes.</p>

<p>Even if you use Velero to backup four times a day, you may still have hours of data loss if you only rely on Velero backups.</p>

<p>In that case, if asynchronous replication is configured on SolidFire, it’s better to automate that failover as I mentioned in the failover/failback article, and separately, test one (or two, if you have one for each site) Velero instances in such cases to make sure your view of Velero backups does not lose relevance regardless of how sites change, or which SolidFire cluster you use.</p>

<p>Figuring that out would also help you with “DR to the cloud”, or “DR to on-premises” if you want to repatriate some Kubernetes workloads or use on-premises SolidFire for Dev/Test.</p>

<p>Velero backup metadata is stored in Velero’s S3 bucket (if that’s your backup destination), Kubernetes and Trident information is stored on each Kubernetes cluster, and SolidFire information is stored just on SolidFire. If you want to get a big picture of what’s going on, it may be hard to assess or visualize all that information.</p>

<p>Perhaps it would be useful to drive Velero, Trident and SolidFire automation from a job scheduler, and send responses to something like Elasticsearch, so that queries can be created for volume history, or that different data properties can be queried and tracked over time. For SolidFire I’ve done that many times, including <a href="/2021/10/18/solidfire-syslog-filebeat-logstash-elk-stack.html">here</a> for API log.</p>

<p>We could use SolidFire Demo VMs to prototype that - we’d need just 4 VMs (2 for Kubernetes clusters, and 2 SolidFire VMs to for two storage clusters).</p>

<h2 id="conclusion">Conclusion</h2>

<p>My objective was to examine the new level of detail related to PVCs, PVS in Velero v1.13.</p>

<p>In the very first CSI post with Velero v1.5.3 from early 2021 I see:</p>

<ul>
  <li>Backup Volumes &gt; CSI Snapshots: Snapshot Content Name wasn’t included</li>
  <li>Backup Item Operations: this section did not exist</li>
</ul>

<p>The level of detail is now very good and makes it easy to feed those details into a central location where it can be cross referenced with Trident and SolidFire logs or API audit log, for example.</p>

<p>Even without additional work, just knowing the entire “lineage” of a backup, from SolidFire volume over PV/PVC to CSI snapshot is great - especially when snapshot names that exist in Velero are not visible in <code class="language-plaintext highlighter-rouge">get volumesnapshot</code> output (which seems to be how it works, to prevent accidental deletion).</p>

<p>If an applications is already gone, being able to trace these things is even more important because there’s nothing to see on storage or Kubernetes - at that point all you have may be what Velero backup details can give you.</p>

<p>Or, if you collected logs and data from Kubernetes and SolidFire, you may even be abel to find out the old Storage Class / QoS settings for the volume, the original efficiency ratio and some other useful details.</p>

<p>Apart from that backup detail, Velero now behaves more in line with expectations. Earlier, and especially early releases, had both bugs and seemingly illogical behavior.</p>

<p>I’m sure some backup and restore options could be used to make Velero work even better. In Appendix D I give an example of using hooks to suspend filesystem IO before backup, and in my <a href="2024/03/23/velero-netapp-verda-scripts-and-trident.html">next post</a> I’ll show how to use hook scripts from NetApp’s Verda repository.</p>

<h2 id="appendix-a---configuration-details">Appendix A - configuration details</h2>

<p>This time I tested on a x86_64 system, but the entire stack also works on ARM64 systems (and you can get pre-built Trident v24.02 for ARM64 <a href="/2024/03/21/netapp-trident-v2402-arm64.html">here</a> if you want to try that).</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">cat</span> /etc/lsb-release 
<span class="nv">DISTRIB_ID</span><span class="o">=</span>Ubuntu
<span class="nv">DISTRIB_RELEASE</span><span class="o">=</span>24.04
<span class="nv">DISTRIB_CODENAME</span><span class="o">=</span>noble
<span class="nv">DISTRIB_DESCRIPTION</span><span class="o">=</span><span class="s2">"Ubuntu Noble Numbat (development branch)"</span>

<span class="nv">$ </span>k3s <span class="nt">--version</span>
k3s version v1.28.7+k3s1 <span class="o">(</span>051b14b2<span class="o">)</span>
go version go1.21.7

<span class="nv">$ </span>kubectl version <span class="nt">-o</span> yaml
clientVersion:
  major: <span class="s2">"1"</span>
  minor: <span class="s2">"28"</span>
serverVersion:
  major: <span class="s2">"1"</span>
  minor: <span class="s2">"28"</span>
  platform: linux/amd64

<span class="nv">$ </span>./tridentctl <span class="nt">-n</span> trident version
+----------------+----------------+
| SERVER VERSION | CLIENT VERSION |
+----------------+----------------+
| 24.02.0        | 24.02.0        |
+----------------+----------------+

sean@minikube:~/k2<span class="nv">$ </span>velero version
Client:
	Version: v1.13.1
	Git commit: ea5a89f83b89b2cb7a27f54148683c1ee8d57a37
Server:
	Version: v1.13.1

</code></pre></div></div>

<p>SolidFire was version 12.5, running in a VM on VMware ESXi 7 (x86_64). There’s no ARM64 version of this, but it can be used by ARM64 clients as you can see from previous Velero posts where an ARM64-based stack was used.</p>

<p>Trident has just one back-end, <code class="language-plaintext highlighter-rouge">solidfire-san</code>, and there’s one storage class for which there’s also a volume snapshot class.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>./tridentctl <span class="nt">-n</span> trident get backend 
+--------------------------+----------------+--------------------------------------+--------+------------+---------+
|           NAME           | STORAGE DRIVER |                 UUID                 | STATE  | USER-STATE | VOLUMES |
+--------------------------+----------------+--------------------------------------+--------+------------+---------+
| solidfire_192.168.105.30 | solidfire-san  | 6ebdc64a-76bd-4e2e-969f-64bcd575e288 | online | normal     |       3 |
+--------------------------+----------------+--------------------------------------+--------+------------+---------+

<span class="nv">$ </span>kubectl get sc
NAME                   PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
local-path <span class="o">(</span>default<span class="o">)</span>   rancher.io/local-path   Delete          WaitForFirstConsumer   <span class="nb">false                  </span>3h32m
silver <span class="o">(</span>default<span class="o">)</span>       csi.trident.netapp.io   Retain          Immediate              <span class="nb">true                   </span>122m

<span class="nv">$ </span>kubectl get volumesnapshotclass
NAME                    DRIVER                  DELETIONPOLICY   AGE
trident-snapshotclass   csi.trident.netapp.io   Delete           103m

<span class="nv">$ </span>kubectl describe sc silver
Name:            silver
IsDefaultClass:  Yes
Annotations:     kubectl.kubernetes.io/last-applied-configuration<span class="o">={</span><span class="s2">"allowVolumeExpansion"</span>:true,<span class="s2">"apiVersion"</span>:<span class="s2">"storage.k8s.io/v1"</span>,<span class="s2">"kind"</span>:<span class="s2">"StorageClass"</span>,<span class="s2">"metadata"</span>:<span class="o">{</span><span class="s2">"annotations"</span>:<span class="o">{</span><span class="s2">"storageclass.kubernetes.io/is-default-class"</span>:<span class="s2">"true"</span>,<span class="s2">"trident.netapp.io/blockSize"</span>:<span class="s2">"4096"</span><span class="o">}</span>,<span class="s2">"name"</span>:<span class="s2">"silver"</span><span class="o">}</span>,<span class="s2">"mountOptions"</span>:[<span class="s2">"discard"</span><span class="o">]</span>,<span class="s2">"parameters"</span>:<span class="o">{</span><span class="s2">"IOPS"</span>:<span class="s2">"800"</span>,<span class="s2">"backendType"</span>:<span class="s2">"solidfire-san"</span>,<span class="s2">"clones"</span>:<span class="s2">"true"</span>,<span class="s2">"fsType"</span>:<span class="s2">"xfs"</span>,<span class="s2">"snapshots"</span>:<span class="s2">"true"</span><span class="o">}</span>,<span class="s2">"provisioner"</span>:<span class="s2">"csi.trident.netapp.io"</span>,<span class="s2">"reclaimPolicy"</span>:<span class="s2">"Retain"</span><span class="o">}</span>
,storageclass.kubernetes.io/is-default-class<span class="o">=</span><span class="nb">true</span>,trident.netapp.io/blockSize<span class="o">=</span>4096
Provisioner:           csi.trident.netapp.io
Parameters:            <span class="nv">IOPS</span><span class="o">=</span>800,backendType<span class="o">=</span>solidfire-san,clones<span class="o">=</span><span class="nb">true</span>,fsType<span class="o">=</span>xfs,snapshots<span class="o">=</span><span class="nb">true
</span>AllowVolumeExpansion:  True
MountOptions:
  discard
ReclaimPolicy:      Retain
VolumeBindingMode:  Immediate
Events:             &lt;none&gt;

<span class="nv">$ </span>kubectl describe volumesnapshotclass trident-snapshotclass
Name:             trident-snapshotclass
Namespace:        
Labels:           &lt;none&gt;
Annotations:      &lt;none&gt;
API Version:      snapshot.storage.k8s.io/v1
Deletion Policy:  Delete
Driver:           csi.trident.netapp.io
Kind:             VolumeSnapshotClass
Metadata:
  Creation Timestamp:  2024-03-22T08:22:59Z
  Generation:          1
  Resource Version:    3787
  UID:                 68518d1a-a1cb-47ab-9744-efb8a8a1618c
Events:                &lt;none&gt;

</code></pre></div></div>

<h2 id="appendix-b---backup-and-details">Appendix B - backup and details</h2>

<p>Let’s see how that works. Details of my setup are in Appendix A.</p>

<p>Velero was installed as usual - please see my older posts about this (but note that AWS and CSI plugin versions are newer, so it’s best to read the Velero documentation and decide what’s relevant for you).</p>

<p>I have a SolidFire-based Trident Storage Class and Volume Snapshot Class.</p>

<p>While “warming up” for Velero I “manually” created two test PVCs and a test snapshot using <code class="language-plaintext highlighter-rouge">kubectl</code>. Then I removed the first volume.</p>

<p>Because my storage class has “retains” deleted volumes, I ended up with three volumes on SolidFire - one deleted and two in my target namespace (of which one was for testing, and the other one was in use by application protected by Velero).</p>

<p><img src="/assets/images/velero-trident-solidfire-description-02.png" alt="" /></p>

<p>There’s also a manually-created (not by Velero) snapshot of the testing volume that appears in CLI output, so don’t get confused by those.</p>

<p>Our app is NGINX and it runs in the namespace called <code class="language-plaintext highlighter-rouge">important</code> where it uses just one volume. That’s it.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="nv">$ </span>kubectl get statefulset <span class="nt">-n</span> important
NAME   READY   AGE
web    1/1     120m

<span class="nv">$ </span>kubectl describe statefulset web <span class="nt">-n</span> important
Name:               web
Namespace:          important
CreationTimestamp:  Fri, 22 Mar 2024 16:13:03 +0800
Selector:           <span class="nv">app</span><span class="o">=</span>nginx
Labels:             &lt;none&gt;
Annotations:        &lt;none&gt;
Replicas:           1 desired | 1 total
Update Strategy:    RollingUpdate
  Partition:        0
Pods Status:        1 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  <span class="nv">app</span><span class="o">=</span>nginx
  Containers:
   nginx:
    Image:        nginx:stable-alpine3.17
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  &lt;none&gt;
    Mounts:
      /usr/share/nginx/html from www <span class="o">(</span>rw<span class="o">)</span>
  Volumes:  &lt;none&gt;
Volume Claims:
  Name:          www
  StorageClass:  silver
  Labels:        &lt;none&gt;
  Annotations:   &lt;none&gt;
  Capacity:      2Gi
  Access Modes:  <span class="o">[</span>ReadWriteOnce]
Events:          &lt;none&gt;

<span class="nv">$ </span>kubectl get pods <span class="nt">-n</span> important
NAME    READY   STATUS    RESTARTS   AGE
web-0   1/1     Running   0          118m

<span class="nv">$ </span>kubectl get pvc <span class="nt">-n</span> important
NAME        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
www-web-0   Bound    pvc-9812208f-72f5-41d8-9348-4fb42db8e6af   2Gi        RWO            silver         123m
basic       Bound    pvc-a7b61fe0-7e9d-40f4-bc06-9c1623adade4   2Gi        RWO            silver         124m
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">www-web-0</code> is used by NGINX and that’s the one I aim to backup with Velero. (<code class="language-plaintext highlighter-rouge">basic</code> is the idle test volume that hasn’t been deleted and which has a manual snapshot.)</p>

<p>Let’s backup the entire namespace:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>velero backup create nginx-backup <span class="nt">--include-namespaces</span> important
Backup request <span class="s2">"nginx-backup"</span> submitted successfully.
Run <span class="sb">`</span>velero backup describe nginx-backup<span class="sb">`</span> or <span class="sb">`</span>velero backup logs nginx-backup<span class="sb">`</span> <span class="k">for </span>more details.
</code></pre></div></div>

<p>After I executed the above, I described the backup job.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>velero backup describe nginx-backup
Name:         nginx-backup
Namespace:    velero
Labels:       velero.io/storage-location<span class="o">=</span>default
Annotations:  velero.io/resource-timeout<span class="o">=</span>10m0s
              velero.io/source-cluster-k8s-gitversion<span class="o">=</span>v1.28.7+k3s1
              velero.io/source-cluster-k8s-major-version<span class="o">=</span>1
              velero.io/source-cluster-k8s-minor-version<span class="o">=</span>28

Phase:  InProgress

Namespaces:
  Included:  important
  Excluded:  &lt;none&gt;

Resources:
  Included:        <span class="k">*</span>
  Excluded:        &lt;none&gt;
  Cluster-scoped:  auto
</code></pre></div></div>

<p>I immediately realized I should have excluded the test volume (“basic”) from that namespace, but as you can see “excluded” is “none” - it wasn’t done, so both PVCs in the namespace were backed up.</p>

<p>Backup succeeded.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>velero get backups
NAME           STATUS      ERRORS   WARNINGS   CREATED                         EXPIRES   STORAGE LOCATION   SELECTOR
nginx-backup   Completed   0        0          2024-03-22 16:32:58 +0800 CST   29d       default            &lt;none&gt;
</code></pre></div></div>

<p>What about the highlight feature - backup details?</p>

<p>snapcontent-2f7b… in Velero backup maps to SolidFire snapshot ID 281, for example (you may open images in new tab to see the original size).</p>

<p><img src="/assets/images/velero-trident-solidfire-description-01.png" alt="Backup details in Velero v1.13" /></p>

<p>Let’s see this with <code class="language-plaintext highlighter-rouge">--details</code>. (Notice the junk volume “basic” was backed up due to being in target namespace and Velero took a snapshot of it, too.)</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>velero backup describe nginx-backup <span class="nt">--details</span>
...
Name:         nginx-backup
Namespace:    velero
Labels:       velero.io/storage-location<span class="o">=</span>default
Annotations:  velero.io/resource-timeout<span class="o">=</span>10m0s
              velero.io/source-cluster-k8s-gitversion<span class="o">=</span>v1.28.7+k3s1
              velero.io/source-cluster-k8s-major-version<span class="o">=</span>1
              velero.io/source-cluster-k8s-minor-version<span class="o">=</span>28

Phase:  Completed

Namespaces:
  Included:  important
  Excluded:  &lt;none&gt;

Total items to be backed up:  49
Items backed up:              49

Backup Item Operations:
  Operation <span class="k">for </span>volumesnapshots.snapshot.storage.k8s.io important/velero-www-web-0-ndwpw:
    Backup Item Action Plugin:  velero.io/csi-volumesnapshot-backupper
    Operation ID:               important/velero-www-web-0-ndwpw/2024-03-22T08:33:06Z
    Items to Update:
              volumesnapshots.snapshot.storage.k8s.io important/velero-www-web-0-ndwpw
    Phase:    Completed
    Created:  2024-03-22 16:33:06 +0800 CST
    Started:  2024-03-22 16:33:06 +0800 CST
  Operation <span class="k">for </span>volumesnapshotcontents.snapshot.storage.k8s.io /snapcontent-2f7b608e-64b2-4a2f-9709-c185ec2fed16:
    Backup Item Action Plugin:  velero.io/csi-volumesnapshotcontent-backupper
    Operation ID:               snapcontent-2f7b608e-64b2-4a2f-9709-c185ec2fed16/2024-03-22T08:33:06Z
    Items to Update:
              volumesnapshotcontents.snapshot.storage.k8s.io /snapcontent-2f7b608e-64b2-4a2f-9709-c185ec2fed16
    Phase:    Completed
    Created:  2024-03-22 16:33:06 +0800 CST
    Started:  2024-03-22 16:33:06 +0800 CST
  Operation <span class="k">for </span>volumesnapshots.snapshot.storage.k8s.io important/velero-basic-cx9b4:
    Backup Item Action Plugin:  velero.io/csi-volumesnapshot-backupper
    Operation ID:               important/velero-basic-cx9b4/2024-03-22T08:33:11Z
    Items to Update:
              volumesnapshots.snapshot.storage.k8s.io important/velero-basic-cx9b4
    Phase:    Completed
    Created:  2024-03-22 16:33:11 +0800 CST
    Started:  2024-03-22 16:33:11 +0800 CST
  Operation <span class="k">for </span>volumesnapshotcontents.snapshot.storage.k8s.io /snapcontent-19977bfb-7652-44d3-ac17-f1b4038bd3e5:
    Backup Item Action Plugin:  velero.io/csi-volumesnapshotcontent-backupper
    Operation ID:               snapcontent-19977bfb-7652-44d3-ac17-f1b4038bd3e5/2024-03-22T08:33:11Z
    Items to Update:
              volumesnapshotcontents.snapshot.storage.k8s.io /snapcontent-19977bfb-7652-44d3-ac17-f1b4038bd3e5
    Phase:    Completed
    Created:  2024-03-22 16:33:11 +0800 CST
    Started:  2024-03-22 16:33:11 +0800 CST
Resource List:
  apiextensions.k8s.io/v1/CustomResourceDefinition:
    - volumesnapshots.snapshot.storage.k8s.io
  apps/v1/ControllerRevision:
    - important/web-66bbffc487
  apps/v1/StatefulSet:
    - important/web
  discovery.k8s.io/v1/EndpointSlice:
    - important/nginx-z4svx
  snapshot.storage.k8s.io/v1/VolumeSnapshot:
    - important/basicsnap
    - important/velero-basic-cx9b4
    - important/velero-www-web-0-ndwpw
  snapshot.storage.k8s.io/v1/VolumeSnapshotClass:
    - trident-snapshotclass
  snapshot.storage.k8s.io/v1/VolumeSnapshotContent:
    - snapcontent-19977bfb-7652-44d3-ac17-f1b4038bd3e5
    - snapcontent-2f7b608e-64b2-4a2f-9709-c185ec2fed16
    - snapcontent-2ffba5b9-b0b9-418b-bc2e-98d1e2c9c77e
  v1/ConfigMap:
    - important/kube-root-ca.crt
  v1/Endpoints:
    - important/nginx
  v1/Event:
    - important/basic.17bf07b8f5c6b4cd
    - important/web-0.17bf0802ec13dedd
    - important/web-0.17bf0802f0235aef
    - important/web.17bf07be5d0d47c1
    - important/web.17bf0800fdfddd52
    - important/www-web-0.17bf07be7808fb85
  v1/Namespace:
    - important
  v1/PersistentVolume:
    - pvc-9812208f-72f5-41d8-9348-4fb42db8e6af
    - pvc-a7b61fe0-7e9d-40f4-bc06-9c1623adade4
  v1/PersistentVolumeClaim:
    - important/basic
    - important/www-web-0
  v1/Pod:
    - important/web-0
  v1/Service:
    - important/nginx
  v1/ServiceAccount:
    - important/default

Backup Volumes:
  Velero-Native Snapshots: &lt;none included&gt;

  CSI Snapshots:
    important/www-web-0:
      Snapshot:
        Operation ID: important/velero-www-web-0-ndwpw/2024-03-22T08:33:06Z
        Snapshot Content Name: snapcontent-2f7b608e-64b2-4a2f-9709-c185ec2fed16
        Storage Snapshot ID: pvc-9812208f-72f5-41d8-9348-4fb42db8e6af/snapshot-2f7b608e-64b2-4a2f-9709-c185ec2fed16
        Snapshot Size <span class="o">(</span>bytes<span class="o">)</span>: 2147483648
        CSI Driver: csi.trident.netapp.io
    important/basic:
      Snapshot:
        Operation ID: important/velero-basic-cx9b4/2024-03-22T08:33:11Z
        Snapshot Content Name: snapcontent-19977bfb-7652-44d3-ac17-f1b4038bd3e5
        Storage Snapshot ID: pvc-a7b61fe0-7e9d-40f4-bc06-9c1623adade4/snapshot-19977bfb-7652-44d3-ac17-f1b4038bd3e5
        Snapshot Size <span class="o">(</span>bytes<span class="o">)</span>: 2147483648
        CSI Driver: csi.trident.netapp.io

  Pod Volume Backups: &lt;none included&gt;

HooksAttempted:  0
HooksFailed:     0

</code></pre></div></div>

<p>That is great!</p>

<p>Now, how does that map to Kubernetes, Trident and SolidFire?</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl get pvc <span class="nt">-n</span> important
NAME        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS
www-web-0   Bound    pvc-9812208f-72f5-41d8-9348-4fb42db8e6af   2Gi        RWO            silver      
basic       Bound    pvc-a7b61fe0-7e9d-40f4-bc06-9c1623adade4   2Gi        RWO            silver      

<span class="nv">$ </span>kubectl get volumesnapshot <span class="nt">-n</span> important
NAME        READYTOUSE   SOURCEPVC   RESTORESIZE   SNAPSHOTCLASS           SNAPSHOTCONTENT                                 
basicsnap   <span class="nb">true         </span>www-web-0   2Gi           trident-snapshotclass   snapcontent-2ffba5b9-b0b9-418b-bc2e-98d1e2c9c77e

<span class="nv">$ </span>kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS     CLAIM                 STORAGECLASS
pvc-fc799089-9559-4d97-84c8-d98e9dfbf884   2Gi        RWO            Retain           Released   default/basic         silver      
pvc-a7b61fe0-7e9d-40f4-bc06-9c1623adade4   2Gi        RWO            Retain           Bound      important/basic       silver      
pvc-9812208f-72f5-41d8-9348-4fb42db8e6af   2Gi        RWO            Retain           Bound      important/www-web-0   silver      

</code></pre></div></div>

<p>From <code class="language-plaintext highlighter-rouge">get pv</code> we see that early on (I deleted the AGE column, by the way, to save space) we had the volume <code class="language-plaintext highlighter-rouge">default/basic</code> and it was deleted. Since the Storage Class uses “Retain”, the deleted PV is marked “Released” and still there on SolidFire.</p>

<p>Volume claim “important/basic” is a test PVC, but “important/www-web-0” is our app’s PVC and its PV name is <code class="language-plaintext highlighter-rouge">pvc-9812208f-72f5...</code>. As seen from <code class="language-plaintext highlighter-rouge">get pvc</code>, only two claims still exist, and <code class="language-plaintext highlighter-rouge">www-web-0</code> is the one we care about.</p>

<p>The snapshot <code class="language-plaintext highlighter-rouge">basicsnap</code> is also a manually created test object. Notice its SNAPSHOTCONTENT is snapcontent-2ffba5b9… That’s the manual one. Velero’s snapshot ID for the same volume is snapshot-19977bfb, and so “basic” has 2 snapshots.</p>

<p>In the Velero backup description above there’s also snapcontent-<strong><em>2f7b608e</em></strong> - that is snapshot of the volume used by NGINX.</p>

<p>So, related to our stateful set and its PVC “www-web-0”, Velero details are:</p>

<ul>
  <li>Velero snapshot operation detail: important/velero-www-web-0-ndwpw/2024-03-22T08:33:06Z</li>
  <li>Snapshot name: snapcontent-<strong>2f7b608e</strong>-64b2-4a2f-9709-c185ec2fed16</li>
  <li>Storage PV and Snapshot ID: pvc-9812208f-72f5-41d8-9348-4fb42db8e6af/snapshot-2f7b608e-64b2-4a2f-9709-c185ec2fed16</li>
  <li>Original PV (and therefore snapshot) size (bytes): 2147483648 (2Gi)</li>
  <li>CSI driver: csi.trident.netapp.io</li>
</ul>

<p>Notice how the two Velero-created snapshots (snapcontent-2f7b608e… and snapcontent-19977bfb…) are still available on SolidFire, but not listed in “get volumesnapshot” output.</p>

<p><img src="/assets/images/velero-trident-solidfire-description-03.png" alt="" /></p>

<p>Maybe it’s worth a mention that our Trident volume snapshot class “trident-snapshotclass” has the retention policy “Delete”, but the snapshots are not listed - only the test snapshot “basicsnap” is. Assuming that is by design, that’s a good thing because you won’t accidentally delete “backups” referenced by Velero.</p>

<p>Other than that, you’d expect that you’d see all snapshots in “kubectl get volumesnapshots”, but you don’t.</p>

<p>At the same time, <code class="language-plaintext highlighter-rouge">tridentctl</code> allows us to see <em>all</em> snapshots we took - 19977bfb and 2f7b608e taken by Velero, as well as the manually taken 2ffba5b9.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>./tridentctl <span class="nt">-n</span> trident get snapshot
+-----------------------------------------------+------------------------------------------+---------+
|                     NAME                      |                  VOLUME                  | MANAGED |
+-----------------------------------------------+------------------------------------------+---------+
| snapshot-2f7b608e-64b2-4a2f-9709-c185ec2fed16 | pvc-9812208f-72f5-41d8-9348-4fb42db8e6af | <span class="nb">true</span>    |
| snapshot-2ffba5b9-b0b9-418b-bc2e-98d1e2c9c77e | pvc-9812208f-72f5-41d8-9348-4fb42db8e6af | <span class="nb">true</span>    |
| snapshot-19977bfb-7652-44d3-ac17-f1b4038bd3e5 | pvc-a7b61fe0-7e9d-40f4-bc06-9c1623adade4 | <span class="nb">true</span>    |
+-----------------------------------------------+------------------------------------------+---------+

</code></pre></div></div>

<p>As a summary of this section, I’m again sharing that screenshot (which is larger and you can open it in new tab).</p>

<p><img src="/assets/images/velero-trident-solidfire-description-00.png" alt="Mapping from Velero to Trident to SolidFire" /></p>

<p>It shows the ease of mapping Velero backup details to Kubernetes and SolidFire. (The unmarked snapshot-19977bfb was also taken by Velero, but I didn’t mark it because it’s a test volume.)</p>

<p>In hindsight I should have deleted those unrelated volumes and snapshots, but I realized that too late. I did that in Appendix C which doesn’t have them.</p>

<h2 id="appendix-c---restore-and-delete">Appendix C - restore and delete</h2>

<p>I performed a few more operations just to see if it behaves consistently with slightly different settings and workflows.</p>

<p>After backup taken in the main content, I deleted the testing volume and its manual CSI snapshot, then deleted the only Velero backup.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>velero backup delete nginx-backup
Are you sure you want to <span class="k">continue</span> <span class="o">(</span>Y/N<span class="o">)</span>? y
Request to delete backup <span class="s2">"nginx-backup"</span> submitted successfully.
The backup will be fully deleted after all associated data <span class="o">(</span>disk snapshots, backup files, restores<span class="o">)</span> are removed.

</code></pre></div></div>

<p>After that move, SolidFire snapshots were deleted from SolidFire, but the volume (and application) remained as expected.</p>

<p>The next Velero backup took another storage snapshot of the same NGINX volume, so now there was only one.</p>

<pre><code class="language-raw">  CSI Snapshots:
    important/www-web-0:
      Snapshot:
        Operation ID: important/velero-www-web-0-5gjd2/2024-03-22T14:08:55Z
        Snapshot Content Name: snapcontent-51e370b3-6518-4528-a440-5a9f8416f073
        Storage Snapshot ID: pvc-9812208f-72f5-41d8-9348-4fb42db8e6af/snapshot-51e370b3-6518-4528-a440-5a9f8416f073
        Snapshot Size (bytes): 2147483648
        CSI Driver: csi.trident.netapp.io

</code></pre>

<p>I then deleted the stateful set and PVC, and restored data from Velero backup. This created a new volume (vol ID 118).</p>

<p><img src="/assets/images/velero-trident-solidfire-description-04.png" alt="Data restore creates a new volume" /></p>

<p>Volume 117 was still there (my Storage Class has Reclaim Policy set to Retain) and SolidFire would let you restore that pvc-9812208f (volume ID 117) from snapshot-51e370b3 (which is snapshot Velero created on that volume, just above).</p>

<p>But Velero instead spun a new volume from snapshot-51e370b3 and created volume 118.</p>

<p>Then you may wonder what is the purpose of volume ID 117. It appears to be there because snapshot used by Velero is based on that volume.</p>

<p>I did another backup-restore cycle with Storage Class reclaim policy set to Delete.</p>

<ul>
  <li>Create a new app and take a Velero backup (in which Velero creates a snapshot as well)</li>
  <li>Delete the app and PVC using kubectl. Both remain on SolidFire (despite reclaim policy Delete) but are gone from kubectl output</li>
  <li>After restoring this from velero backup, a new volume was created by Velero, while the old volume (and its snapshot) still remained as backup was referencing them</li>
  <li>After I deleted the backup I used to restore the app, Velero “released” these resources and deleted the old volume and snapshot that only Velero was referencing</li>
</ul>

<p>That’s still a bit unusual, but what’s important this oddness doesn’t cause data to <em>unexpectedly</em> go <em>missing</em>.</p>

<p>Early on (Velero v1.5.3), I saw odd behavior which included bugs but also “odd by design” and in possibly harmful ways.</p>

<h2 id="appendix-d---using-velero-hooks">Appendix D - using Velero hooks</h2>

<p><a href="https://velero.io/docs/main/backup-hooks/">Hooks</a> are executed in a container in a pod that’s being backed up. We can decide which container to use.</p>

<p>Pre-hooks run before a backup, and post-hooks after.</p>

<p>A pre-hook could for example freeze/suspend the app similar to the functionality available in Kasten’s <a href="/2022/04/13/backup-restore-beegfs-csi-pv-with-kanister-kasten.html">Kanister</a> integrations.</p>

<p>For NGINX (which I used in this testing), there’s an interesting example <a href="https://github.com/vmware-tanzu/velero/blob/v1.13.1/examples/nginx-app/with-pv.yaml">here</a>: although normally we wouldn’t expect NGINX to write, if logs are stored locally and not forwarded, that would be something we may want to freeze while performing a backup - <em>especially</em> if we’re doing doing a CSI-enabled backup (which would take a snapshot in an instant).</p>

<p>A post-hook could unfreeze the app and even send some details to Elasticsearch or other database.</p>

<p>Environmental variables available to a pod are also available to Velero hooks executing in it.</p>

<p>As an example, a user running <a href="https://github.com/scaleoutsean/eseries-perf-analyzer/">E-Series Performance Analyzer (EPA)</a> could run a post hook <code class="language-plaintext highlighter-rouge">post.hook.backup.velero.io/on-error</code> that would use ENV variables from EPA collector to create a Grafana notification (or a record in DB used by EPA that is surfaced in Grafana) about the failed job on that particular array, along with other details.</p>

<p>Now I’m making stuff up, so don’t try this verbatim, but to run a post hook in an EPA collector pod we could use Python to leverage ENV vars from the container. <code class="language-plaintext highlighter-rouge">/bin/sh</code> is required to “pick up” ENV variables, even if the notifier script may be written in another language.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">post</span><span class="pi">:</span>
<span class="pi">-</span> <span class="na">exec</span><span class="pi">:</span>
    <span class="na">container</span><span class="pi">:</span> <span class="s">collector-r24u04-e2824</span>
    <span class="na">command</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">/bin/sh</span>
      <span class="pi">-</span> <span class="s">-c</span>
      <span class="pi">-</span> <span class="s">influxwrite.py --password=${PASSWORD} --system=${SYSNAME} --endpoint=${API} --db=${DB_ADDRESS} --token=${TOKEN}</span>
    <span class="na">onError</span><span class="pi">:</span> <span class="s">Fail</span>
</code></pre></div></div>

<p>This would only offer information about the array on which a backup failed. We’d also want to know which backup failed, but I don’t see a way to pass that information to a hook. It probably can be hard-coded in each job, but that’s inconvenient at scale.</p>

<p>Finally, I decided to bite the bullet and do it…  I used and modified the example from the Velero repository, with hooks configured in annotations.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Copyright 2024 @scaleoutsean (Github).</span>
<span class="c1"># Copyright 2017 the Velero contributors.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the "License");</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an "AS IS" BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>

<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Namespace</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">nginx-example</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">nginx</span>

<span class="nn">---</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">PersistentVolumeClaim</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">nginx-logs</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">nginx-example</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">nginx</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="c1"># Optional: change SC name or remove the line with storageClassName to use default</span>
  <span class="na">storageClassName</span><span class="pi">:</span> <span class="s">silver</span>
  <span class="na">accessModes</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">ReadWriteOnce</span>
  <span class="na">resources</span><span class="pi">:</span>
    <span class="na">requests</span><span class="pi">:</span>
      <span class="na">storage</span><span class="pi">:</span> <span class="s">1Gi</span>

<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">apps/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Deployment</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">nginx-deployment</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">nginx-example</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">1</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">matchLabels</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s">nginx</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">app</span><span class="pi">:</span> <span class="s">nginx</span>
      <span class="na">annotations</span><span class="pi">:</span>
        <span class="s">pre.hook.backup.velero.io/container</span><span class="pi">:</span> <span class="s">fsfreeze</span>
        <span class="s">pre.hook.backup.velero.io/command</span><span class="pi">:</span> <span class="s1">'</span><span class="s">["/usr/sbin/fsfreeze",</span><span class="nv"> </span><span class="s">"--freeze",</span><span class="nv"> </span><span class="s">"/var/log/nginx"]'</span>
        <span class="s">post.hook.backup.velero.io/container</span><span class="pi">:</span> <span class="s">fsfreeze</span>
        <span class="s">post.hook.backup.velero.io/command</span><span class="pi">:</span> <span class="s1">'</span><span class="s">["/usr/sbin/fsfreeze",</span><span class="nv"> </span><span class="s">"--unfreeze",</span><span class="nv"> </span><span class="s">"/var/log/nginx"]'</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">volumes</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">nginx-logs</span>
          <span class="na">persistentVolumeClaim</span><span class="pi">:</span>
           <span class="na">claimName</span><span class="pi">:</span> <span class="s">nginx-logs</span>
      <span class="na">containers</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">image</span><span class="pi">:</span> <span class="s">nginx:1.25.4-bookworm</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">nginx</span>
        <span class="na">ports</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">80</span>
        <span class="na">volumeMounts</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">mountPath</span><span class="pi">:</span> <span class="s2">"</span><span class="s">/var/log/nginx"</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">nginx-logs</span>
            <span class="na">readOnly</span><span class="pi">:</span> <span class="no">false</span>
      <span class="pi">-</span> <span class="na">image</span><span class="pi">:</span> <span class="s">ubuntu:noble</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">fsfreeze</span>
        <span class="na">securityContext</span><span class="pi">:</span>
          <span class="na">privileged</span><span class="pi">:</span> <span class="no">true</span>
        <span class="na">volumeMounts</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">mountPath</span><span class="pi">:</span> <span class="s2">"</span><span class="s">/var/log/nginx"</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">nginx-logs</span>
            <span class="na">readOnly</span><span class="pi">:</span> <span class="no">false</span>
        <span class="na">command</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="s2">"</span><span class="s">/usr/bin/bash"</span>
          <span class="pi">-</span> <span class="s2">"</span><span class="s">-c"</span>
          <span class="pi">-</span> <span class="s2">"</span><span class="s">sleep</span><span class="nv"> </span><span class="s">infinity"</span>

  
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">nginx</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">my-nginx</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">nginx-example</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">ports</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">port</span><span class="pi">:</span> <span class="m">80</span>
    <span class="na">targetPort</span><span class="pi">:</span> <span class="m">80</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">nginx</span>
  <span class="na">type</span><span class="pi">:</span> <span class="s">LoadBalancer</span>

</code></pre></div></div>

<p>Apply and make sure it works.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl get pods <span class="nt">-n</span> nginx-example
NAME                               READY   STATUS    RESTARTS   AGE
nginx-deployment-f7bb8bd94-lpnvg   2/2     Running   0          25m

<span class="nv">$ </span>kubectl get pvc <span class="nt">-n</span> nginx-example
NAME         STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
nginx-logs   Bound    pvc-a9531e89-7900-4265-9910-030142b4646a   1Gi        RWO            silver         25m

</code></pre></div></div>

<p>Now we can backup:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>velero backup create nginx-frozen <span class="nt">--include-namespaces</span> nginx-example
Backup request <span class="s2">"nginx-frozen"</span> submitted successfully.
Run <span class="sb">`</span>velero backup describe nginx-frozen<span class="sb">`</span> or <span class="sb">`</span>velero backup logs nginx-frozen<span class="sb">`</span> <span class="k">for </span>more details.

<span class="nv">$ </span>velero backup describe nginx-frozen <span class="nt">--details</span> 
Name:         nginx-frozen
Namespace:    velero
Labels:       velero.io/storage-location<span class="o">=</span>default
Annotations:  velero.io/resource-timeout<span class="o">=</span>10m0s
              velero.io/source-cluster-k8s-gitversion<span class="o">=</span>v1.28.7+k3s1
              velero.io/source-cluster-k8s-major-version<span class="o">=</span>1
              velero.io/source-cluster-k8s-minor-version<span class="o">=</span>28

Phase:  Completed
...
Backup Volumes:
  Velero-Native Snapshots: &lt;none included&gt;

  CSI Snapshots:
    nginx-example/nginx-logs:
      Snapshot:
        Operation ID: nginx-example/velero-nginx-logs-ssf6z/2024-03-23T07:14:11Z
        Snapshot Content Name: snapcontent-17afb278-f1e9-4929-8077-7a697e56c97d
        Storage Snapshot ID: pvc-a9531e89-7900-4265-9910-030142b4646a/snapshot-17afb278-f1e9-4929-8077-7a697e56c97d
        Snapshot Size <span class="o">(</span>bytes<span class="o">)</span>: 1073741824
        CSI Driver: csi.trident.netapp.io

  Pod Volume Backups: &lt;none included&gt;

HooksAttempted:  2
HooksFailed:     0
</code></pre></div></div>

<p>We had two hooks, freeze before and unfreeze after. No hooks failed, which is good. Check the log and inspect hook-related lines.</p>

<p>Example for post-hook that runs unfreeze:</p>

<pre><code class="language-raw">time="2024-03-23T07:14:11Z" level=info msg="stderr: 
  " backup=velero/nginx-frozen hookCommand="[/usr/sbin/fsfreeze --unfreeze /var/log/nginx]" hookContainer=fsfreeze hookName="&lt;from-annotation&gt;" 
  hookOnError=Fail hookPhase=post hookSource=annotation hookTimeout="{30s}" hookType=exec logSource="pkg/podexec/pod_command_executor.go:180" 
  name=nginx-deployment-f7bb8bd94-lpnvg namespace=nginx-example resource=pods

time="2024-03-23T07:14:11Z" level=info 
  msg="hookTracker: map[{podNamespace:nginx-example podName:nginx-deployment-f7bb8bd94-lpnvg hookPhase:post hookName: hookSource:annotation container:fsfreeze}:
  {hookFailed:false hookExecuted:true} {podNamespace:nginx-example podName:nginx-deployment-f7bb8bd94-lpnvg hookPhase:pre hookName: hookSource:annotation 
  container:fsfreeze}:{hookFailed:false hookExecuted:true}], hookAttempted: 2, hookFailed: 0" backup=velero/nginx-frozen logSource="pkg/backup/backup.go:436"

</code></pre>

<p>Our volume name is pvc-a9531e89. Let’s do another backup and check if we can see its IO.</p>

<p><img src="/assets/images/velero-trident-solidfire-description-05.png" alt="Backup with pre hook fsfreeze" /></p>

<p>SolidFire registers a small IO burst on fsfreeze (with no throughput asssociated IO, since IOs were likely very few). When Velero mover kicks in to copy data to S3, a small throughput burst is registered as well.</p>

<p><img src="/assets/images/velero-trident-solidfire-description-06.png" alt="Backup with pre hook fsfreeze" /></p>

<p>For applications that shouldn’t or can’t use fsfreeze, create own scripts or use community resources.</p>

<p>Many apps don’t need any hooks and for those I’d use hooks only for notification purposes.</p>

      </article>
    </div>
    
      <div class="categories">
    <span><p>Categories:</p>
    
    
      <a href="
      /categories/#storage">storage</a>
      &nbsp; 
    
      <a href="
      /categories/#kubernetes">kubernetes</a>
       
    
  </span>
</div>
    

    
      <div class="related" data-pagefind-ignore>

    <h4>Possibly related - use live search at the top to find other content</h4>
    
    
    
    
    
    
    
        
        
    
        
    
        
          <div class = "related-posts">
          <h5><a href="/2024/05/03/netapp-solidfire-collector-next.html">• Towards next SolidFire Collector</a></h5>
          </div>
          
          
        
    
      
    
        
        
    
        
    
        
          <div class = "related-posts">
          <h5><a href="/2024/04/30/netapp-solidfire-account-attributes.html">• Tenant (account) attributes on NetApp SolidFire</a></h5>
          </div>
          
          
        
    
      
    
        
        
    
        
    
        
          <div class = "related-posts">
          <h5><a href="/2024/04/26/swagger-files-netapp-eseries-arrays.html">• Swagger files for NetApp E-Series</a></h5>
          </div>
          
          
        
    
      
    
        
        
    
        
    
        
          <div class = "related-posts">
          <h5><a href="/2024/04/25/openapi-swagger-for-netapp-solidfire.html">• OpenAPI and SolidFire</a></h5>
          </div>
          
          
        
    
      
    
        
        
    
        
    
        
          <div class = "related-posts">
          <h5><a href="/2024/04/24/netapp-solidfire-monitor-backup-influx-grafana-11.html">• Metrics for NetApp SolidFire backup-to-S3 in InfluxDB and Grafana</a></h5>
          </div>
          
          
            
    
    </div>

    

    
  </div><footer class= "footer">
    <p>2024-05-03 18:50 </p>
    <p>Copyright © 2024 scaleoutSean. Content is released under the CC BY license. Design: Alessio Franceschi</p>
</footer>

</body>
</html>
