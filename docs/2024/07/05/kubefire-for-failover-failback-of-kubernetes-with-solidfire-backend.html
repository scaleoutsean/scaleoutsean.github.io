<!doctype html>
<html lang="en">
  <head>
        <meta charset="UTF-8">
    <meta http-equiv="content-language" content="en">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="googlebot" content="noarchive">
    <meta name="googlebot" content="max-snippet:60">
    <meta name="googlebot" content="max-image-preview:small">
    <meta name="bingbot" content="noarchive">
    <meta name="bingbot" content="max-snippet:60">
    <meta name="bingbot" content="max-image-preview:small">
    <meta name="robots" content="noarchive">
    <meta name="robots" content="max-snippet:60">
    <meta name="google-site-verification" content="F6q7vIwQ2G0j8tk-KL9rAXOMLXMDMMUkEz4fRs1Nnnc" />
    <title>
        
            Kubefire - tools and recipes for Kubernetes failover/failback with SolidFire | Acting Technologist
      
    </title>
    <meta name="description" content="
     Recipes and tools for Kubernetes failover and failback with Trident CSI and NetApp SolidFire backends
     ">

    <!-- LINK TO ATOM FEED FOR SEO -->
    <link rel="alternate" type="application/atom+xml" href="https://scaleoutsean.github.io/feed.xml" />

    <!-- FAVICON -->
    <link rel="icon" href="/assets/favicon.ico" type="image/x-icon">
    <link rel="shortcut icon" href="/assets/favicon.ico" type="image/x-icon">

    <!-- CSS -->
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="stylesheet" href="/assets/css/syntax.css">

    <!-- Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css">

    <link href="/_pagefind/pagefind-ui.css" rel="stylesheet">
    <script src="/_pagefind/pagefind-ui.js" type="text/javascript"></script>
    <div id="search"></div>
    <script src="https://cdn.counter.dev/script.js" data-id="83dc700a-b821-4e57-9425-02b8336a9456" data-utcoffset="0"></script>
    <script>
        window.addEventListener('DOMContentLoaded', (event) => {
            new PagefindUI({ element: "#search" });
        });
    </script>

    <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Kubefire - tools and recipes for Kubernetes failover/failback with SolidFire | Acting Technologist</title>
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="Kubefire - tools and recipes for Kubernetes failover/failback with SolidFire" />
<meta name="author" content="scaleoutSean" />
<meta property="og:locale" content="en" />
<meta name="description" content="Recipes and tools for Kubernetes failover and failback with Trident CSI and NetApp SolidFire backends" />
<meta property="og:description" content="Recipes and tools for Kubernetes failover and failback with Trident CSI and NetApp SolidFire backends" />
<link rel="canonical" href="https://scaleoutsean.github.io/2024/07/05/kubefire-for-failover-failback-of-kubernetes-with-solidfire-backend.html" />
<meta property="og:url" content="https://scaleoutsean.github.io/2024/07/05/kubefire-for-failover-failback-of-kubernetes-with-solidfire-backend.html" />
<meta property="og:site_name" content="Acting Technologist" />
<meta property="og:image" content="https://scaleoutsean.github.io/assets/images/kubefire-longhorny-advanced-replication-report.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-07-05T00:00:00+08:00" />
<script type="application/ld+json">
{"image":"https://scaleoutsean.github.io/assets/images/kubefire-longhorny-advanced-replication-report.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://scaleoutsean.github.io/2024/07/05/kubefire-for-failover-failback-of-kubernetes-with-solidfire-backend.html"},"author":{"@type":"Person","name":"scaleoutSean"},"description":"Recipes and tools for Kubernetes failover and failback with Trident CSI and NetApp SolidFire backends","@type":"BlogPosting","url":"https://scaleoutsean.github.io/2024/07/05/kubefire-for-failover-failback-of-kubernetes-with-solidfire-backend.html","headline":"Kubefire - tools and recipes for Kubernetes failover/failback with SolidFire","dateModified":"2024-07-05T00:00:00+08:00","datePublished":"2024-07-05T00:00:00+08:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  </head>

<body>

  <div class="container">
    <header id="header">
	<div id = site_title>
		<a href="https://scaleoutsean.github.io/">
			<h2 style="font-size:1.8em;">Acting Technologist</h2>
		</a>
		
		<h3>
			civilizations are created by individuals
		</h3>
		
	</div>

	<div id="subheader">
		
		<nav class="pages">
<a href="/about.html">About</a>

<a href="/archive.html">Archive</a>

<a href="/categories/">Categories</a>

<a href="/projects.html">Projects</a>
</nav>
		
		
		<nav class="social">
			
  
    <a href="https://www.github.com/scaleoutsean" target="_blank" id="github"><i class="fab fa-github" aria-hidden="true"></i></a>
  
   

  
    <a href="https://twitter.com/scaleoutsean" target="_blank" id="twitter"><i class="fab fa-twitter" aria-hidden="true"></i></a>
  
   

  
  
    <a href="/feed.xml" target="_blank" id="rss"><i class="fas fa-rss" aria-hidden="true"></i></a>
   

		</nav>
		
	</div>
</header>


    <div class="post-container">
      <article id = "post">
        <h1 id = "post-title">Kubefire - tools and recipes for Kubernetes failover/failback with SolidFire</h1>

  
  <div class = "post-info">
    <span>
        <i class="far fa-calendar" aria-hidden="true"></i> <span>05 Jul 2024</span> - <i class="far fa-clock"></i> 


  
  
    8 minute read
  

    </span>
  </div>
  
        <ul>
  <li><a href="#introduction">Introduction</a></li>
  <li><a href="#kubefire">Kubefire</a>
    <ul>
      <li><a href="#simplified-version-of-longhorny-focused-on-kubernetes">Simplified version of Longhorny focused on Kubernetes</a></li>
      <li><a href="#automated-replication-service-for-solidfire-replication-in-a-kubernetes-environment">Automated replication service for SolidFire replication in a Kubernetes environment</a></li>
    </ul>
  </li>
  <li><a href="#conclusion">Conclusion</a></li>
  <li><a href="#appendix-a---implementation-ideas">Appendix A - implementation ideas</a></li>
</ul>

<h2 id="introduction">Introduction</h2>

<p>In <a href="/2024/07/02/solidfire-volume-attributes-from-trident-and-other-apps.html">this post</a> I described how SolidFire Collector collects and stores volume attributes from NetApp Trident CSI.</p>

<p>It’s one of several pieces of the storage replication, failover and failback puzzle - maybe not even a necessary, but certainly a useful one.</p>

<p>As a reminder:</p>

<ul>
  <li>Since CSI took over storage provisioning and management in Kubernetes, it is no longer possible to simply tell Kubernetes workers to use a different path to your storage. Failover to a different back end implies different PVs.</li>
  <li>As Trident CSI hasn’t developed any features to make failover and failback with SolidFire backends easier, the only viable way to failover and failback is to connect each Kubernetes cluster to its own SolidFire backend</li>
</ul>

<p>To make that easier, I wrote Longhorny (see the post linked at the top) which makes replication, failover and failback easier, and SFC that collects and stores volume-related settings and metadata. Another script maps Trident CSI PVCs to SolidFire volumes and tenants.</p>

<p>Now it’s a matter of putting those bits and pieces together, and maybe adding an extra script or documenting workflow, to make all this more automated and prescriptive. Another benefit is maybe some users out there can reuse some of this and create workflows that work better for their environments.</p>

<h2 id="kubefire">Kubefire</h2>

<p>The <a href="https://github.com/scaleoutsean/kubefire">repository</a> is currently empty - apart from an introduction and links to the above scripts and tools - but I expect to add a recipe or workflow to make it complete.</p>

<p>I’ve been thinking about the way Kubefire would work for me personally, and my current top two workflow candidates are:</p>

<h3 id="simplified-version-of-longhorny-focused-on-kubernetes">Simplified version of Longhorny focused on Kubernetes</h3>

<p>Longhorny can already perform all SolidFire replication and failover/failback tasks.</p>

<p>What it does “too much” (as far as Kubefire-related tasks are concerned) is checking and validation. Longhorny is meant to deliver its functionality in environments where each site may have multiple clusters, not just Kubernetes, but also other. It is also very careful when tearing down replication relationships.</p>

<p>What Longhorny does “too little” is it won’t let you flip one storage tenant’s volumes from one site to another, and the reason is most people don’t do that anyway (I mean, have multiple tenants in a paired cluster environment and not all run at the same site). It isn’t a crazy idea, but it can be challenging to manage - imagine having 2 vSphere and 3 Kubernetes clusters whose administrators switch them over from one site to the other as they please.</p>

<p>Longhorny could allow that, but just checking for all the possible problems and mis-steps would be a complex task.</p>

<p>For Kubefire, however, that may be exactly what people want - it would do similar things that Longhorny does, but account-specific:</p>

<ul>
  <li>Specify a (Trident CSI) storage account for each site</li>
  <li>Let Kubefire create replicated volume pairs for these two accounts</li>
  <li>Allow failover/failback (change of replication direction) for all volumes owned by these accounts</li>
</ul>

<p>If you screw things up by randomly deleting volumes or fiddling with access modes, it’s up to you to fix it.</p>

<p>Longhorny wouldn’t delete volumes in this approach, but careless management could leave you with messed up relationships where some volumes would replicate from A to B, others from B to A, and yet others not replicating at all. It would require manual intervention to sort out.</p>

<h3 id="automated-replication-service-for-solidfire-replication-in-a-kubernetes-environment">Automated replication service for SolidFire replication in a Kubernetes environment</h3>

<p>The other approach is more radical.</p>

<p>It occurred to me that it’s possible to make that workflow even easier and more automated if we allow Kubefire to perform destructive actions. More automation, more risk, more benefits.</p>

<p>In this approach we also have a unique storage account pair for two sites, but volume pairing relationships are managed by Kubefire.</p>

<p>Every 5-10 minutes Kubefire checks Trident CSI volumes at the active site:</p>

<ul>
  <li>If a volume is new on the active site, create a new, replica volume at the passive site and enable replication between the pair
    <ul>
      <li>To avoid replicating all sorts of junk, potentially set this up so that it only works for specific volume QoS settings or Kubernetes namespace</li>
      <li>To help pick the right replication mode (Async, Sync, SnapshotsOnly), also apply filters based on QoS policy setting or Kubernetes namespace</li>
    </ul>
  </li>
  <li>If a volume is larger than the replica, automatically resize the replica and resume replication
    <ul>
      <li>This isn’t controversial and eliminates the need to notify the storage administrator of volume resizing at the active site</li>
    </ul>
  </li>
  <li>If a volume deleted and only an orphaned replica available, delete the replica to eliminate orphaned volumes (this is possibly controversial, so we could optionally not delete it, or make it an option)</li>
</ul>

<p>It would also have the ability to change the direction of replication for all paired volumes owned by a pair of accounts. This would be performed manually, of course.</p>

<p>There isn’t anything dangerous in this approach, but most people don’t like to have scripts with destructive power running in autopilot mode.</p>

<p>In the case you haven’t noticed, <a href="https://github.com/NetApp/trident/releases/tag/v23.10.0">Astra Trident CSI v23.10</a> added some volume replication feature for ONTAP backends. I don’t even know what it does, but I doubt it would be more convenient than this automated approach.</p>

<h2 id="conclusion">Conclusion</h2>

<p>A modified version of Longhorny would probably be more appealing to SolidFire users with multiple Kubernetes or other clusters per site. If they don’t mind the super-careful approach taken by Longhorny, they could use this approach today - the only step that Longhorny can’t do is flip volumes based on account ID (it currently flips all volumes from one site to another), but they could achieve that with a single PowerShell command.</p>

<p>The radical version sounds more appealing, but it has to be reliable and transparent, otherwise no one will want to try it.</p>

<p>I’ll toy with these ideas and see which one wins me over.</p>

<p>The Longhorny approach works today and even without any new code we only need 2-3 PowerShell commands for users to execute when they want to fail over or change the direction of replication. This makes the radical approach more interesting, as the conservative approach already works, so maybe I’ll try to do both:</p>

<ul>
  <li>Provide PowerShell commands for failover/failback for Longhorny users</li>
  <li>Create a new “autopilot” script based on Longhorny code</li>
</ul>

<h2 id="appendix-a---implementation-ideas">Appendix A - implementation ideas</h2>

<ul>
  <li>Site A: Kubernetes volume information is obtained by ingesting <code class="language-plaintext highlighter-rouge">tridentctl</code> output (as mentioned in one of previous posts on this topic, the idea behind this is Kubefire doesn’t require access to either <code class="language-plaintext highlighter-rouge">kubectl</code> or <code class="language-plaintext highlighter-rouge">tridentctl</code> that way - Kubernetes admin schedules these to output configuration to files)</li>
  <li>Site B: the same approach, but <code class="language-plaintext highlighter-rouge">tridentctl</code> and <code class="language-plaintext highlighter-rouge">kubectl</code> inputs are collected from Kubernetes/SolidFire at the second site</li>
  <li><code class="language-plaintext highlighter-rouge">tridentctl</code> and <code class="language-plaintext highlighter-rouge">kubectl</code> from each site uploads volume information to S3 bucket
    <ul>
      <li>This makes it possible to always get this information and provides HA to <code class="language-plaintext highlighter-rouge">tridentctl</code> output</li>
      <li>Kubefire downloads information from this location/bucket and needs just <em>outbound</em> access to three-four locations (one or two S3 buckets, and two SolidFire MVIPs)</li>
    </ul>
  </li>
  <li>Kubefire has the Trident CSI storage account ID from each site and the active site is provided in startup arguments
    <ul>
      <li>Account IDs can also be determined from <code class="language-plaintext highlighter-rouge">tridentctl</code> output and used for verification/sanity check</li>
      <li>Kubefire gets volumes names and sizes from <code class="language-plaintext highlighter-rouge">tridentctl</code> and creates and grows remote (replica) volumes as necessary</li>
      <li>On Kubernetes/SolidFire site failover, Kubefire is restarted with the surviving site designed as active</li>
      <li>Kubefire takes note of, and logs volumes “orphaned” from the active site, but it’s up to the administrator to decide what to do about them. As mentioned earlier, namespaces where volumes are “randomly” added and removed may be excluded from Kubefire’s coverage to eliminate the need to frequently fix these discrepancies</li>
    </ul>
  </li>
  <li>If a site fails, Kubefire needs to be restarted to reverse the direction of replication and start watching for changes at the new active site, but we also need to use <code class="language-plaintext highlighter-rouge">tridentctl</code> information to import replicated volumes to Kubernetes at the new active site. This can be done by Kubefire, or even better manually with a separate command following a review of latest replication status
    <ul>
      <li>If replication of some volumes was significantly (e.g. 15 minutes) delayed, decision needs to be made whether it’s worth failing over (and losing 15 minutes of data) or waiting for the source site to recover</li>
    </ul>
  </li>
  <li>Volume relationship and status reports are available from InfluxDB (<a href="/2024/06/15/sfc-adds-volume-replication-monitoring.html#what-can-we-see">populated by SFC collector</a>)
    <ul>
      <li>Longhorny’s “mismatched volumes” feature can also report mismatched volumes and a more advanced “volume replication report” (I wrote about that <a href="/2024/06/12/longhorny-cluster-volume-replication-report.html">here</a>) may be added as well; it would report detailed replication status and maybe send changes to SFC (InfluxDB, that is) and make no configuration changes</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/images/kubefire-longhorny-advanced-replication-report.png" alt="Longhorny - advanced replication report" /></p>

<p>This example shows the situation in which four volume pairs are set up for replication and there are no problems in terms of direction, account ID or size mismatches.</p>

<p>Status reports - such as whether replication is delayed and by how much - is already collected by SFC and can be viewed in Grafana or queried in InfluxDB. Before failback the administrator needs to wait until all replicated volume pairs are in sync, to avoid data loss on failback. Fortunately this information is already available in SFC.</p>


      </article>
    </div>
    
      <div class="categories">
    <span><p>Categories:</p>
    
    
      <a href="
      /categories/#storage">storage</a>
      &nbsp; 
    
      <a href="
      /categories/#kubernetes">kubernetes</a>
      &nbsp; 
    
      <a href="
      /categories/#solidfire">solidfire</a>
      &nbsp; 
    
      <a href="
      /categories/#projects">projects</a>
       
    
  </span>
</div>
    

  
    <div>
      <h3>Related Posts</h3>
      <ul>
      
        <li><a href="/2024/07/02/solidfire-volume-attributes-from-trident-and-other-apps.html">Extract SolidFire volume attributes created by Trident or other apps</a></li>
      
        <li><a href="/2024/07/26/netapp-trident-csi-rapid-volume-provisioning-solidfire.html">Rapid PVC provisioning with NetApp Trident and SolidFire</a></li>
      
        <li><a href="/2021/03/20/kubernetes-solidfire-failover-failback.html">Kubernetes failover and failback with Trident CSI and SolidFire</a></li>
      
        <li><a href="/2024/06/15/sfc-adds-volume-replication-monitoring.html">SFC v2 adds volume replication monitoring</a></li>
      
        <li><a href="/2024/06/14/netapp-solidfire-replication-monitoring.html">Monitoring volume replication in NetApp SolidFire</a></li>
      
      </ul>
    </div>
  

    
  </div><footer class= "footer">
    <p>2025-06-05 03:19 </p>
    <p>Copyright © 2025 scaleoutSean. Content is released under the CC BY license. Design: Alessio Franceschi</p>
</footer>

</body>
</html>
