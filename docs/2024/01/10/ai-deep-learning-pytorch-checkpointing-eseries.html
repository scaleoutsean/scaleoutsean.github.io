<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.27.3 by Michael Rose
  Copyright 2013-2025 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>PyTorch checkpointing workload with NetApp E-Series | Acting Technologist</title>
<meta name="description" content="Notes on checkpoints and checkpointing workload with Torch and E-Series">


  <meta name="author" content="scaleoutSean">
  
  <meta property="article:author" content="scaleoutSean">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Acting Technologist">
<meta property="og:title" content="PyTorch checkpointing workload with NetApp E-Series">
<meta property="og:url" content="https://scaleoutsean.github.io/2024/01/10/ai-deep-learning-pytorch-checkpointing-eseries.html">


  <meta property="og:description" content="Notes on checkpoints and checkpointing workload with Torch and E-Series">





  <meta name="twitter:site" content="@scaleoutSean">
  <meta name="twitter:title" content="PyTorch checkpointing workload with NetApp E-Series">
  <meta name="twitter:description" content="Notes on checkpoints and checkpointing workload with Torch and E-Series">
  <meta name="twitter:url" content="https://scaleoutsean.github.io/2024/01/10/ai-deep-learning-pytorch-checkpointing-eseries.html">

  
    <meta name="twitter:card" content="summary">
    
  

  



  <meta property="article:published_time" content="2024-01-10T00:00:00+08:00">






<link rel="canonical" href="https://scaleoutsean.github.io/2024/01/10/ai-deep-learning-pytorch-checkpointing-eseries.html">







  <meta name="google-site-verification" content="F6q7vIwQ2G0j8tk-KL9rAXOMLXMDMMUkEz4fRs1Nnnc" />


  <meta name="msvalidate.01" content="7cf5b7d96a77410a8ad035f764dc81b3">





<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Acting Technologist Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  window.enable_copy_code_button = true;
</script>

<script src="https://cdn.counter.dev/script.js" data-id="83dc700a-b821-4e57-9425-02b8336a9456" data-utcoffset="0"></script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <link href="https://fonts.googleapis.com/css?family=PT+Sans+Narrow:400,700|PT+Serif:400,700,400italic" rel="stylesheet" type="text/css">


  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
          <a class="site-logo" href="/"><img src="/assets/images/scaleoutsean-acting-technologist.png" alt="Acting Technologist"></a>
        
        <a class="site-title" href="/">
          Acting Technologist
          <span class="site-subtitle">human action through technology</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/archive/"
                
                
              >Archive</a>
            </li><li class="masthead__menu-item">
              <a
                href="/categories/"
                
                
              >Categories</a>
            </li><li class="masthead__menu-item">
              <a
                href="/tags/"
                
                
              >Tags</a>
            </li><li class="masthead__menu-item">
              <a
                href="/projects.html"
                
                
              >Projects</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main">
  


  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="PyTorch checkpointing workload with NetApp E-Series">
    <meta itemprop="description" content="Recommendations for saving and loading Torch checkpoints with E-Series">
    <meta itemprop="datePublished" content="2024-01-10T00:00:00+08:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://scaleoutsean.github.io/2024/01/10/ai-deep-learning-pytorch-checkpointing-eseries.html" itemprop="url">PyTorch checkpointing workload with NetApp E-Series
</a>
          </h1>
          

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2024-01-10T00:00:00+08:00">2024-01-10 00:00</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          10 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
        <ul>
  <li><a href="#introduction">Introduction</a></li>
  <li><a href="#torch-checkpoints">Torch checkpoints</a></li>
  <li><a href="#filesystem-format-path-and-files">Filesystem format, path and files</a></li>
  <li><a href="#example">Example</a></li>
  <li><a href="#storage-considerations">Storage considerations</a></li>
  <li><a href="#file-system-considerations">File system considerations</a></li>
  <li><a href="#s3-vs-cluster-file-system">S3 vs cluster file system</a></li>
  <li><a href="#hybrid-cloud-options">Hybrid cloud options</a></li>
  <li><a href="#conclusion">Conclusion</a></li>
  <li><a href="#appendix-a---orbax">Appendix A - Orbax</a></li>
</ul>

<h2 id="introduction">Introduction</h2>

<p>Checkpoints save the state of the system and that state can be used to resume work without losing work already done.</p>

<p>One could, for example, use <a href="https://spot.io">Spot.io</a> to take advantage of low-cost compute instances, and save checkpoints to be able to resume work later in the case one or more go down before training is over.</p>

<p>They're not a new concept or technology. In fact, they are very common in <a href="https://duckduckgo.com/?t=ffab&amp;q=hpc+checkpointing+mpi">HPC</a>.</p>

<p>If you're looking for information on AI checkpointing in general, there's <a href="https://duckduckgo.com/?t=ffab&amp;q=Checkpointing+AI+models">plenty of it</a> on the Internet.</p>

<p>My objective was to find how they work and see what would be some general approaches to optimize checkpointing on NetApp E-Series.</p>

<p>This post focuses on PyTorch-related checkpointing.</p>

<h2 id="torch-checkpoints">Torch checkpoints</h2>

<p>There are two kinds:</p>

<ul>
  <li>traditional: <a href="https://pytorch.org/docs/stable/generated/torch.save.html">torch.save</a></li>
  <li>distributed: <a href="https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html">dcp</a></li>
</ul>

<p>As the name suggests, the latter is useful because in distributed DL parameters and gradients are partitioned, and the number of worker nodes and GPUs can be different when checkpoint needs to be loaded. <code class="language-plaintext highlighter-rouge">dcp</code> may be used with non-distributed as well, but it may have some limitations.</p>

<p><code class="language-plaintext highlighter-rouge">dcp</code> works on single-GPU systems, but as long as it's not asynchronous, I'm not sure it's beneficial to use it instead of the usual. For what it's worth, here's <code class="language-plaintext highlighter-rouge">dcp</code> on a single-GPU system:</p>

<p><img src="/assets/images/pytorch-checkpoint-07.png" alt="" /></p>

<p>For asynchronous checkpointing, at this time PyTorch Lighting offers it as "experimental" feature (see <a href="https://lightning.ai/docs/pytorch/stable/common/checkpointing_expert.html#asynchronous-checkpointing">here</a>).</p>

<h2 id="filesystem-format-path-and-files">Filesystem format, path and files</h2>

<p>Since the 1.6 release PyTorch uses a new zipfile-based file format. This is great for E-Series which doesn't support deduplication in any case and modern clients can compress data at a high speed.</p>

<p>Paths In some cases you can provide preferred path, in others it may be hard-coded (example: <code class="language-plaintext highlighter-rouge">/opt/ml/checkpoints</code>).</p>

<h2 id="example">Example</h2>

<p>To take a closer look I used one of Hugging Face transformer examples (text summarization) on a system with a single GPU.</p>

<p>The workload occupied 4.6 GiB VRAM.</p>

<p><img src="/assets/images/pytorch-checkpoint-01.png" alt="" /></p>

<p>With this framework I could decide the output directory (<code class="language-plaintext highlighter-rouge">--output_dir /tmp/tst-summarization</code>). Checkpoints were periodically saved to subdirectories in that path.</p>

<p><img src="/assets/images/pytorch-checkpoint-02.png" alt="" /></p>

<p>The exact interval depends on several factors and although here we can see it happen every one-two minutes, in production it could happen much less frequently.</p>

<p><img src="/assets/images/pytorch-checkpoint-03.png" alt="" /></p>

<p>As you can see here, each checkpoint is roughly the same and the first one was same large, so these appear to be full checkpoints. I didn't find a way to create incremental checkpoints with PyTorch.</p>

<p>Multiple (N) independent nodes using "classic" checkpoints would increase the frequency of saving N times.</p>

<p>Using <code class="language-plaintext highlighter-rouge">dcp</code> on multiple nodes, on the other hand, would create a larger "aggregate" checkpoint composed of multiple files (one per GPU on which checkpointed job runs) as all jobs would save their memory at once.</p>

<p>Checkpoint files (from regular PyTorch checkpoint) look like this. Sizes of largest files are likely model size-dependent.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">ls</span> <span class="nt">-l</span> /tmp/tst-summarization/checkpoint-5500/
total 711424
<span class="nt">-rw-rw-r--</span> 1 sean sean      1507 Jan 10 14:23 config.json
<span class="nt">-rw-rw-r--</span> 1 sean sean       147 Jan 10 14:23 generation_config.json
<span class="nt">-rw-rw-r--</span> 1 sean sean 242041896 Jan 10 14:23 model.safetensors
<span class="nt">-rw-rw-r--</span> 1 sean sean 484163514 Jan 10 14:23 optimizer.pt
<span class="nt">-rw-rw-r--</span> 1 sean sean     14244 Jan 10 14:23 rng_state.pth
<span class="nt">-rw-rw-r--</span> 1 sean sean      1064 Jan 10 14:23 scheduler.pt
<span class="nt">-rw-rw-r--</span> 1 sean sean      2543 Jan 10 14:23 special_tokens_map.json
<span class="nt">-rw-rw-r--</span> 1 sean sean    791656 Jan 10 14:23 spiece.model
<span class="nt">-rw-rw-r--</span> 1 sean sean     20746 Jan 10 14:23 tokenizer_config.json
<span class="nt">-rw-rw-r--</span> 1 sean sean   2422289 Jan 10 14:23 tokenizer.json
<span class="nt">-rw-rw-r--</span> 1 sean sean      1821 Jan 10 14:23 trainer_state.json
<span class="nt">-rw-rw-r--</span> 1 sean sean      4856 Jan 10 14:23 training_args.bin

</code></pre></div></div>

<p>Some files may or may not be there depending on options. For example, rng_state.pth may be optional if you don't need it.</p>

<p>Each checkpoint had the same size, which wasn't unexpected. Considering that VRAM size was 4.6 GiB, 695 MB (compressed) checkpoint size seems reasonable.</p>

<p>This is an oversimplification, but let us assume that a checkpoint is somewhere around 1/8th of GPU memory used.</p>

<h2 id="storage-considerations">Storage considerations</h2>

<p>E-Series is used in HPC and many users store TB-sized checkpoints on cluster file systems that reside on E-Series.</p>

<p>It is my guess that Deep Learning checkpoints are relatively smaller, and in environments sized to provide many GB/s in read performance it is not difficult to find room for a few GB/s in write performance.</p>

<p>In DL environments with E-Series we usually see a parallel file system such as BeeGFS, GPFS or Lustre, and save checkpoints to it. In the case of S3 we <a href="/2022/10/21/minio-performance-netapp-e-series.html">know</a> MinIO on E-Series can also deliver GB/s in write performance and <a href="/2023/09/20/versity-gw-s3-posix-gateway-beegfs-eseries.html">Versity S3 Gateway</a> also looks promising.</p>

<p>A GB-sized checkpoint would take just second to write. A 20-node cluster with 8 GPUs per node and 40 GB VRAM per GPU would have 6.4 TB GPU RAM  / 8 or some 800 GB on-disk checkpoint data. ("/ 8" comes from the $CAPACITY_GB / 8 assumption made 5-6 paragraphs ago.)</p>

<p>At 10 GB/s write speed (doable with a single EF600), it would take a minute to checkpoint 800 GB.</p>

<p>Smaller workloads with checkpoints taken at different times would of course take less time and space.</p>

<p>If we keep many checkpoints for a while, we may want to use NL-SAS instead of flash disks. This being a sequential write (and later read) workload, there normally isn't much advantage to keeping many checkpoints on flash storage. If you can keep the recent few on flash, that may be okay.</p>

<p>RAID-wise, RAID 6 would be suitable here. Optionally we could use RAID 10, but that would be more expensive capacity-wise.</p>

<p>Assuming that RAID 10 could be 30% faster than RAID 6, flash may make sense if time savings (which prevent GPUs from idling) can justify RAID 10. We also have to consider load time here - if workloads are frequently preempted or interrupted and checkpoints equally frequently loaded back, then using RAID 10 becomes even more attractive.</p>

<p>DDP (disk pools) could also be used (both with RAID 6 and 10-style volumes), but in HPC/DL disks are rarely pooled in DDP groups because RAID 6 and RAID 10 perform slightly better.</p>

<p>Reads are usually easier on storage than writes, so loading (restoring) checkpoints is less taxing on filesystem and storage controllers, and with E-Series can be much faster. If a checkpoint takes 30 seconds to save, it may take 15-20 seconds to load it. Of course, "it (always) depends", but loading a checkpoint is expected to be faster than saving it.</p>

<h2 id="file-system-considerations">File system considerations</h2>

<p>Some parallel file systems support tiers and ILM (GPFS, for example) and it is possible to write new checkpoints to RAID 10 and move older checkpoints to a lower-cost RAID 6-backed filesystem. But that's likely to be effective only in very large environments.</p>

<p>Since parallel file systems are also used in HPC, I would just recommend to apply general considerations for HPC storage and - if available - checkpoint-specific or scratch space-specific tuning information for dedicated checkpoint filesystems.</p>

<p>Some general considerations for BeeGFS and E-Series design can be found <a href="/2022/08/28/configuring-netapp-e-series-solution-for-beegfs.html">here</a> and on the NetApp Web site.</p>

<p>About storage and filesystem efficiencies: because large checkpoint files are compressed, compressing these checkpoints again (before replication, for example) with generic compression approaches doesn't work well. For example, compressing compressed checkpoints saved me only single digit percentage points.</p>

<p>Saving uncompressed checkpoints would make storage efficiencies possible, but E-Series doesn't have any so that approach wouldn't be helpful unless one created uncompressed checkpoints and had a compressing and deduplicating filesystem that could potentially not only compress data, but also deduplicate any similarities between various checkpoints. I have no idea if there is much similarity between checkpoints, though, and haven't explored these options.</p>

<p>If you want to make a checkpoint available elsewhere, simply copy it, which can be part of your regular deep learning jobs, or another on-demand job.</p>

<p><img src="/assets/images/pytorch-checkpoint-05.png" alt="" /></p>

<h2 id="s3-vs-cluster-file-system">S3 vs cluster file system</h2>

<p>If, for some reason, checkpoints need to be copied to S3, sometimes it may be better to save them to disk and upload to S3 after that.</p>

<p>Although - with MinIO sharing the same E-Series (array or arrays) as parallel file system - that means writing the same thing twice, the first save would go to the disk at full speed (e.g. 10, 20 GB/s) and allow GPUs to resume work ASAP, while the second (filesystem to S3) could run slowly at a steady pace (e.g. 1 GB/s). For example:</p>

<ul>
  <li>30 minute checkpoints to filesystem at 10 GB/s,</li>
  <li>followed by bandwidth rate-limited upload from filesystem to S3 at 1-2 GB/s, after which the filesystem copy may be deleted (with or without a sleep/delay)</li>
</ul>

<p>Upload to S3 can be done as separate step (say, in Jupyter) using your favorite S3 client. To save bandwidth and disk space you may want to run this "sync to S3" step only if certain conditions have been met (e.g. the latest checkpoint, or every 4th checkpoint, or every checkpoint when epoch result meets certain criteria).</p>

<p>If you use NetApp DataOps Toolkit, you can see <a href="/2022/01/28/storagegrid-hybrid-cloud-processing-without-data-at-rest.html#netapp-dataops-toolkit">this example</a>. That could allow you to resume from a checkpoint in public cloud, for example using containers managed by Spot.io (see this <a href="/2023/01/12/beegfs-eseries-hybrid-cloud-spot-ocean-spark.html#workflow">workflow example</a>).</p>

<h2 id="hybrid-cloud-options">Hybrid cloud options</h2>

<p>As mentioned above (and in the linked articles), S3 is a good option for hybrid cloud situations because checkpoints can be dumped on to S3, which eliminates the need to first dump to filesystem and then upload to AWS or other S3 service.</p>

<p>Without S3, one can use rsync or similar approach to securely copy your data to/from another location.</p>

<p>Checkpointing to S3 - even on-prem S3 - avoids copying and allows secure access to S3 from any cloud, so I would tend to prefer this approach. If you're willing to experiment with it, you could try creating checkpoints on <a href="/2023/03/16/aws-mountpoint-s3-eseries-solidfire.html">S3 mount points</a> mapped to an object store accessible from another location.</p>

<p>If you create more checkpoints than you want to upload to S3, then checkpoint to shared filesystem and selectively upload checkpoints to S3. You may take advantage of enriching those with user tags, Object Lock protection, and so on.</p>

<p>Checkpoints for jobs that may need to be continued elsewhere can be written to an S3 mount point and loaded in the cloud.</p>

<p><img src="/assets/images/pytorch-checkpoint-04.png" alt="" /></p>

<p>The same can work in the opposite direction (cloud-to-on-prem), too.</p>

<p>There are commercial options, too, such as scheduled replication jobs that NetApp Cloud Sync does, or asynchronous directory-level replication by Komprise.</p>

<h2 id="conclusion">Conclusion</h2>

<p>AI checkpointing is not very different from HPC checkpointing.</p>

<p>Because GPUs have less RAM than compute nodes, it may even be less of a challenge.</p>

<p>It appears no special considerations exist for PyTorch and E-Series. If E-Series is properly sized for read performance required, it is very likely it can also handle frequent checkpoints with ease.</p>

<h2 id="appendix-a---orbax">Appendix A - Orbax</h2>

<p>A colleague pointed out there's <a href="https://orbax.readthedocs.io/en/latest/orbax_checkpoint_api_overview.html">Orbax</a> for <a href="https://github.com/google/jax">JAX</a> users.</p>

<p>Example:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">my_tree</span> <span class="o">=</span> <span class="p">{</span>
<span class="p">...</span>     <span class="sh">'</span><span class="s">a</span><span class="sh">'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">8</span><span class="p">),</span>
<span class="p">...</span>     <span class="sh">'</span><span class="s">b</span><span class="sh">'</span><span class="p">:</span> <span class="p">{</span>
<span class="p">...</span>         <span class="sh">'</span><span class="s">c</span><span class="sh">'</span><span class="p">:</span> <span class="mi">42</span><span class="p">,</span>
<span class="p">...</span>         <span class="sh">'</span><span class="s">d</span><span class="sh">'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">16</span><span class="p">),</span>
<span class="p">...</span>     <span class="p">},</span>
<span class="p">...</span>     <span class="sh">'</span><span class="s">c</span><span class="sh">'</span><span class="p">:</span> <span class="p">{</span>
<span class="p">...</span>             <span class="sh">'</span><span class="s">x</span><span class="sh">'</span><span class="p">:</span> <span class="mi">3333333</span><span class="p">,</span>
<span class="p">...</span>             <span class="sh">'</span><span class="s">z</span><span class="sh">'</span><span class="p">:</span> <span class="mi">4444444</span><span class="p">,</span>
<span class="p">...</span>     <span class="p">}</span>
<span class="p">...</span> <span class="p">}</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">checkpointer</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="n">path</span> <span class="o">/</span> <span class="sh">'</span><span class="s">checkpoint_05</span><span class="sh">'</span><span class="p">,</span> <span class="n">my_tree</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">my_tree</span>
<span class="p">{</span><span class="sh">'</span><span class="s">a</span><span class="sh">'</span><span class="p">:</span> <span class="nf">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">]),</span> <span class="sh">'</span><span class="s">b</span><span class="sh">'</span><span class="p">:</span> <span class="p">{</span><span class="sh">'</span><span class="s">c</span><span class="sh">'</span><span class="p">:</span> <span class="mi">42</span><span class="p">,</span> <span class="sh">'</span><span class="s">d</span><span class="sh">'</span><span class="p">:</span> <span class="nf">array</span><span class="p">([</span> <span class="mi">0</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">4</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">15</span><span class="p">])},</span> <span class="sh">'</span><span class="s">c</span><span class="sh">'</span><span class="p">:</span> <span class="p">{</span><span class="sh">'</span><span class="s">x</span><span class="sh">'</span><span class="p">:</span> <span class="mi">3333333</span><span class="p">,</span> <span class="sh">'</span><span class="s">z</span><span class="sh">'</span><span class="p">:</span> <span class="mi">4444444</span><span class="p">}}</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">checkpointer</span><span class="p">.</span><span class="nf">restore</span><span class="p">(</span><span class="n">path</span> <span class="o">/</span> <span class="sh">'</span><span class="s">checkpoint_04/</span><span class="sh">'</span><span class="p">)</span>
<span class="p">{</span><span class="sh">'</span><span class="s">a</span><span class="sh">'</span><span class="p">:</span> <span class="nf">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">]),</span> <span class="sh">'</span><span class="s">b</span><span class="sh">'</span><span class="p">:</span> <span class="p">{</span><span class="sh">'</span><span class="s">c</span><span class="sh">'</span><span class="p">:</span> <span class="mi">42</span><span class="p">,</span> <span class="sh">'</span><span class="s">d</span><span class="sh">'</span><span class="p">:</span> <span class="nf">array</span><span class="p">([</span> <span class="mi">0</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">4</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">15</span><span class="p">])},</span> <span class="sh">'</span><span class="s">c</span><span class="sh">'</span><span class="p">:</span> <span class="p">{</span><span class="sh">'</span><span class="s">x</span><span class="sh">'</span><span class="p">:</span> <span class="mi">11111111</span><span class="p">,</span> <span class="sh">'</span><span class="s">z</span><span class="sh">'</span><span class="p">:</span> <span class="mi">2222222</span><span class="p">}}</span>
</code></pre></div></div>

<p>Similarly to PyTorch, checkpoint data is saved to a directory of your choice:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">ls</span> <span class="nt">-lat</span> ~/Temp/orbax-checkpoint/
total 64
drwxrwxr-x 8 sean sean  8 Feb  1 13:49 <span class="nb">.</span>
drwxrwxr-x 4 sean sean  8 Feb  1 13:49 checkpoint_05
drwxrwxr-x 4 sean sean  8 Feb  1 13:49 checkpoint_04
drwxrwxr-x 4 sean sean  8 Feb  1 13:43 checkpoint_03
drwxrwxr-x 4 sean sean  8 Feb  1 13:41 checkpoint_02
drwxrwxr-x 4 sean sean  8 Feb  1 13:40 checkpoint_01
drwxrwxr-x 9 sean sean 55 Feb  1 13:39 ..

<span class="nv">$ </span><span class="nb">ls</span> <span class="nt">-lat</span> ~/Temp/orbax-checkpoint/checkpoint_04/
total 55
drwxrwxr-x 8 sean sean   8 Feb  1 13:49 ..
drwxrwxr-x 4 sean sean   8 Feb  1 13:49 <span class="nb">.</span>
<span class="nt">-rw-rw-r--</span> 1 sean sean 144 Feb  1 13:49 manifest.ocdbt
drwxrwxr-x 2 sean sean   3 Feb  1 13:49 d
drwxrwxr-x 3 sean sean   5 Feb  1 13:49 ocdbt.process_0
<span class="nt">-rw-rw-r--</span> 1 sean sean 105 Feb  1 13:49 checkpoint
<span class="nt">-rw-rw-r--</span> 1 sean sean 829 Feb  1 13:49 _METADATA

<span class="nv">$ </span><span class="nb">ls</span> <span class="nt">-lat</span> ~/Temp/orbax-checkpoint/checkpoint_05/
total 55
drwxrwxr-x 4 sean sean   8 Feb  1 13:49 <span class="nb">.</span>
drwxrwxr-x 8 sean sean   8 Feb  1 13:49 ..
drwxrwxr-x 2 sean sean   3 Feb  1 13:49 d
<span class="nt">-rw-rw-r--</span> 1 sean sean 144 Feb  1 13:49 manifest.ocdbt
drwxrwxr-x 3 sean sean   5 Feb  1 13:49 ocdbt.process_0
<span class="nt">-rw-rw-r--</span> 1 sean sean 105 Feb  1 13:49 checkpoint
<span class="nt">-rw-rw-r--</span> 1 sean sean 829 Feb  1 13:49 _METADATA

</code></pre></div></div>

<p><img src="/assets/images/pytorch-checkpoint-pytree-06.png" alt="" /></p>

<ul>
  <li>1 - initial tree data</li>
  <li>2 - checkpoint number 04</li>
  <li>3 - update tree data</li>
  <li>4 - save updated checkpoint (number 05)</li>
  <li>5 - restore previous checkpoint number 04</li>
</ul>

<p>One thing I haven't tried is large tree structures to see if compression of checkpoints yields any savings. With tiny trees it's hard to say.</p>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/checkpoint" class="page__taxonomy-item p-category" rel="tag">checkpoint</a><span class="sep">, </span>
    
      <a href="/tags/dcp" class="page__taxonomy-item p-category" rel="tag">dcp</a><span class="sep">, </span>
    
      <a href="/tags/distributed" class="page__taxonomy-item p-category" rel="tag">distributed</a><span class="sep">, </span>
    
      <a href="/tags/eseries" class="page__taxonomy-item p-category" rel="tag">eseries</a><span class="sep">, </span>
    
      <a href="/tags/load" class="page__taxonomy-item p-category" rel="tag">load</a><span class="sep">, </span>
    
      <a href="/tags/pytorch" class="page__taxonomy-item p-category" rel="tag">pytorch</a><span class="sep">, </span>
    
      <a href="/tags/save" class="page__taxonomy-item p-category" rel="tag">save</a><span class="sep">, </span>
    
      <a href="/tags/torch" class="page__taxonomy-item p-category" rel="tag">torch</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/ai" class="page__taxonomy-item p-category" rel="tag">ai</a><span class="sep">, </span>
    
      <a href="/categories/storage" class="page__taxonomy-item p-category" rel="tag">storage</a>
    
    </span>
  </p>


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time class="dt-published" datetime="2024-01-10T00:00:00+08:00">2024-01-10 00:00</time></p>

      </footer>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://x.com/intent/tweet?via=scaleoutSean&text=PyTorch+checkpointing+workload+with+NetApp+E-Series%20https%3A%2F%2Fscaleoutsean.github.io%2F2024%2F01%2F10%2Fai-deep-learning-pytorch-checkpointing-eseries.html" class="btn btn--x" aria-label="Share on X" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on X">
    <i class="fab fa-fw fa-x-twitter" aria-hidden="true"></i><span> X</span>
  </a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://scaleoutsean.github.io/2024/01/10/ai-deep-learning-pytorch-checkpointing-eseries.html" class="btn btn--linkedin" aria-label="Share on LinkedIn" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn">
    <i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span>
  </a>

</section>


      
  <nav class="pagination">
    
      <a href="/2023/12/12/solidfire-unmap-hyper-v.html" class="pagination--pager" title="UNMAP/TRIM Hyper-V volumes backed by NetApp SolidFire">Previous</a>
    
    
      <a href="/2024/01/24/kubernetes-keda-netapp-solidfire-eseries.html" class="pagination--pager" title="Kubernetes KEDA with NetApp SolidFire and E-Series">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
    
<div class="page__related">
  
  <h2 class="page__related-title">You may also enjoy</h2>
  <div class="grid__wrapper">
    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/2026/03/01/adblock-andnetapp-docs.html" rel="permalink">Block external sites to improve loading speed of NetApp docs
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2026-03-01T00:00:00+08:00">2026-03-01 00:00</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          1 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Block external domains on https://docs.netapp.com
</p>
  </article>
</div>

    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/2026/02/26/ibm-block-storage-cis-driver-santricity-fork.html" rel="permalink">IBM Block Storage CSI driver patched for NetApp E-Series
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2026-02-26T00:00:00+08:00">2026-02-26 00:00</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          11 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Unofficial patch to IBM Block CSI driver for E-Series SANtricity storage
</p>
  </article>
</div>

    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/2026/02/19/the-shocking-truth-about-ef600-200g-ports.html" rel="permalink">Connecting to 200G HICs on E-Series EF600
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2026-02-19T00:00:00+08:00">2026-02-19 00:00</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          2 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Connecting to EF-Series EF600 200G HIC in DAS mode
</p>
  </article>
</div>

    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/2026/02/19/linux-nvme-roce-ef-series.html" rel="permalink">Linux, NVMe/RoCE and HA NetApp EF-Series NVMe-oF storage
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2026-02-19T00:00:00+08:00">2026-02-19 00:00</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          19 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Linux NVMe/RoCE clients with NetApp EF-Series NVMe-oF
</p>
  </article>
</div>

    
  </div>
</div>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        

<div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
          <li><a href="https://twitter.com/scaleoutsean" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/scaleoutsean" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>


<div class="page__footer-copyright">&copy; 2020-2026 scaleoutSean.github.io | <a href="https://scaleoutsean.github.io">Acting Technologist</a> | Terms: <a href="https://creativecommons.org/licenses/by/4.0/deed.en" target="_blank">CC BY 4.0</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>






  </body>
</html>
