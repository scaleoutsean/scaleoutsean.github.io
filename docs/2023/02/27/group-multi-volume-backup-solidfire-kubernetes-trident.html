<!doctype html>
<html lang="en">
  <head>
        <meta charset="UTF-8">
    <meta http-equiv="content-language" content="en">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="googlebot" content="noarchive">
    <meta name="googlebot" content="max-snippet:60">
    <meta name="googlebot" content="max-image-preview:small">
    <meta name="bingbot" content="noarchive">
    <meta name="bingbot" content="max-snippet:60">
    <meta name="bingbot" content="max-image-preview:small">
    <meta name="robots" content="noarchive">
    <meta name="robots" content="max-snippet:60">
    <meta name="google-site-verification" content="F6q7vIwQ2G0j8tk-KL9rAXOMLXMDMMUkEz4fRs1Nnnc" />
    <title>
        
            Protect multi-volume Kubernetes applications with NetApp Trident and SolidFire | Acting Technologist
      
    </title>
    <meta name="description" content="
     Snapshot, backup and restore multi-volume Kubernetes applications on NetApp SolidFire
     ">

    <!-- LINK TO ATOM FEED FOR SEO -->
    <link rel="alternate" type="application/atom+xml" href="https://scaleoutsean.github.io/feed.xml" />

    <!-- FAVICON -->
    <link rel="icon" href="/assets/favicon.ico" type="image/x-icon">
    <link rel="shortcut icon" href="/assets/favicon.ico" type="image/x-icon">

    <!-- CSS -->
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="stylesheet" href="/assets/css/syntax.css">

    <!-- Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css">

    <link href="/_pagefind/pagefind-ui.css" rel="stylesheet">
    <script src="/_pagefind/pagefind-ui.js" type="text/javascript"></script>
    <div id="search"></div>
    <script src="https://cdn.counter.dev/script.js" data-id="83dc700a-b821-4e57-9425-02b8336a9456" data-utcoffset="0"></script>
    <script>
        window.addEventListener('DOMContentLoaded', (event) => {
            new PagefindUI({ element: "#search" });
        });
    </script>

    <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Protect multi-volume Kubernetes applications with NetApp Trident and SolidFire | Acting Technologist</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Protect multi-volume Kubernetes applications with NetApp Trident and SolidFire" />
<meta name="author" content="scaleoutSean" />
<meta property="og:locale" content="en" />
<meta name="description" content="Snapshot, backup and restore multi-volume Kubernetes applications on NetApp SolidFire" />
<meta property="og:description" content="Snapshot, backup and restore multi-volume Kubernetes applications on NetApp SolidFire" />
<link rel="canonical" href="https://scaleoutsean.github.io/2023/02/27/group-multi-volume-backup-solidfire-kubernetes-trident.html" />
<meta property="og:url" content="https://scaleoutsean.github.io/2023/02/27/group-multi-volume-backup-solidfire-kubernetes-trident.html" />
<meta property="og:site_name" content="Acting Technologist" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-02-27T00:00:00+08:00" />
<script type="application/ld+json">
{"headline":"Protect multi-volume Kubernetes applications with NetApp Trident and SolidFire","dateModified":"2023-02-27T00:00:00+08:00","datePublished":"2023-02-27T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://scaleoutsean.github.io/2023/02/27/group-multi-volume-backup-solidfire-kubernetes-trident.html"},"author":{"@type":"Person","name":"scaleoutSean"},"@type":"BlogPosting","url":"https://scaleoutsean.github.io/2023/02/27/group-multi-volume-backup-solidfire-kubernetes-trident.html","description":"Snapshot, backup and restore multi-volume Kubernetes applications on NetApp SolidFire","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  </head>

<body>

  <div class="container">
    <header id="header">
	<div id = site_title>
		<a href="https://scaleoutsean.github.io/">
			<h2 style="font-size:1.8em;">Acting Technologist</h2>
		</a>
		
		<h3>
			civilizations are created by individuals
		</h3>
		
	</div>

	<div id="subheader">
		
		<nav class="pages">
<a href="/about.html">About</a>

<a href="/archive.html">Archive</a>

<a href="/categories/">Categories</a>

<a href="/projects.html">Projects</a>
</nav>
		
		
		<nav class="social">
			
  
    <a href="https://www.github.com/scaleoutsean" target="_blank" id="github"><i class="fab fa-github" aria-hidden="true"></i></a>
  
   

  
    <a href="https://twitter.com/scaleoutsean" target="_blank" id="twitter"><i class="fab fa-twitter" aria-hidden="true"></i></a>
  
   

  
  
    <a href="/feed.xml" target="_blank" id="rss"><i class="fas fa-rss" aria-hidden="true"></i></a>
   

		</nav>
		
	</div>
</header>


    <div class="post-container">
      <article id = "post">
        <h1 id = "post-title">Protect multi-volume Kubernetes applications with NetApp Trident and SolidFire</h1>

  
  <div class = "post-info">
    <span>
        <i class="far fa-calendar" aria-hidden="true"></i> <span>27 Feb 2023</span> - <i class="far fa-clock"></i> 


  
  
    14 minute read
  

    </span>
  </div>
  
        <ul>
  <li><a href="#introduction">Introduction</a></li>
  <li><a href="#cold-multi-volume-backup-without-snapshots">Cold multi-volume backup without snapshots</a></li>
  <li><a href="#group-snapshot-assisted-multi-volume-backup">Group Snapshot-assisted multi-volume backup</a>
    <ul>
      <li><a href="#backup-multi-volume-snapshot-to-s3-with-built-in-backup-to-s3">Backup multi-volume snapshot to S3 with built-in backup-to-S3</a></li>
      <li><a href="#using-other-methods-to-backup-data-from-storage-snapshots">Using other methods to backup data from storage snapshots</a></li>
    </ul>
  </li>
  <li><a href="#application-integrated-backups">Application-integrated backups</a></li>
  <li><a href="#application-consistent-snapshots-vs-multi-volume-storage-snapshots">Application-consistent snapshots vs. multi-volume storage snapshots</a>
    <ul>
      <li><a href="#injecting-storage-snapshot-into-application-aware-workflows">Injecting storage snapshot into application-aware workflows</a></li>
    </ul>
  </li>
  <li><a href="#sub-volume-partitioning-and-other-tricks">Sub-volume partitioning and other tricks</a></li>
  <li><a href="#conclusion">Conclusion</a></li>
</ul>

<h2 id="introduction">Introduction</h2>

<p>Recently I <a href="/2023/02/10/backup-epa-data-on-kubernetes.html">wrote</a> about the challenge of backing up multi-volume databases: in enterprise SAN environments that’s usually addressed by “consistency group” snapshots:</p>

<ul>
  <li>Freeze/suspend or stop application I/O</li>
  <li>Take a snapshot of all volumes involved (e.g. data volume, log volume)</li>
</ul>

<p>Then such snapshots can be mounted and backed up, or replicated to another location.</p>

<p>SolidFire supports consistency groups, but NetApp Trident (and maybe even Kubernetes CSI) doesn’t have a way of interacting with them.</p>

<p>We can’t, for example, tell Kubernetes “volumes 121, 122 and 125 belong to consistency group SQL01”.</p>

<p>We also can’t tell Kubernetes “backup volumes 121, 122 and 125 at the same time”. If snapshots are executed sequentially, or maybe not in a guaranteed order, results may be unpredictable.</p>

<p>And of course, because Kubernetes has no idea about the concept of volume consistency groups, even if we created one manually after volumes 121, 122 and 125 have been created, we couldn’t directly access it from Kubernetes.</p>

<h2 id="cold-multi-volume-backup-without-snapshots">Cold multi-volume backup without snapshots</h2>

<p>This isn’t specific to SolidFire - it should work the same way with other CSI storage when snapshots are taken without CSI - but one basic approach is to scale our deployment down to 0, i.e. stop the application, and then backup a single set of volumes.</p>

<p>That means a short downtime, of course.</p>

<p>To make that downtime shorter, we could <a href="/2020/11/28/powershell-set-sfqosexception.html">temporarily adjust</a> the volumes’ QoS setting to a high value such as 50,000 IOPS, and restore the original QoS setting or policy after backup is complete. A higher QoS limit would allow backup to work faster, making backup time shorter.</p>

<p>As far as backup utilities and applications are concerned, most of them support custom before/after actions.</p>

<p>Kanister - I’ve blogged about it before - has <a href="https://docs.kanister.io/functions.html#preparedata">functions</a> that can create such simple workflows.</p>

<blockquote>
  <p>The typical sequence is to stop the application using ScaleWorkload, perform the data manipulation using PrepareData, and then restart the application using ScaleWorkload.</p>
</blockquote>

<pre><code class="language-raw">- func: ScaleWorkload
  name: ShutdownApplication
  args:
    namespace: "{{ .Deployment.Namespace }}"
    name: "{{ .Deployment.Name }}"
    kind: deployment
    replicas: 0
- func: PrepareData
  name: ManipulateData
  args:
    namespace: "{{ .Deployment.Namespace }}"
    image: busybox
    volumes:
      application-pvc-1: "/data"
      application-pvc-2: "/logs"
      application-pvc-3: "/backup"
    command:
      - sh
      - -c
      - |
        rsync -av /data/ /backup/data
        rsync -av /logs/ /backup/logs
# Now backup PVC-3 (not shown) and scale up to 1
- func: ScaleWorkload
  name: StartApplication
  args:
    namespace: "{{ .Deployment.Namespace }}"
    name: "{{ .Deployment.Name }}"
    kind: deployment
    replicas: 1
</code></pre>

<p>We scale the deployment to zero, copy data and logs to the third volume and backup that third volume (mounted at /backup).</p>

<p>At (say) 500 MB/s, a 100 GB database could be backed up in under 5 minutes of scheduled downtime. That’s not too bad for small applications that don’t need to run 24x7.</p>

<p>After backup is done, we could delete all files in /backup/ because we use overwrite that directory every time we run that backup, and we don’t want to waste capacity. But:</p>

<ul>
  <li>in the case the rsync command is modified to overwrite only changed files, we’d save time that way and then it’d be useful to leave that data there</li>
  <li>some DBAs like to have a copy of their data handy (DB dump, not necessarily mounted)</li>
  <li>having a second copy of all DB data may deduplicate reasonably well and end up consuming very little data capacity</li>
</ul>

<p>So there are situations where we could leave that data there. And on this topic, I found <a href="https://docs.kasten.io/5.5.6/kanister/mssql/install.html#known-limitations">this</a> detail related to Kanister’s MS SQL blueprint:</p>

<blockquote>
  <p>the backup process in the Kanister Blueprint creates the temporary database backup files in the same volume as the database. Due to this, it is necessary to use a PVC at least twice the size of the database.</p>
</blockquote>

<p>So it wouldn’t be just your DIY backup that doesn’t make wasteful backups - others do it, too! At least an uncompressed rsync copy would’t take much extra space (whereas real dumps to disk don’t dedupe well against database data.)</p>

<h2 id="group-snapshot-assisted-multi-volume-backup">Group Snapshot-assisted multi-volume backup</h2>

<p>Now that we know how to use that function, we can sneak into it anything we want. Such as taking a snapshot of the entire volume group.</p>

<p>We could build an image with SolidFire CLI (or PowerShell, etc.) to do that. A spartan version could use curl. To use SolidFire CLI (sfcli), we’d need a Python container with <code class="language-plaintext highlighter-rouge">solidfire-cli</code> module and its dependencies.</p>

<pre><code class="language-raw">- func: PrepareData
  name: ManipulateData
  args:
    namespace: "{{ .Deployment.Namespace }}"
    # you'd have to create your own
    image: docker.io/scaleoutsean/solidfire-cli:latest
    volumes:
      application-pvc-1: "/data"
      application-pvc-2: "/logs"
    command:
      - sh
      - -c
      - |
        sfcli snapshot creategroup --volumes 121,122 -name sql01 --retention 02:00:00
</code></pre>

<p>That calls <code class="language-plaintext highlighter-rouge">CreateGroupSnapshot</code> method which creates a (consistency) group snapshot of volumes with IDs 121 and 122, and retains it for two hours.</p>

<p>We can scale a deployment down to zero (or just enter a hot backup mode in the application), take a snapshot of the volumes, and scale back to the original value. Now our downtime is very short - probably less than 30 seconds.</p>

<p>Downsides? The first is Kasten or Velero don’t know anything about SolidFire snapshots taken without Kubernetes’ <strong>CSI</strong> snapshotter. We’d have to make use of that storage snapshot on our own.</p>

<h3 id="backup-multi-volume-snapshot-to-s3-with-built-in-backup-to-s3">Backup multi-volume snapshot to S3 with built-in backup-to-S3</h3>

<p>We know this group snapshot’s name is <code class="language-plaintext highlighter-rouge">sql01</code>, but SolidFire may have several such group snapshots with different group snapshot IDs. But given that we keep such snapshots for 2 hours and users tend to give applications unique names, it’s unlikely that we’d actually find multiple snapshots with the same name.</p>

<p>To be on the safe side we’d use the most recent one. An even safer approach - maybe not necessary - would be to record the snapshot ID returned to <code class="language-plaintext highlighter-rouge">sfcli</code> and send a Web hook request to a scheduler to schedule a backup - maybe SolidFire’s built-in backup to S3 - with volume and snapshot IDs.</p>

<p>Thinking about this I realized I didn’t recall that SolidFire’s backup to S3 feature could refer to group snapshot IDs. Hmmm, that’s something we should check.</p>

<p>First I checked the backup-to-S3 API method - only snapshot IDs, not group snapshot IDs, were supported. Ouch.</p>

<p>Then I poked around the Web UI and realized - it took me only six or so years to figure this out - that a group snapshot’s “member snapshots” can be referenced by their ID. Wow!</p>

<p>As a reminder, this is how they can be created from the Web UI (“archive” screenshot taken some time ago):</p>

<p><img src="/assets/images/solidfire-group-snapshot.png" alt="Create SolidFire group snapshot" /></p>

<p>What this returns is a group snapshot ID. If we look for the constituent volume snapshots, they’re not there.</p>

<p><img src="/assets/images/solidfire-group-snapshots-01.png" alt="List SolidFire volume snapshots" /></p>

<p>Instead, group snapshots are listed in here, without any individual snapshot IDs:</p>

<p><img src="/assets/images/solidfire-group-snapshots-02.png" alt="List SolidFire group snapshots" /></p>

<p>I hadn’t noticed - since 2016 until now - that the constituent snapshots are listed here and that snapshot IDs are available.</p>

<p><img src="/assets/images/solidfire-group-snapshots-03.png" alt="List group group member snapshots" /></p>

<p>I mean, I did click on the link below <code class="language-plaintext highlighter-rouge">"# of volumes"</code> on that page, but didn’t notice that snapshot volume IDs were there. So, we can use the usual method to Backup to S3, after all!</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">solidfire.factory</span> <span class="kn">import</span> <span class="n">ElementFactory</span>
<span class="n">sfe</span> <span class="o">=</span> <span class="n">ElementFactory</span><span class="p">.</span><span class="n">create</span><span class="p">(</span><span class="s">'192.168.105.32'</span><span class="p">,</span> <span class="s">'admin'</span><span class="p">,</span> <span class="s">'*****************'</span><span class="p">,</span> <span class="n">verify_ssl</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">print_ascii_art</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="c1"># prepare params and run backup to S3 in the native format for volume ID 201, snapshot ID 324
</span><span class="n">sfe</span><span class="p">.</span><span class="n">start_bulk_volume_read</span><span class="p">(</span><span class="mi">201</span><span class="p">,</span><span class="s">"native"</span><span class="p">,</span> <span class="n">snapshot_id</span><span class="o">=</span><span class="mi">324</span><span class="p">,</span> <span class="n">script</span><span class="o">=</span><span class="s">"bv_internal.py"</span><span class="p">,</span> <span class="n">script_parameters</span><span class="o">=</span><span class="n">params</span><span class="p">)</span>
<span class="c1"># prepare params for volume ID 202, Snapshot ID 325
# ...
</span></code></pre></div></div>

<p>I tried to back-up these to S3 <a href="/2021/04/21/solidfire-backup-to-s3.html#backup-using-the-api-powershell-or-python">using the SolidFire API</a>, and it worked as one would expect from a “stand-alone” volume snapshot being backed up to S3.</p>

<p>If executed from a script, <code class="language-plaintext highlighter-rouge">"start_bulk_volume_read"</code> returns an async handle, key, and management IP of the SolidFire <em>node</em> (not cluster’s virtual management IP) running this backup job, like so:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">StartBulkVolumeReadResult</span><span class="p">(</span><span class="n">async_handle</span><span class="o">=</span><span class="mi">134</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="s">'b26292dc52425ac93c7f5d84a7c6fae3'</span><span class="p">,</span> <span class="n">url</span><span class="o">=</span><span class="s">'https://192.168.1.31:8443/'</span><span class="p">)</span>
</code></pre></div></div>

<p>We’d just need to watch for failures and retry (or alert) if any. For large backups (say, TiB-sized volumes) we’d need to retain group snapshots longer than one hour. If you want to retry without having to take another snapshot, set longer retention, say twice the time required to backup.</p>

<p>Each group snapshot can be deleted if backup completes, so that snapshots don’t linger longer than necessary. Although - if there are no other snapshots for short term-retention - it’s a good idea to keep the most recent one or two, just in case.</p>

<p>The second major downside of doing ad-hoc backups is that some backups are in Velero, Kasten or whatever else you use, and some elsewhere. That’s usually a bad idea from a governance and management perspective.</p>

<h3 id="using-other-methods-to-backup-data-from-storage-snapshots">Using other methods to backup data from storage snapshots</h3>

<p>Fortunately, it’s not mandatory to use SolidFire’s built-in Backup to S3 to transfer snapshot data out.</p>

<p>Once you have a snapshot, you can clone those volumes and do with them whatever you want.</p>

<p>We could create a new namespace (say, <code class="language-plaintext highlighter-rouge">backup</code>) and:</p>

<ul>
  <li>Use SolidFire CLI or API to clone those snapshots</li>
  <li>Use <code class="language-plaintext highlighter-rouge">tridentctl</code> to import cloned volumes and access them from a dummy pod, and</li>
  <li>Use Kasten or containerized <a href="/2023/09/03/solidbackup-with-kopia.html">Kopia</a> or Velero to make a backup (which would be consistent, considering there’s no application that’s using any of the data). You’d just backup one resource, the PV(C) itself. Since all of them can restore data to a different name space, any restore action could work directly to your application namespace.</li>
</ul>

<p>Because PVC backups <em>can</em> be restored to another namespace, this would still work reasonably well for many use cases. And all your snapshots and backups would be managed by the same application.</p>

<h2 id="application-integrated-backups">Application-integrated backups</h2>

<p>It is often forgotten - particularly among the “storage-focused IT people” - that many modern databases have decent CLIs and APIs that not only enable application-side snapshots, but also take care of database consistency, incremental backup and more.</p>

<p>An example of this can be found in <a href="/2021/09/09/kasten-v4-with-solidfire-logical-and-snapshot-assisted-data-protection.html">this post about logical backups in Kasten</a>. These are storage-independent and although they can be less efficient and consume more resources, they have their advantages.</p>

<p>In this case there’s no multi-volume storage snapshot, though. You’d simply backup your application to S3, for example. (You could also take a storage-assisted CSI snapshot, but wouldn’t backup data from snapshots - such snapshots could be rotated daily and serve as a quick way to recover data without restoring from S3, for example.)</p>

<h2 id="application-consistent-snapshots-vs-multi-volume-storage-snapshots">Application-consistent snapshots vs. multi-volume storage snapshots</h2>

<p>Application-side snapshots (MS SQL has them, for example) don’t employ storage snapshots. Such snapshots can often be accessed on “hot” database and used to perform a backup (see logical backups just above).</p>

<p>Scripts that put an application in a crash-consistent state (there’s a bunch of examples on the Internet, including Kanister, <a href="https://github.com/NetApp/Verda">Verda</a>, etc.) don’t deliver application backups, but prepare applications for a storage snapshot - in that sense they are more similar to <code class="language-plaintext highlighter-rouge">fsfreeze</code> (which freezes a filesystem), only - unlike <code class="language-plaintext highlighter-rouge">fsfreeze</code> - works on the application layer. Kasten/Kanister users can see the details <a href="https://docs.kasten.io/5.5.6/kanister/testing.html#application-consistent-backups">here</a>.</p>

<p><a href="/2024/03/23/velero-netapp-verda-scripts-and-trident.html">Here</a> you can see how NetApp Verda can be used with Velero to create application-consistent snapshots and backups.</p>

<h3 id="injecting-storage-snapshot-into-application-aware-workflows">Injecting storage snapshot into application-aware workflows</h3>

<p>Let’s say we have two volumes, DATA and LOG. Using <code class="language-plaintext highlighter-rouge">fsfreeze</code> we’d freeze both volumes, and take a storage snapshot which would give us a crash-consistent group snapshot.</p>

<p>When using a database-aware script, we’d lock the DB to prevent updates, maybe enter full-archive (log) mode (for an online snapshot or backup) or detach a DB (for an offline snapshot or backup), and optionally also run <code class="language-plaintext highlighter-rouge">fsfreeze</code> to prevent filesystem changes during that time. Then we would take an application-aware multi-volume snapshot, and return to normal online mode.</p>

<p>As <code class="language-plaintext highlighter-rouge">fsfreeze</code> requires privileged containers, it’s less likely that <code class="language-plaintext highlighter-rouge">fsfreeze</code> will grow in popularity. It may be possible to coordinate this from the worker - freeze from the worker, take a snapshot in Kubernetes - but this is complex and every time we’d need to find out which worker is hosting the pod(s), ensure the workers’ OS clocks are very precise, and potentially run jobs on multiple workers. I doubt that would work well!</p>

<p>Using Kanister, Kasten or <a href="/2024/03/22/velero-trident-backup-job-details.html">Velero snapshot hooks</a> we can take a multi-volume storage snapshot that’s independent of logical backup to S3, NFS or elsewhere. <a href="https://docs.kasten.io/5.5.6/kanister/postgresql/install_app_cons.html#application-consistent-postgresql-backup">This example</a> shows how to perform application-specific backup (with PostgreSQL, in this case). We could add a hook for storage snapshot that runs in addition to classic logical backup.</p>

<p>What that would do is create backups externally as it normally does, but also SolidFire group snapshots that Kanister/Kasten aren’t aware of. But they would still be very useful. Why? We’d know that each such backup has an application-aware backup, but also a hardware-based multi-volume snapshot (with retention of say 24 hours). To restore data, we could use whichever is available and better:</p>

<ul>
  <li>If our application was deleted by mistake (and with it our storage snapshots as well), we’d restore data from S3/NFS/etc.</li>
  <li>If we mistakenly deleted a table, we’d scale our deployment to zero, use the SolidFire API or CLI or Web UI to revert the group snapshot.</li>
</ul>

<p>This isn’t perfect - we still need to use a separate interface to restore a snapshot, but this step could be <a href="https://docs.kasten.io/5.5.6/kanister/hooks.html">included in pre-restore hooks</a> - that would make it easier to execute it, but it wouldn’t make it easier to switch between approaches (restore from an exported backup, or from a storage multi-volume snapshot).</p>

<h2 id="sub-volume-partitioning-and-other-tricks">Sub-volume partitioning and other tricks</h2>

<p>One common trick for the lack of consistency group snapshot support is to partition a volume, so that we have something like this:</p>

<ul>
  <li>0 to 512MiB - data</li>
  <li>512MiB+1byte to 1,024 MiB - logs</li>
</ul>

<p>In the case of 4kiB blocks (SolidFire LUNs), 0-131072 blocks (512MiB) would be the data partition.</p>

<p>Then you have two filesystems on the same disk (or “LUN”), and a single snapshot would be done without multi-volume considerations.</p>

<p>And we could try to modify the range values in SolidFire’s backup to S3 call (262144 x 4KiB = 1GiB).</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"id"</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w">
  </span><span class="nl">"method"</span><span class="p">:</span><span class="w"> </span><span class="s2">"StartBulkVolumeRead"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"params"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"volumeID"</span><span class="p">:</span><span class="w"> </span><span class="mi">201</span><span class="p">,</span><span class="w">
    </span><span class="nl">"format"</span><span class="p">:</span><span class="w"> </span><span class="s2">"native"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"snapshotID"</span><span class="p">:</span><span class="w"> </span><span class="mi">324</span><span class="p">,</span><span class="w">
    </span><span class="nl">"script"</span><span class="p">:</span><span class="w"> </span><span class="s2">"bv_internal.py"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"scriptParameters"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
      </span><span class="nl">"range"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="nl">"lba"</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w">
        </span><span class="nl">"blocks"</span><span class="p">:</span><span class="w"> </span><span class="mi">262144</span><span class="w">
        </span><span class="p">}</span><span class="w">
        </span><span class="p">}</span><span class="w">
    </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>For example, <code class="language-plaintext highlighter-rouge">lba: 0, blocks: 131072</code> would read the first 512MiB, and (maybe, I haven’t tried), <code class="language-plaintext highlighter-rouge">lba: 1, blocks: 131072</code> would read the last 512 MiB.</p>

<p>But even if that worked for backup, Trident CSI can only use non-partitioned volumes, so that’s out of the question and that trick from physical and VM world doesn’t look viable with Kubernetes.</p>

<p>Other CSI drivers - such as <a href="/2022/12/09/directpv-topolvm-csi-lvm-das-k8s-with-eseries.html">TopoLVM</a> - use LVM so may be able to snapshot multiple volumes allocated on the same worker as a group, but still wouldn’t work for volumes allocated from different physical disks (for example, a NOSQL deployment with pods on three workers, each using a different LVM group.)</p>

<h2 id="conclusion">Conclusion</h2>

<p>When application data spans multiple volumes, snapshots must consider the effect of such data layout and data-to-storage mapping.</p>

<p>Merely hoping that crash-consistent application snapshot will work may not be enough to guarantee a trouble-free restore: yes, data would be crash-consistent, but snapshots may be taken with millisecond delays potentially resulting in inconsistencies among volumes.</p>

<p>Kanister - and by extension Kasten - can be used to create application-consistent cold backups at the cost of some downtime.</p>

<p>SolidFire group snapshots can be used in pre-backup scripts, but we should check if the application (and potentially the filesystem) can be quiesced/frozen, so that these snapshots and their backups can be restored without a problem.</p>

      </article>
    </div>
    
      <div class="categories">
    <span><p>Categories:</p>
    
    
      <a href="
      /categories/#kubernetes">kubernetes</a>
      &nbsp; 
    
      <a href="
      /categories/#storage">storage</a>
       
    
  </span>
</div>
    

    
      <div class="related" data-pagefind-ignore>

    <h4>Possibly related - use live search at the top to find other content</h4>
    
    
    
    
    
    
    
        
        
    
        
    
        
          <div class = "related-posts">
          <h5><a href="/2024/04/23/econfig-update.html">• New EConfig tools</a></h5>
          </div>
          
          
        
    
      
    
        
        
    
        
    
        
          <div class = "related-posts">
          <h5><a href="/2024/04/11/netapp-eseries-containerized-beegfs-nfs-s3-all-in-one.html">• NetApp E-Series with containerized BeeGFS, NFS, S3</a></h5>
          </div>
          
          
        
    
      
    
        
        
    
        
    
        
          <div class = "related-posts">
          <h5><a href="/2024/04/09/solidbackup-velero-backup-non-k8s-volumes-netapp-solidfire-to-s3.html">• Backup NetApp SolidFire's non-Kubernetes volumes with Velero</a></h5>
          </div>
          
          
        
    
      
    
        
        
    
        
    
        
          <div class = "related-posts">
          <h5><a href="/2024/04/02/storagegrid-networking.html">• NetApp StorageGRID networks</a></h5>
          </div>
          
          
        
    
      
    
        
        
    
        
    
        
          <div class = "related-posts">
          <h5><a href="/2024/04/01/windows-server-2025-with-solidfire-part-two-sql-server-2022.html">• Windows Server 2025 with NetApp SolidFire 12 iSCSI Part Two</a></h5>
          </div>
          
          
            
    
    </div>

    

    
  </div><footer class= "footer">
    <p>2024-04-23 03:13 </p>
    <p>Copyright © 2024 scaleoutSean. Content is released under the CC BY license. Design: Alessio Franceschi</p>
</footer>

</body>
</html>
