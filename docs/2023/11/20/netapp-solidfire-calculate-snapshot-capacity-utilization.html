<!doctype html>
<html lang="en">
  <head>
        <meta charset="UTF-8">
    <meta http-equiv="content-language" content="en">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="googlebot" content="noarchive">
    <meta name="googlebot" content="max-snippet:60">
    <meta name="googlebot" content="max-image-preview:small">
    <meta name="bingbot" content="noarchive">
    <meta name="bingbot" content="max-snippet:60">
    <meta name="bingbot" content="max-image-preview:small">
    <meta name="robots" content="noarchive">
    <meta name="robots" content="max-snippet:60">
    <meta name="google-site-verification" content="F6q7vIwQ2G0j8tk-KL9rAXOMLXMDMMUkEz4fRs1Nnnc" />
    <title>
        
            Calculate snapshot capacity utilization on NetApp SolidFire | Acting Technologist
      
    </title>
    <meta name="description" content="
     How to calculate snapshot capacity for a storage account on NetApp SolidFire
     ">

    <!-- LINK TO ATOM FEED FOR SEO -->
    <link rel="alternate" type="application/atom+xml" href="https://scaleoutsean.github.io/feed.xml" />

    <!-- FAVICON -->
    <link rel="icon" href="/assets/favicon.ico" type="image/x-icon">
    <link rel="shortcut icon" href="/assets/favicon.ico" type="image/x-icon">

    <!-- CSS -->
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="stylesheet" href="/assets/css/syntax.css">

    <!-- Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css">

    <link href="/_pagefind/pagefind-ui.css" rel="stylesheet">
    <script src="/_pagefind/pagefind-ui.js" type="text/javascript"></script>
    <div id="search"></div>
    <script src="https://cdn.counter.dev/script.js" data-id="83dc700a-b821-4e57-9425-02b8336a9456" data-utcoffset="0"></script>
    <script>
        window.addEventListener('DOMContentLoaded', (event) => {
            new PagefindUI({ element: "#search" });
        });
    </script>

    <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Calculate snapshot capacity utilization on NetApp SolidFire | Acting Technologist</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Calculate snapshot capacity utilization on NetApp SolidFire" />
<meta name="author" content="scaleoutSean" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="How to calculate snapshot capacity for a storage account on NetApp SolidFire" />
<meta property="og:description" content="How to calculate snapshot capacity for a storage account on NetApp SolidFire" />
<link rel="canonical" href="https://scaleoutsean.github.io/2023/11/20/netapp-solidfire-calculate-snapshot-capacity-utilization.html" />
<meta property="og:url" content="https://scaleoutsean.github.io/2023/11/20/netapp-solidfire-calculate-snapshot-capacity-utilization.html" />
<meta property="og:site_name" content="Acting Technologist" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-11-20T00:00:00+08:00" />
<script type="application/ld+json">
{"headline":"Calculate snapshot capacity utilization on NetApp SolidFire","dateModified":"2023-11-20T00:00:00+08:00","datePublished":"2023-11-20T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://scaleoutsean.github.io/2023/11/20/netapp-solidfire-calculate-snapshot-capacity-utilization.html"},"author":{"@type":"Person","name":"scaleoutSean"},"@type":"BlogPosting","url":"https://scaleoutsean.github.io/2023/11/20/netapp-solidfire-calculate-snapshot-capacity-utilization.html","description":"How to calculate snapshot capacity for a storage account on NetApp SolidFire","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  </head>

<body>

  <div class="container">
    <header id="header">
	<div id = site_title>
		<a href="https://scaleoutsean.github.io/">
			<h2 style="font-size:1.8em;">Acting Technologist</h2>
		</a>
		
		<h3>
			civilizations are created by individuals
		</h3>
		
	</div>

	<div id="subheader">
		
		<nav class="pages">
<a href="/about.html">About</a>

<a href="/archive.html">Archive</a>

<a href="/categories/">Categories</a>

<a href="/projects.html">Projects</a>
</nav>
		
		
		<nav class="social">
			
  
    <a href="https://www.github.com/scaleoutsean" target="_blank" id="github"><i class="fab fa-github" aria-hidden="true"></i></a>
  
   

  
    <a href="https://twitter.com/scaleoutsean" target="_blank" id="twitter"><i class="fab fa-twitter" aria-hidden="true"></i></a>
  
   

  
  
    <a href="/feed.xml" target="_blank" id="rss"><i class="fas fa-rss" aria-hidden="true"></i></a>
   

		</nav>
		
	</div>
</header>


    <div class="post-container">
      <article id = "post">
        <h1 id = "post-title">Calculate snapshot capacity utilization on NetApp SolidFire</h1>

  
  <div class = "post-info">
    <span>
        <i class="far fa-calendar" aria-hidden="true"></i> <span>20 Nov 2023</span> - <i class="far fa-clock"></i> 


  
  
    16 minute read
  

    </span>
  </div>
  
        <ul>
  <li><a href="#introduction">Introduction</a></li>
  <li><a href="#clone-and-analyze">Clone and analyze</a></li>
  <li><a href="#example-with-postgres">Example with Postgres</a></li>
  <li><a href="#writebytes-metric">writeBytes metric</a></li>
  <li><a href="#cost-issue">Cost issue</a></li>
  <li><a href="#do-we-still-need-to-clone-those-snapshots">Do we still need to clone those snapshots?</a></li>
  <li><a href="#how-to-minimize-storage-utilization-by-snapshots">How to minimize storage utilization by snapshots</a></li>
  <li><a href="#conclusion">Conclusion</a></li>
</ul>

<h2 id="introduction">Introduction</h2>

<p>Today I came up with an idea on how to calculate snapshot capacity utilization for a storage account on SolidFire.</p>

<p>As a reminder: each SolidFire storage account may have one or more volumes, and each volume may have one or more snapshots.</p>

<p>Problem is, there’s no per-account accounting for capacity occupied by snapshot data.</p>

<p>At most, one can call <code class="language-plaintext highlighter-rouge">GetClusterCapacity</code> and get something like this (this is a demo VM, so “cluster” capacity is small):</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
    </span><span class="nl">"id"</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w">
    </span><span class="nl">"result"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="nl">"clusterCapacity"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
            </span><span class="nl">"activeBlockSpace"</span><span class="p">:</span><span class="w"> </span><span class="mi">12273615988</span><span class="p">,</span><span class="w">
            </span><span class="nl">"activeSessions"</span><span class="p">:</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w">
            </span><span class="nl">"averageIOPS"</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w">
            </span><span class="nl">"clusterRecentIOSize"</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w">
            </span><span class="nl">"currentIOPS"</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w">
            </span><span class="nl">"maxIOPS"</span><span class="p">:</span><span class="w"> </span><span class="mi">3000</span><span class="p">,</span><span class="w">
            </span><span class="nl">"maxOverProvisionableSpace"</span><span class="p">:</span><span class="w"> </span><span class="mi">18554258718720</span><span class="p">,</span><span class="w">
            </span><span class="nl">"maxProvisionedSpace"</span><span class="p">:</span><span class="w"> </span><span class="mi">3710851743744</span><span class="p">,</span><span class="w">
            </span><span class="nl">"maxUsedMetadataSpace"</span><span class="p">:</span><span class="w"> </span><span class="mi">14495514624</span><span class="p">,</span><span class="w">
            </span><span class="nl">"maxUsedSpace"</span><span class="p">:</span><span class="w"> </span><span class="mi">161061273600</span><span class="p">,</span><span class="w">
            </span><span class="nl">"nonZeroBlocks"</span><span class="p">:</span><span class="w"> </span><span class="mi">2277952</span><span class="p">,</span><span class="w">
            </span><span class="nl">"peakActiveSessions"</span><span class="p">:</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w">
            </span><span class="nl">"peakIOPS"</span><span class="p">:</span><span class="w"> </span><span class="mi">6</span><span class="p">,</span><span class="w">
            </span><span class="nl">"provisionedSpace"</span><span class="p">:</span><span class="w"> </span><span class="mi">27183284224</span><span class="p">,</span><span class="w">
            </span><span class="nl">"snapshotNonZeroBlocks"</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w">
            </span><span class="nl">"timestamp"</span><span class="p">:</span><span class="w"> </span><span class="s2">"2023-11-20T11:30:20Z"</span><span class="p">,</span><span class="w">
            </span><span class="nl">"totalOps"</span><span class="p">:</span><span class="w"> </span><span class="mi">922097</span><span class="p">,</span><span class="w">
            </span><span class="nl">"uniqueBlocks"</span><span class="p">:</span><span class="w"> </span><span class="mi">1594454</span><span class="p">,</span><span class="w">
            </span><span class="nl">"uniqueBlocksUsedSpace"</span><span class="p">:</span><span class="w"> </span><span class="mi">5202512500</span><span class="p">,</span><span class="w">
            </span><span class="nl">"usedMetadataSpace"</span><span class="p">:</span><span class="w"> </span><span class="mi">39579648</span><span class="p">,</span><span class="w">
            </span><span class="nl">"usedMetadataSpaceInSnapshots"</span><span class="p">:</span><span class="w"> </span><span class="mi">38637568</span><span class="p">,</span><span class="w">
            </span><span class="nl">"usedSpace"</span><span class="p">:</span><span class="w"> </span><span class="mi">5248927378</span><span class="p">,</span><span class="w">
            </span><span class="nl">"zeroBlocks"</span><span class="p">:</span><span class="w"> </span><span class="mi">2404800</span><span class="w">
        </span><span class="p">}</span><span class="w">
    </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>We can see <code class="language-plaintext highlighter-rouge">snapshotNonZeroBlocks=0</code>. <code class="language-plaintext highlighter-rouge">usedMetadataSpaceInSnapshots</code> refers to data in metadata tables, which is not of concern to us now (except the general idea that we don’t want to run out of metadata capacity in the cluster).</p>

<p>According to <a href="https://docs.netapp.com/us-en/element-software/api/reference_element_api_getclustercapacity.html">SolidFire documentation</a>, it appears snapshotNonZeroBlocks are logical blocks, i.e. “before efficiency”:</p>

<pre><code class="language-raw">deDuplicationFactor = (nonZeroBlocks + snapshotNonZeroBlocks) / uniqueBlocks
</code></pre>

<p>If I had any snapshots at this time and left them in place, the next time Garbage Collection runs it’d come up with a positive integer number.</p>

<p>But the moment you have different accounts, or even different users (say, several departments using one VMware cluster), you’re stuck. There’s no way to easily tell who’s using how much capacity for their snapshots.</p>

<p>SolidFire didn’t implement that either before or after the acquisition.</p>

<p>We can’t come up with something out of nothing, so if the method isn’t there, I have no way to make it appear.</p>

<p>But today - after so many years of using SolidFire - I came up with not just one, but two related ideas - maybe it’s because I’ve been using the SolidFire API in recent days.</p>

<h2 id="clone-and-analyze">Clone and analyze</h2>

<p>This is the first idea, which involves a workaround that’s more practical (because, unlike <code class="language-plaintext highlighter-rouge">snapshotNonZeroBlocks</code>, it works with many accounts), but also more “expensive”.</p>

<p>Let’s say we’re interested in the account s198. This guy happens to have one volume called “data”. It’s 2 GB large and there are 4 snapshots of it.</p>

<p><img src="/assets/images/solidfire-snapshot-capacity-utilization-00.png" alt="Storage account of interest" /></p>

<p>I create a utility storage account, say “accountant”, and note its account ID (in my case, 9).</p>

<p>I create clones of s198’s four snapshots and assign them to this new account.</p>

<p>Now I can use <code class="language-plaintext highlighter-rouge">GetAccountEfficiency</code> on the account ID of “accountant”. Here we can see the four snapshots which we’ll clone for accountant.</p>

<p><img src="/assets/images/solidfire-snapshot-capacity-utilization-01.png" alt="Storage account of interest" /></p>

<p>If we want to automate, we’d use <code class="language-plaintext highlighter-rouge">ListAccounts</code> to see volume IDs resulting from creating clones from snapshots.</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
    </span><span class="nl">"accountID"</span><span class="p">:</span><span class="w"> </span><span class="mi">9</span><span class="p">,</span><span class="w">
    </span><span class="nl">"attributes"</span><span class="p">:</span><span class="w"> </span><span class="p">{},</span><span class="w">
    </span><span class="nl">"enableChap"</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span><span class="w">
    </span><span class="nl">"initiatorSecret"</span><span class="p">:</span><span class="w"> </span><span class="s2">"testtesttest"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"status"</span><span class="p">:</span><span class="w"> </span><span class="s2">"active"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"storageContainerID"</span><span class="p">:</span><span class="w"> </span><span class="s2">"00000000-0000-0000-0000-000000000000"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"targetSecret"</span><span class="p">:</span><span class="w"> </span><span class="s2">"d3adB33F"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"username"</span><span class="p">:</span><span class="w"> </span><span class="s2">"accountant"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"volumes"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
        </span><span class="mi">65</span><span class="p">,</span><span class="w">
        </span><span class="mi">66</span><span class="p">,</span><span class="w">
        </span><span class="mi">67</span><span class="p">,</span><span class="w">
        </span><span class="mi">69</span><span class="w">
    </span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>I sit tight until top of the hour for Garbage Collection.</p>

<p>How the snapshots were created:</p>
<ul>
  <li>Snapshot 1 was taken after 50 MB was written to a file on the original volume belonging to s198</li>
  <li>Snapshot 2 was taken after 50 MB was appended to the first file (making it 100 MB large)</li>
  <li>Snapshot 3 was taken after 3 identical 50 MB files were copied to the filesystem</li>
  <li>Snapshot 4 was taken after the 100 MB file was deleted, and discard allowed to run</li>
</ul>

<p>The same thing in a chart. All files were “storage-efficient” with both dedupe and compression factors greater than 1.</p>

<table>
  <thead>
    <tr>
      <th>State</th>
      <th style="text-align: right">Delta (MB)</th>
      <th style="text-align: left">Comment</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Create 50 MB</td>
      <td style="text-align: right">-</td>
      <td style="text-align: left">Snapshot 1 (Clone 1) - 1st file, 50 MB</td>
    </tr>
    <tr>
      <td>Append 50 MB</td>
      <td style="text-align: right">+50</td>
      <td style="text-align: left">Snapshot 2 (Clone 2) - enlarged first file now 100 MB</td>
    </tr>
    <tr>
      <td>Create 3 x 50 MB</td>
      <td style="text-align: right">+150</td>
      <td style="text-align: left">Snapshot 3 (Clone 3) - 3 identical 50 MB files added</td>
    </tr>
    <tr>
      <td>Delete 100 MB file</td>
      <td style="text-align: right">-100</td>
      <td style="text-align: left">Snapshot 4 (Clone 4) - 100 MB file deleted, 3 50MB files left</td>
    </tr>
  </tbody>
</table>

<p>After GC is done running, we do two things: re-check cluster capacity, and check account efficiency for “accountant”. Again, all files were storage-efficient.</p>

<p>First, <code class="language-plaintext highlighter-rouge">GetClusterCapacity</code> now says <code class="language-plaintext highlighter-rouge">snapshotNonZeroBlocks=27693</code>. That is 110 MB occupied by snapshots (before SolidFire efficiencies).</p>

<p>Second, let’s check <code class="language-plaintext highlighter-rouge">GetAccountEfficiency</code> for account ID 9 (“accountant”).</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
    </span><span class="nl">"id"</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w">
    </span><span class="nl">"result"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="nl">"compression"</span><span class="p">:</span><span class="w"> </span><span class="mf">1.952278156703324</span><span class="p">,</span><span class="w">
        </span><span class="nl">"deduplication"</span><span class="p">:</span><span class="w"> </span><span class="mf">7.432667245873154</span><span class="p">,</span><span class="w">
        </span><span class="nl">"missingVolumes"</span><span class="p">:</span><span class="w"> </span><span class="p">[],</span><span class="w">
        </span><span class="nl">"thinProvisioning"</span><span class="p">:</span><span class="w"> </span><span class="mf">14.27334093100728</span><span class="p">,</span><span class="w">
        </span><span class="nl">"timestamp"</span><span class="p">:</span><span class="w"> </span><span class="s2">"2023-11-20T12:00:01Z"</span><span class="w">
    </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>14.27x (1.95 x 7.43) is a lot, but we have 4 snapshot-derived volumes with overlapping data. Logical data sizes of clone volumes is:</p>
<ul>
  <li>50 MB (first file)</li>
  <li>100 MB (50 MB appended)</li>
  <li>250 MB (3 50MB files added, one 100 MB file already present)</li>
  <li>150 MB (the 100 MB file was deleted)</li>
  <li>TOTAL: 50+100+250+150 = 550 MB</li>
</ul>

<p>Time progression with a total that reflects clones’ size (excluding the original volume that belongs to the user s198):</p>

<table>
  <thead>
    <tr>
      <th>File</th>
      <th style="text-align: right">t0 (orig)</th>
      <th style="text-align: right">snap1</th>
      <th style="text-align: right">snap2</th>
      <th style="text-align: right">snap3</th>
      <th style="text-align: right">snap4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>data</td>
      <td style="text-align: right">50</td>
      <td style="text-align: right">50</td>
      <td style="text-align: right">100</td>
      <td style="text-align: right">100</td>
      <td style="text-align: right">0</td>
    </tr>
    <tr>
      <td>data-3</td>
      <td style="text-align: right">-</td>
      <td style="text-align: right">-</td>
      <td style="text-align: right"> </td>
      <td style="text-align: right">50</td>
      <td style="text-align: right">50</td>
    </tr>
    <tr>
      <td>data-4</td>
      <td style="text-align: right">-</td>
      <td style="text-align: right">-</td>
      <td style="text-align: right"> </td>
      <td style="text-align: right">50</td>
      <td style="text-align: right">50</td>
    </tr>
    <tr>
      <td>data-5</td>
      <td style="text-align: right">-</td>
      <td style="text-align: right">-</td>
      <td style="text-align: right"> </td>
      <td style="text-align: right">50</td>
      <td style="text-align: right">50</td>
    </tr>
    <tr>
      <td>EACH CLONE</td>
      <td style="text-align: right">-</td>
      <td style="text-align: right">50</td>
      <td style="text-align: right">100</td>
      <td style="text-align: right">250</td>
      <td style="text-align: right">150</td>
    </tr>
  </tbody>
</table>

<p>Total fulness of all 4 clones added together is 28% of 2 GB which is 560 MB, which matches our notes (550 MB plus some filesystem overheads).</p>

<p>Efficiency based on account efficiency report from the SolidFire API and UI is <strong>14.27x</strong>. That’s credible, because the original volume also has a high efficiency and we know files are storage-efficient. Four clones with overlapping contents could be reduced to 1/14-th of its original size.</p>

<p>I <em>think</em> we can say that the four clone volumes use (560 MB / 14.27x = 38.5 MB) on disk before SolidFire’s RF2.</p>

<p>I’m not 100% sure of that. I should have used a more deterministic approach and my three files (the first 50 MB file, the appended 50 MB file and the one I copied three times) weren’t uniform in their data efficiency.</p>

<h2 id="example-with-postgres">Example with Postgres</h2>

<p>I tried another scenario and approach, with a workload that’s easier to relate to.</p>

<ul>
  <li>Create a 2 GB volume for s198, format it with XFS, initialize a PostgreSQL database on it (around 70 MB of data after initialization)</li>
</ul>

<p><img src="/assets/images/solidfire-snapshot-capacity-utilization-03-postgres-initial-72mb.png" alt="" /></p>

<ul>
  <li>Start a low-intensity PostgreSQL workload (3tps, ~64 kB/s), with data and log on the same 2 GB volume. Take the first snapshot after a GC run</li>
  <li>Create a discard loop for the volume shortly before each snapshot and GC run</li>
  <li>Create an hourly snapshot schedule to run until after workload stops 3 hours later</li>
</ul>

<p><img src="/assets/images/solidfire-snapshot-capacity-utilization-04-postgres-snapshots.png" alt="" /></p>

<ul>
  <li>Then repeat the earlier comparison - create 3 more clones and check data efficiency for the account “accountant”</li>
</ul>

<p><img src="/assets/images/solidfire-snapshot-capacity-utilization-05-postgres-clones.png" alt="" /></p>

<p>This screenshot above shows that initially - right after the first snapshot was taken, but the first discard hadn’t executed until just before snapshot #2 was taken - fulness was 6.27% because of that. Later discard was executed each time so snapshot #2 and the subsequent two (and clones created from these three snapshots) closely resembled the size of original volume.</p>

<p>Data efficiency of the original volume:</p>

<p><img src="/assets/images/solidfire-snapshot-capacity-utilization-07-original-efficency.png" alt="" /></p>

<p>Data efficiency of the clones:</p>

<p><img src="/assets/images/solidfire-snapshot-capacity-utilization-06-accountant-efficency.png" alt="" /></p>

<p>To check if the original volume impacts data efficiency of clone volumes, I deleted all snapshots and the original volume (from the user s198).</p>

<p>Data efficiency of the clones after the original volume was gone and GC executed was unchanged, so we know it’s not impacted by presence or absence of the original data. Note, however, that we couldn’t delete the original volume <em>and</em> leave snapshots in place - the snapshots were deleted as well. This “test” is possible only because the snapshots were cloned before we deleted original volume and its snapshots.</p>

<p><img src="/assets/images/solidfire-snapshot-capacity-utilization-08-clones-only-efficency.png" alt="" /></p>

<p>Summary:</p>

<ul>
  <li>Efficiency of snapshot-derived clones was (4.92 x 1.63 = 8.01x), closely resembling the efficiency of the source volume (4.51 x 1.30 = 5.86x)</li>
  <li>The source volume had 72 MB of data after database was initialized (measured with <code class="language-plaintext highlighter-rouge">du -sh /mnt/data</code>) and 95 MB at the end, so only 23 MB was added over 3-4 hours (as tables and log files were overwritten)</li>
  <li>The four snapshots reportedly (snapshotNonZeroBlocks) occupied 68976 4kB blocks, or 269 MB</li>
</ul>

<p>If each of the four clones was almost completely overwritten (70 MB x 4 = 280 MB), each clone would have very different data. But internally since DB rows were mostly integers, SolidFire was quite successful deduplicating their blocks, resulting in a high efficiency ratio of 8x.</p>

<p>The fulness of all clones is (4.58 + 4.5 + 4.43 + 6.27 = 19.78%) of 2000 MB, i.e. 395 MB.</p>

<p>If we assume that snapshotNonZeroBlocks are pre-efficiency, they would occupy (269 MB / 8.01 = 33 MB) on disk (pre-RF2).</p>

<p>The part that doesn’t make complete sense is that the total size of the clones (derived from snapshots) is 395 MB, except if they had some 30% of blocks in common. But snapshotNonZeroBlocks are 269 MB and supposed to be pre-efficiency, so that shouldn’t matter.</p>

<p>So, I don’t think I fully understand how the maths works here, but whether we take 395 MB or 269 MB before dividing it by 8.01x, disk space utilization (before RF2) would be between 38 and 49 MB.</p>

<p>We need to remember that the 269 MB figure (snapshotNonZeroBlocks) is the total for all accounts, so we wouldn’t even know it for any individual account in a situation where there are multiple storage accounts with snapshots. Hence our only choice is to use the 395 MB figure by adding up fulness of each snapshot-derived clone.</p>

<p>Given that total data size before and after were 72 MB and 95 MB (23 MB added), 40-50 MB in (usable) snapshot capacity is a lot.</p>

<p>At scale (suppose 230 GB data growth with 400 GB occupied by snapshots) that would definitively be worth knowing.</p>

<h2 id="writebytes-metric">writeBytes metric</h2>

<p>The second idea I got wasn’t another way to estimate capacity used by snapshots, but something that can help us identify write-intensive volumes.</p>

<p>As mentioned earlier, one way is to look at a performance chart with volumes and see which volume has a relatively (considering its size and fulness) “high” write workload.</p>

<p>If we want to automate this, we can look at some average or mean, but we can also use the API to compare two points in time:</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="nl">"method"</span><span class="p">:</span><span class="w"> </span><span class="s2">"GetVolumeStats"</span><span class="p">,</span><span class="w"> </span><span class="nl">"id"</span><span class="p">:</span><span class="w"> </span><span class="mi">7</span><span class="p">,</span><span class="w"> </span><span class="nl">"params"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="nl">"volumeID"</span><span class="p">:</span><span class="w"> </span><span class="mi">63</span><span class="p">}}</span><span class="w">
</span></code></pre></div></div>

<p>Response:</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
    </span><span class="nl">"id"</span><span class="p">:</span><span class="w"> </span><span class="mi">7</span><span class="p">,</span><span class="w">
    </span><span class="nl">"result"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="nl">"volumeStats"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
            </span><span class="nl">"accountID"</span><span class="p">:</span><span class="w"> </span><span class="mi">7</span><span class="p">,</span><span class="w">
            </span><span class="nl">"actualIOPS"</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w">
            </span><span class="nl">"asyncDelay"</span><span class="p">:</span><span class="w"> </span><span class="kc">null</span><span class="p">,</span><span class="w">
            </span><span class="nl">"averageIOPSize"</span><span class="p">:</span><span class="w"> </span><span class="mi">5074</span><span class="p">,</span><span class="w">
            </span><span class="nl">"burstIOPSCredit"</span><span class="p">:</span><span class="w"> </span><span class="mi">12000</span><span class="p">,</span><span class="w">
            </span><span class="nl">"clientQueueDepth"</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w">
            </span><span class="nl">"desiredMetadataHosts"</span><span class="p">:</span><span class="w"> </span><span class="kc">null</span><span class="p">,</span><span class="w">
            </span><span class="nl">"latencyUSec"</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w">
            </span><span class="nl">"metadataHosts"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
                </span><span class="nl">"deadSecondaries"</span><span class="p">:</span><span class="w"> </span><span class="p">[],</span><span class="w">
                </span><span class="nl">"liveSecondaries"</span><span class="p">:</span><span class="w"> </span><span class="p">[],</span><span class="w">
                </span><span class="nl">"primary"</span><span class="p">:</span><span class="w"> </span><span class="mi">5</span><span class="w">
            </span><span class="p">},</span><span class="w">
            </span><span class="nl">"nonZeroBlocks"</span><span class="p">:</span><span class="w"> </span><span class="mi">22414</span><span class="p">,</span><span class="w">
            </span><span class="nl">"normalizedIOPS"</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w">
            </span><span class="nl">"readBytes"</span><span class="p">:</span><span class="w"> </span><span class="mi">5825024</span><span class="p">,</span><span class="w">
            </span><span class="nl">"readBytesLastSample"</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w">
            </span><span class="nl">"readLatencyUSec"</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w">
            </span><span class="nl">"readLatencyUSecTotal"</span><span class="p">:</span><span class="w"> </span><span class="mi">43202</span><span class="p">,</span><span class="w">
            </span><span class="nl">"readOps"</span><span class="p">:</span><span class="w"> </span><span class="mi">416</span><span class="p">,</span><span class="w">
            </span><span class="nl">"readOpsLastSample"</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w">
            </span><span class="nl">"samplePeriodMSec"</span><span class="p">:</span><span class="w"> </span><span class="mi">500</span><span class="p">,</span><span class="w">
            </span><span class="nl">"throttle"</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w">
            </span><span class="nl">"timestamp"</span><span class="p">:</span><span class="w"> </span><span class="s2">"2023-11-21T03:55:16.102916Z"</span><span class="p">,</span><span class="w">
            </span><span class="nl">"unalignedReads"</span><span class="p">:</span><span class="w"> </span><span class="mi">90</span><span class="p">,</span><span class="w">
            </span><span class="nl">"unalignedWrites"</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w">
            </span><span class="nl">"volumeAccessGroups"</span><span class="p">:</span><span class="w"> </span><span class="p">[],</span><span class="w">
            </span><span class="nl">"volumeID"</span><span class="p">:</span><span class="w"> </span><span class="mi">63</span><span class="p">,</span><span class="w">
            </span><span class="nl">"volumeSize"</span><span class="p">:</span><span class="w"> </span><span class="mi">2000683008</span><span class="p">,</span><span class="w">
            </span><span class="nl">"volumeUtilization"</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w">
            </span><span class="nl">"writeBytes"</span><span class="p">:</span><span class="w"> </span><span class="mi">2249699328</span><span class="p">,</span><span class="w">
            </span><span class="nl">"writeBytesLastSample"</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w">
            </span><span class="nl">"writeLatencyUSec"</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w">
            </span><span class="nl">"writeLatencyUSecTotal"</span><span class="p">:</span><span class="w"> </span><span class="mi">111588013</span><span class="p">,</span><span class="w">
            </span><span class="nl">"writeOps"</span><span class="p">:</span><span class="w"> </span><span class="mi">115090</span><span class="p">,</span><span class="w">
            </span><span class="nl">"writeOpsLastSample"</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w">
            </span><span class="nl">"zeroBlocks"</span><span class="p">:</span><span class="w"> </span><span class="mi">466034</span><span class="w">
        </span><span class="p">}</span><span class="w">
    </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">

</span></code></pre></div></div>

<p>See those <code class="language-plaintext highlighter-rouge">writeBytes</code> and <code class="language-plaintext highlighter-rouge">writeOps</code>?</p>

<p>I ran the same workload for 60 seconds and writeBytes jumped to 2253455360 (917 kB) while writeOps went up to 115298 (208 additional write requests); from that we know the average throughput was 15.28kB/s and the average IO request size 4.4 kB (clearly no DB log rotation happened during that minute).</p>

<p>Those are very low figures, but these metrics can be used to determine whether we should inspect the volume’s snapshots or not.</p>

<p>Let’s say we gather these metrics with HCI Collector or SolidFire Exporter, and realize the following:</p>

<ul>
  <li>There’s a 10 GB volume with fullness between 49-50%</li>
  <li>It has 10 daily snapshots</li>
  <li>Over 10 days, writeBytes add up to what amounts to 4 GB</li>
  <li>Volume efficiency for this volume is 3x</li>
</ul>

<p>That means 4 GB of IO on 5 GB of data, which is 80% rate of change over just 10 days, or some 7-8% a day. That’s considered high as many backup applications default to 2-3%, which means even after deduplication and compression this volume <em>could</em> still use quite a lot of capacity for snapshots.</p>

<p>We can’t know for sure unless we take external backups (which is the same idea as making clones from snapshots): <em>if</em> these writes are repeated updates of one small SQL table, snapshots may not consume a lot of capacity. But if they involve other patterns, then they may.</p>

<p>These indicators may help us eliminate suspects and save resources. Even if if a volume is big and has old snapshots, a low number of writeBytes since the oldest snapshot until now would mean this volume’s snapshots don’t use a lot of space.</p>

<p>In other words, it seems to me that a high ratio of writes divided by fulness is not a sure sign of a high snapshot capacity consumption, but a low ratio is likely a sign of low snapshot capacity consumption.</p>

<h2 id="cost-issue">Cost issue</h2>

<p>The approach is said to be “expensive” because it involves creating clones out of potentially hundreds (but hopefully only dozens) of snapshots that a user may have.</p>

<p>SolidFire does that <a href="/2023/08/30/monitoring-solidfire-clone-and-backup-jobs.html#opportunities-for-improvements-and-integrations">quickly and efficiently</a>, so it’s not absolutely prohibitive.</p>

<p>For example, let’s say a user has 60 snapshots of 10 volumes.</p>

<ul>
  <li>If most snapshots are mere hours or days old <em>and</em> using <a href="https://github.com/scaleoutsean/hcicollector/">HCI Collector</a> (see <a href="https://github.com/scaleoutsean/hcicollector/tree/master/sfcollector-kubernetes">this</a> for a K8s/container version) or <a href="https://github.com/mjavier2k/solidfire-exporter">SolidFire Exporter</a> we observe volume write rate isn’t excessive, that means snapshots likely take a small capacity and there’s no reason to do anything about them</li>
  <li>If the volumes are large and/or snapshots old (days, or weeks), that may require periodic evaluation. Let’s consider this case.</li>
</ul>

<p>Since these snapshots stay around for days or weeks, we don’t have to check them every day. Once a week is enough.</p>

<p>We can use SolidFire PowerShell tools or Python SDK to completely automate this process:</p>

<ul>
  <li>Pick a random account ID</li>
  <li>Check the account’s volumes and decide whether to check. We’d use own logic here, such as: if the product of volumes bigger than 1 TB with at least one snapshot older than 1 week is greater than 10, run the check</li>
  <li>Clone those volumes’ snapshots and assign them to your “utility” account. You could also create clones from the largest volume’s snapshots.</li>
  <li>Wait until next Garbage Collection, report account efficiency (send metrics to your monitoring solution such as Elasticsearch, Splunk, or some TSDB)</li>
  <li>Delete and purge cloned volumes</li>
</ul>

<p>That can be done in 200 lines of code.</p>

<p>We’d have to be careful to not hit the maximum number of volumes per node/cluster, or run out of metadata capacity, which could be addressed with a cluster fullness check between cloning of each snapshot.</p>

<p>With one storage account, we could do one examination per hour, or dozens of checks during a weekend.</p>

<p>Other opportunities for controlling disk consumption by snapshots are related to snapshots’ number and age. We could gather output of <code class="language-plaintext highlighter-rouge">ListSnapshots</code> (collected by HCI Collector or SolidFire Exporter) and create alerts for cases where snapshots are older than X days and such. It’s best to create <a href="/2022/02/14/middle-class-rbac-solidfire-ansible.html">snapshot schedules</a> for users so that they don’t have to remember to delete them. Kubernetes admin can create snapshot quotas (in number, not capacity terms), but I’m not sure if that can be done on Docker. Still, volumes with a high number of Docker snapshots can be identified with a monitoring solution.</p>

<p>In fact I’ve been working on updating SF Collector and one new thing I plan to do is gather metadata from volumes managed by NetApp Trident. The idea is to provide exactly this insight, and also help with backup, replication and other scenarios in which we’d like to watch Trident-tagged volumes. More on that in a future post!</p>

<h2 id="do-we-still-need-to-clone-those-snapshots">Do we still need to clone those snapshots?</h2>

<p>Now that we’ve seen how this <em>roughly</em> works, it just ocurred to me that in most cases it should be enough to simply analyze SolidFire metrics, identify “suspect volumes” and send a reminder to the owner(s) to delete unnecessary snapshots.</p>

<p>SolidFire clusters with centrally managed “snapshot SLAs” in place wouldn’t even have this problem, but if it was suspected we could create clones to get a better estimate as described in this post.</p>

<p>For mainstream applications (SQL Server, PostgreSQL, NodeJS, etc.) we would quickly get an idea how each behaves in terms of change rate.</p>

<h2 id="how-to-minimize-storage-utilization-by-snapshots">How to minimize storage utilization by snapshots</h2>

<ul>
  <li>Configure unmap/discard on your hosts (you saw what happened with the first PostgreSQL clone)</li>
  <li>Institute “storage SLAs” or “snapshot SLAs” as illustrated in that linked post</li>
  <li>For databases, encrypt &amp; dump them to S3 rather than keeping snapshots for weeks or months, and create 10-line scripts that occasionally test these backups in temporary containers or VMs</li>
  <li>For containers, send logs application <em>out</em> to syslog servers, don’t save them inside the container</li>
</ul>

<h2 id="conclusion">Conclusion</h2>

<p>The good thing about SolidFire snapshots is you don’t need to reserve (and thereby strand) capacity for them, which allows you to maximize the use of the single pool of storage that a cluster offers. The bad thing is you can’t reserve (and thereby can’t limit either) the capacity of snapshots, although in some cases it can be done at the virtualization management layer.</p>

<p>SolidFire failed to implement per-account snapshot capacity reporting, but it is possible to work around this if your cluster capacity report indicates <code class="language-plaintext highlighter-rouge">snapshotNonZeroBlocks</code> occupy many TBs of capacity which may justify the “cost” of cloning for the purpose of finding snapshots that use a lot of storage capacity.</p>

<p>Although the approach is not very straight-froward, it is not terribly complex or “expensive” either.</p>

<p>If you gather SolidFire storage metrics, that task becomes even easier as metrics can help you identify and prioritize the right volumes for examination. An SLA-driven approach to snapshot schedules would almost completely eliminate the need to use of this snapshot-cloning approach.</p>

      </article>
    </div>
    
      <div class="categories">
    <span><p>Categories:</p>
    
    
      <a href="
      /categories/#storage">storage</a>
      &nbsp; 
    
      <a href="
      /categories/#solidfire">solidfire</a>
      &nbsp; 
    
      <a href="
      /categories/#monitoring">monitoring</a>
       
    
  </span>
</div>
    

    
      <div class="related" data-pagefind-ignore>

    <h4>Possibly related - use live search at the top to find other content</h4>
    
    
    
    
    
    
    
        
        
    
        
    
        
          <div class = "related-posts">
          <h5><a href="/2024/04/11/netapp-eseries-containerized-beegfs-nfs-s3-all-in-one.html">• NetApp E-Series with containerized BeeGFS, NFS, S3</a></h5>
          </div>
          
          
        
    
      
    
        
        
    
        
    
        
          <div class = "related-posts">
          <h5><a href="/2024/04/09/solidbackup-velero-backup-non-k8s-volumes-netapp-solidfire-to-s3.html">• Backup NetApp SolidFire's non-Kubernetes volumes with Velero</a></h5>
          </div>
          
          
        
    
      
    
        
        
    
        
    
        
          <div class = "related-posts">
          <h5><a href="/2024/04/02/storagegrid-networking.html">• NetApp StorageGRID networks</a></h5>
          </div>
          
          
        
    
      
    
        
        
    
        
    
        
          <div class = "related-posts">
          <h5><a href="/2024/04/01/windows-server-2025-with-solidfire-part-two-sql-server-2022.html">• Windows Server 2025 with NetApp SolidFire 12 iSCSI Part Two</a></h5>
          </div>
          
          
        
    
      
    
        
        
    
        
    
        
          <div class = "related-posts">
          <h5><a href="/2024/04/01/windows-server-2025-with-solidfire-part-three-hyper-v.html">• Windows Server 2025 with NetApp SolidFire 12 iSCSI Part Three</a></h5>
          </div>
          
          
            
    
    </div>

    

    
  </div><footer class= "footer">
    <p>2024-04-12 18:05 </p>
    <p>Copyright © 2024 scaleoutSean. Content is released under the CC BY license. Design: Alessio Franceschi</p>
</footer>

</body>
</html>
