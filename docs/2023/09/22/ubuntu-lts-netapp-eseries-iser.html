<!doctype html>
<html lang="en">
  <head>
        <meta charset="UTF-8">
    <meta http-equiv="content-language" content="en">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="googlebot" content="noarchive">
    <meta name="googlebot" content="max-snippet:60">
    <meta name="googlebot" content="max-image-preview:small">
    <meta name="bingbot" content="noarchive">
    <meta name="bingbot" content="max-snippet:60">
    <meta name="bingbot" content="max-image-preview:small">
    <meta name="robots" content="noarchive">
    <meta name="robots" content="max-snippet:60">
    <meta name="google-site-verification" content="F6q7vIwQ2G0j8tk-KL9rAXOMLXMDMMUkEz4fRs1Nnnc" />
    <title>
        
            Ubuntu 22.04 LTS as iSER client to NetApp E-Series | Acting Technologist
      
    </title>
    <meta name="description" content="
     Ubuntu with E-Series iSER? So what if it's not supported!?
     ">

    <!-- LINK TO ATOM FEED FOR SEO -->
    <link rel="alternate" type="application/atom+xml" href="https://scaleoutsean.github.io/feed.xml" />

    <!-- FAVICON -->
    <link rel="icon" href="/assets/favicon.ico" type="image/x-icon">
    <link rel="shortcut icon" href="/assets/favicon.ico" type="image/x-icon">

    <!-- CSS -->
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="stylesheet" href="/assets/css/syntax.css">

    <!-- Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css">

    <link href="/_pagefind/pagefind-ui.css" rel="stylesheet">
    <script src="/_pagefind/pagefind-ui.js" type="text/javascript"></script>
    <div id="search"></div>
    <script src="https://cdn.counter.dev/script.js" data-id="83dc700a-b821-4e57-9425-02b8336a9456" data-utcoffset="0"></script>
    <script>
        window.addEventListener('DOMContentLoaded', (event) => {
            new PagefindUI({ element: "#search" });
        });
    </script>

    <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Ubuntu 22.04 LTS as iSER client to NetApp E-Series | Acting Technologist</title>
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="Ubuntu 22.04 LTS as iSER client to NetApp E-Series" />
<meta name="author" content="scaleoutSean" />
<meta property="og:locale" content="en" />
<meta name="description" content="Ubuntu with E-Series iSER? So what if it’s not supported!?" />
<meta property="og:description" content="Ubuntu with E-Series iSER? So what if it’s not supported!?" />
<link rel="canonical" href="https://scaleoutsean.github.io/2023/09/22/ubuntu-lts-netapp-eseries-iser.html" />
<meta property="og:url" content="https://scaleoutsean.github.io/2023/09/22/ubuntu-lts-netapp-eseries-iser.html" />
<meta property="og:site_name" content="Acting Technologist" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-09-22T00:00:00+08:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"scaleoutSean"},"description":"Ubuntu with E-Series iSER? So what if it’s not supported!?","@type":"BlogPosting","headline":"Ubuntu 22.04 LTS as iSER client to NetApp E-Series","dateModified":"2023-09-22T00:00:00+08:00","datePublished":"2023-09-22T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://scaleoutsean.github.io/2023/09/22/ubuntu-lts-netapp-eseries-iser.html"},"url":"https://scaleoutsean.github.io/2023/09/22/ubuntu-lts-netapp-eseries-iser.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  </head>

<body>

  <div class="container">
    <header id="header">
	<div id = site_title>
		<a href="https://scaleoutsean.github.io/">
			<h2 style="font-size:1.8em;">Acting Technologist</h2>
		</a>
		
		<h3>
			civilizations are created by individuals
		</h3>
		
	</div>

	<div id="subheader">
		
		<nav class="pages">
<a href="/about.html">About</a>

<a href="/archive.html">Archive</a>

<a href="/categories/">Categories</a>

<a href="/projects.html">Projects</a>
</nav>
		
		
		<nav class="social">
			
  
    <a href="https://www.github.com/scaleoutsean" target="_blank" id="github"><i class="fab fa-github" aria-hidden="true"></i></a>
  
   

  
    <a href="https://twitter.com/scaleoutsean" target="_blank" id="twitter"><i class="fab fa-twitter" aria-hidden="true"></i></a>
  
   

  
  
    <a href="/feed.xml" target="_blank" id="rss"><i class="fas fa-rss" aria-hidden="true"></i></a>
   

		</nav>
		
	</div>
</header>


    <div class="post-container">
      <article id = "post">
        <h1 id = "post-title">Ubuntu 22.04 LTS as iSER client to NetApp E-Series</h1>

  
  <div class = "post-info">
    <span>
        <i class="far fa-calendar" aria-hidden="true"></i> <span>22 Sep 2023</span> - <i class="far fa-clock"></i> 


  
  
    20 minute read
  

    </span>
  </div>
  
        <ul>
  <li><a href="#problem-statement">Problem statement</a></li>
  <li><a href="#e-series-side">E-Series side</a></li>
  <li><a href="#ubuntu-side">Ubuntu side</a></li>
  <li><a href="#hardware-and-software-stack">Hardware and software stack</a></li>
  <li><a href="#setup-workflow">Setup workflow</a></li>
  <li><a href="#tips-and-tricks">Tips and tricks</a>
    <ul>
      <li><a href="#santricity-host-settings">SANtricity Host settings</a></li>
      <li><a href="#santricity-iser-settings">SANtricity iSER settings</a></li>
      <li><a href="#os-rescue-mode">OS rescue mode</a></li>
      <li><a href="#whats-what-and-mellanox-ofed">What’s what and Mellanox OFED</a></li>
      <li><a href="#netplan-and-mtu">Netplan and MTU</a></li>
      <li><a href="#ifaces">ifaces</a></li>
      <li><a href="#multipath">Multipath</a></li>
      <li><a href="#target-discovery">Target discovery</a></li>
      <li><a href="#volume-partitioning-and-sanlun-command">Volume partitioning and sanlun command</a></li>
    </ul>
  </li>
  <li><a href="#conclusion">Conclusion</a></li>
</ul>

<h2 id="problem-statement">Problem statement</h2>

<p>WTF is iSER? <a href="https://en.wikipedia.org/wiki/ISCSI_Extensions_for_RDMA">iSER</a> aka iSCSI Extensions for RDMA is a computer network protocol that extends the Internet Small Computer System Interface (iSCSI) protocol to use Remote Direct Memory Access (RDMA).</p>

<p>NetApp E-Series supports iSER. Here’s an existing image from another blog post that shows EF-Series EF300 and EF600, but E-Series 5700 also has the 100G IB Host Interface Card (HIC) option. See “iSER” in “I/O Interface Options”.</p>

<p><img src="/assets/images/efseries-ef300-ef600-technical-specifications.png" alt="EF300 and EF600 Client Connectivity" /></p>

<p>Anyway, with 100G IB you can get low-latency 100 Gbps (per port) iSCSI to the box, which is very nice for many Big Data, analytics and HPC workloads.</p>

<p>The <a href="https://docs.netapp.com/us-en/e-series/config-linux/iser-ib-verify-linux-config-support-task.html">documentation</a> for Linux with iSER naturally exists, but not for Ubuntu.</p>

<p>Your first choice would be to ask NetApp to qualify Ubuntu (or Debian, or what have you) as a one-off thing. This sometimes happens when you must have the entire stack validated end-to-end.</p>

<p>Your second choice is to <strong>Just Do It (TM)</strong>, if you can.</p>

<p>Maybe this post can help you to “yes, can do” iSER!</p>

<h2 id="e-series-side">E-Series side</h2>

<p>There’s nothing special about configuring E-Series iSER for Ubuntu. Which is also why Oracle KVM should just work with E-Series - same kernel, same drivers, etc.</p>

<p>So, work by the official docs and check this page for supplemental information.</p>

<h2 id="ubuntu-side">Ubuntu side</h2>

<p>This is where it can get tricky. Long story short, there’s one major choice to make, and that is “to use or not to use Mellanox OFED?” and the answer I offer is “use it”.</p>

<p>Officially, supported Linux seem to be tested with built-in drivers.</p>

<p>I tried that using Ubuntu iSER with EF570 and my experience was crappy. Since you’re using an unsupported OS, you may as well use IB driver that work.</p>

<p>The rest is “common sense with Ubuntu characteristics”: follow the E-Series documentation (link at the top) and configure Linux stuff the Ubuntu way.</p>

<h2 id="hardware-and-software-stack">Hardware and software stack</h2>

<ul>
  <li>x86_64 server with Mellanox ConnectX-5 (dual-ported 100Gb/ IB HCA; model: MCX556A-ECAT; FW 16.35.2000)</li>
  <li>Ubuntu 22.04 LTS with all updates as of Sep 21, 2023
    <ul>
      <li>4 IB IPs, 2 on 192.168.100.0/24 and two on 192.168.101.0/24 network</li>
      <li>NIC names configured for iSER: ibs1f0, ibs1f1, ibs5f0, ibs5f1</li>
      <li>Mellanox OFED LTS 5.8-3.0.7.0</li>
    </ul>
  </li>
  <li>E-Series SANtricity 11.80 (model: EF-Series EF570)
    <ul>
      <li>Controller A: 192.168.100.1 (Port 1), 192.168.101.1</li>
      <li>Controller B: 192.168.100.2 (Port 1), 192.168.101.2</li>
    </ul>
  </li>
  <li>No IB switches (direct attach, aka DAS)</li>
</ul>

<p>One note on DAS: usually we connect 1 or 2 servers this way, and 2 servers with 2 ports each consume 4 ports on E-Series IB HICs.</p>

<p>One example of that is BeeGFS node pairs connected E-Series, although that solution by default uses NVMe/RoCE rather than iSER/IB (but it could use iSER/IB, as could IBM Spectrum Scale or other applications).</p>

<h2 id="setup-workflow">Setup workflow</h2>

<p>It’s helpful to understand what needs to be done. Roughly speaking:</p>

<ul>
  <li>Install IB drivers</li>
  <li>Figure out how everything is connected (you were supposed to know before you started, but let’s assume that’s an afterthought)</li>
  <li>Get IB to work (Link Up, etc.)</li>
  <li>Configure iSCSI on top of IB (iSER)</li>
  <li>Discover and login to targets</li>
  <li>Ensure multipathing works, format and mount</li>
</ul>

<h2 id="tips-and-tricks">Tips and tricks</h2>

<p>The primary objective of this post is to share information related to Ubuntu with E-Series without repeating what’s in the official documentation.</p>

<p>I’ll make these points in small sections. Some of it is “common sense stuff” that’s not documented in the E-Series documentation because it’s out of scope, but some of it is missing because Ubuntu is not in-scope.</p>

<h3 id="santricity-host-settings">SANtricity Host settings</h3>

<p>In order to configure E-Series iSER so that a server or cluster can connect to it via iSER, you need to pick iSER in Host settings.</p>

<p>That’s obvious, but it’s not obvious.</p>

<p>Remember to pick <strong>iSER</strong> and not the default (iSCSI): you need the other iSCSI - iSER!</p>

<p><img src="/assets/images/eseries-iser-06-host-iser-add.png" alt="" /></p>

<p>Another weird thing is the host type: you’d expect one of the several Linux settings should apply. But they don’t if you follow the official documentation (which doesn’t tell you how to prepare the client).</p>

<p><img src="/assets/images/eseries-iser-09-santricity-host-settings.png" alt="" /></p>

<p>(In this screenshot, iSCSI interface is selected. Don’t do that for iSER - that <strong>won’t</strong> work for iSER!)</p>

<p>The official documentation says the default multipath.conf settings work, but they don’t. This is the E-Series default if you don’t configure anything in multipath.conf and let it load:</p>

<pre><code class="language-raw">devices {
	device {
		vendor "(NETAPP|LSI|ENGENIO)"
		product "INF-01-00"
		product_blacklist "Universal Xport"
		path_grouping_policy "group_by_prio"
		path_checker "rdac"
		features "2 pg_init_retries 50"
		hardware_handler "1 rdac"
		prio "rdac"
		failback "immediate"
		no_path_retry 30
	}
}
</code></pre>

<p>I think what happens is Ubuntu 22.04 (and 20.04) doesn’t - or you don’t force it to - correctly load a new driver, then - because the E-Series documentation doesn’t suggest you blacklist this legacy entry or change it to use a newer driver - it falls back to RDAC.</p>

<p>And then adding the host other than “Factory Default” doesn’t seem to work.</p>

<p>While trying to figure out this “Factory Default” nonsense, I realized that the undocumented “default” multipathing algorithm for E-Series is now <code class="language-plaintext highlighter-rouge">scsi_dh_alua</code>, which - again, in theory - should automatically load on a host connected to E-Series, which would then make it possible to set Host Type to Linux with DM and a post-v3.10 kernel and DM-MP.</p>

<p>Documentation on selecting OS types in SANtricity can be found <a href="https://docs.netapp.com/us-en/e-series-santricity/sm-storage/how-do-i-know-which-host-operating-system-type-is-correct.html">here</a> and it, too, is misleading:</p>

<blockquote>
  <p>Supports Linux operating systems using a Device Mapper multipath failover solution with a 3.10 or later Kernel.</p>
</blockquote>

<p>That’s wrong. If you use the “default” settings you may very well end up with RDAC like I did, and then this DM-MP host type won’t work well. That should say “… and with such-and-such MPIO driver”. The entire manual doesn’t have anything on what driver is supposed to be used or how <code class="language-plaintext highlighter-rouge">multipath -ll</code> output is supposed to look like. Totally ridiculous!</p>

<p>With Ubuntu’s multipath defaults, RDAC gets loaded and Linux DP 3.10+ SANtricity host type gives me a warning about “suboptimal paths”. Factory Default doesn’t.</p>

<p>Anyway, that’s too much information for what could have been solved by properly documenting it in 50 words.</p>

<p>For newer Linux (kernel &gt;= 3.10), pick Linux DM-MP with kernel 3.10 or later. More on Ubuntu-side multipathing can be found further below.</p>

<h3 id="santricity-iser-settings">SANtricity iSER settings</h3>

<p>This isn’t hard, but the official documentation doesn’t contain screenshots (too damn hard?) so: go to Settings &gt; System and under General you’ll find two iSER-related entries.</p>

<p><img src="/assets/images/eseries-iser-01-configure-eseries.png" alt="" /></p>

<p>This is where you pick a controller (A or B) and then can see if the ports are plugged in and Up/Down, as well as assign IPs to them.</p>

<p><img src="/assets/images/eseries-iser-02-configure-eseries-iser-ip.png" alt="" /></p>

<p>With DAS, there’s no switch and ports will be down as long as proper drivers and OpenSM haven’t started on the host connected to these ports!</p>

<p>Just below that “Configure iSER over InfiniBand settings” you’ll see iSER (and RDMA) statistics. If multiple active volumes are balanced and multipathing is in place, controller ports should be approximately evenly utilized.</p>

<p><img src="/assets/images/eseries-iser-08-santricity-iser-stats.png" alt="" /></p>

<p>This is also the place to look for excessive errors and such.</p>

<h3 id="os-rescue-mode">OS rescue mode</h3>

<p>This is not E-Series-specific, but you may need it.</p>

<p>Before you start, try to boot your OS to Rescue Mode and make sure you are familiar with it (such as how to change OS configuration files in rescue mode).</p>

<p>The reason is if you screw up <em>and</em> make OS hang on boot, you may not be able to get in to unscrew the problem.</p>

<p>I got caught off-guard several times and almost had to re-install the OS…</p>

<p><img src="/assets/images/eseries-iser-04-edit-fstab.png" alt="" /></p>

<h3 id="whats-what-and-mellanox-ofed">What’s what and Mellanox OFED</h3>

<p>As mentioned in Setup Steps, we need to know what’s what:</p>

<ul>
  <li>Which NICs are IB cards</li>
  <li>How is everything connected - ports, device IDs, switch (if any) ports, E-Series controller, etc.</li>
</ul>

<p>The E-Series documentation calls for identification of devices, ports, GIDs and what not.</p>

<p>You need to do that in any case (to understand how everything is connected), but because I installed Mellanox OFED, I did <strong>not</strong> have to do configure, create and enable <a href="https://docs.netapp.com/us-en/e-series/config-linux/iser-ib-configure-subnet-manager-task.html">OpenSM service</a>.</p>

<p>But since we should run some diags for proper awareness, here’s an example of how ibstat output for one port on one of the HCAs looked like.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>ibstat
CA <span class="s1">'ibp134s0f0'</span>
        CA <span class="nb">type</span>: MT4119
        Number of ports: 1
        Firmware version: 16.26.1040
        Hardware version: 0
        Node GUID: 0x1c34da03007ca2da
        System image GUID: 0x1c34da03007ca2da
        Port 1:
                State: Active
                Physical state: LinkUp
                Rate: 100
                Base lid: 1
                LMC: 0
                SM lid: 1
                Capability mask: 0x2651e84a
                Port GUID: 0x1c34da03007ca2da
                Link layer: InfiniBand
...
</code></pre></div></div>

<p>How I installed Mellanox OFED:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo</span> ./mlnxofedinstall  <span class="nt">--umad-dev-rw</span> <span class="nt">--all</span> <span class="nt">--enable-opensm</span>
</code></pre></div></div>

<p>After I installed Mellanox OFED I enabled and started NVIDIA-packaged OpenSM service (remember, I did <strong>not</strong> create my own despite the E-Series documentation instructing otherwise).</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>● opensm.service - LSB: Start opensm subnet manager.
     Loaded: loaded <span class="o">(</span>/etc/init.d/opensm<span class="p">;</span> generated<span class="o">)</span>
     Active: active <span class="o">(</span>running<span class="o">)</span> since Thu 2023-09-21 17:38:05 UTC<span class="p">;</span> 1h 30min ago
       Docs: man:systemd-sysv-generator<span class="o">(</span>8<span class="o">)</span>
      Tasks: 320 <span class="o">(</span>limit: 115284<span class="o">)</span>
     Memory: 30.1M
        CPU: 3.656s
     CGroup: /system.slice/opensm.service
             ├─2616 /usr/sbin/opensm <span class="nt">-g</span> 0x1c34da03007ca283 <span class="nt">-f</span> /var/log/opensm.0x1c34da03007ca283.log
             ├─2631 /usr/sbin/opensm <span class="nt">-g</span> 0x1c34da03007ca2db <span class="nt">-f</span> /var/log/opensm.0x1c34da03007ca2db.log
...
</code></pre></div></div>
<p>Notice the <code class="language-plaintext highlighter-rouge">-g GID</code> thing in the last two lines? That’s one of the nice things Mellanox OFED stack does for us - we didn’t have to assemble that configuration by ourselves as the E-Series iSER documentation suggests (and it suggests so because, as I mentioned, it’s based on built-in RDMA and IB drivers and OpenSM service must be configured manually).</p>

<p>After installation of OFED you may notice there’s a legacy service, <code class="language-plaintext highlighter-rouge">srp_service</code>, running. You may stop, and later disable it, if you don’t need it.</p>

<p>You may want to reboot here and have those rescue mode instructions for Ubuntu 22.04 handy!</p>

<h3 id="netplan-and-mtu">Netplan and MTU</h3>

<p>One thing I noticed about this is on unconfigured IB interfaces MTU was 4092.</p>

<p>Once I configured them in /etc/netplan/*.yaml, and they got an IP address assigned, IB MTUs became 2044.</p>

<p>I first tried the easiest (and the most naive approach, but it was too easy and I had to try) - I set Netplan MTU to 4092. Yeah, no. Still 2044.</p>

<p>I this works as expected as I saw some details about it in the Mellanox documentation. Read it if you want to try 4090.</p>

<p><a href="https://netplan.readthedocs.io/en/latest/examples/">Netplan</a> requires very minimal configuration for these two (ibs1, ibs5) dual-ported HCAs.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="na">ibs1f0</span><span class="pi">:</span>
      <span class="na">addresses</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">192.168.100.10/24</span>
    <span class="na">ibs5f0</span><span class="pi">:</span>
      <span class="na">addresses</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">192.168.101.10/24</span>
    <span class="na">ibs1f1</span><span class="pi">:</span>
      <span class="na">addresses</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">192.168.100.20/24</span>
    <span class="na">ibs5f1</span><span class="pi">:</span>
      <span class="na">addresses</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">192.168.101.20/24</span>
</code></pre></div></div>

<p>If the server’s OFED drivers, OpenSM and links are up, then you may expect to see something like this (example for the first card):</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>7: ibs1f0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 2044 qdisc mq state UP group default qlen 256
    <span class="nb">link</span>/infiniband 00:00:0c:33:fe:80:00:00:00:00:00:00:1c:34:da:03:00:7c:a2:82 brd 00:ff:ff:ff:ff:12:40:1b:ff:ff:00:00:00:00:00:00:ff:ff:ff:ff
    altname ibp47s0f0
    inet 192.168.100.10/24 brd 192.168.100.255 scope global ibs1f0
       valid_lft forever preferred_lft forever
8: ibs1f1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 2044 qdisc mq state UP group default qlen 256
    <span class="nb">link</span>/infiniband 00:00:0d:01:fe:80:00:00:00:00:00:00:1c:34:da:03:00:7c:a2:83 brd 00:ff:ff:ff:ff:12:40:1b:ff:ff:00:00:00:00:00:00:ff:ff:ff:ff
    altname ibp47s0f1
    inet 192.168.100.20/24 brd 192.168.100.255 scope global ibs1f1
       valid_lft forever preferred_lft forever
</code></pre></div></div>

<p>Notice here ibs1f0 has an “altname”, ibp47s0f0, which indicates that doing some homework mapping out PCI slots, IB diagnostics output and network configuration may pay back if you get in trouble.</p>

<h3 id="ifaces">ifaces</h3>

<p>On Ubuntu, those iSCSI configuration files are stored in /etc/iscsi/ifaces/.</p>

<p>Create and edit them as the E-Series documentation suggests.</p>

<p>My server had 2 HCAs, each with 2 ports, so I created four files here. You can name these files any way you want.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">dir</span> <span class="nt">-lat</span> /etc/iscsi/ifaces
iface-ibs5f1
iface-ibs5f0
iface-ibs1f1
iface-ibs1f0

</code></pre></div></div>

<h3 id="multipath">Multipath</h3>

<p>Long story short, E-Series has three choices:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">scsi_dh_alua</code> - in E-Series, host type for that is “Linux DM-MP with kernel 3.10 and newer” (recommended and supported)</li>
  <li><code class="language-plaintext highlighter-rouge">scsi_dh_rdac</code> - in E-Series, select pre-3.10 Linux host type (not recommended, but still supported)</li>
  <li><code class="language-plaintext highlighter-rouge">rdac</code> - legacy MPP/RDAC driver - in E-Series, you probably don’t want that one, but if you use it, in SANtricity select Factory Default host type (this isn’t documented, but seems to work although it’s not supported either)</li>
</ul>

<p>The first two are supported. How do they differ?</p>

<p>There’s a deep-dive Technical Report with low-level details, but there’s nothing on how to get started in the first place (again, unbelievable!).  It seems to go like this:</p>

<ul>
  <li>If you load and force ALUA, Linux SCSI ALUA driver will be used and “Linux DM-MP (Kernel 3.10 or later)” should be the host type selected in E-Series</li>
  <li>If OS default multipath settings are used, you may end up with and old MPIO driver and may need to set Factory Default or maybe try “Linux DM-MP (Kernel 3.9 or earlier)”, although it’s better to troubleshoot and get ALUA to work</li>
  <li>If you change MPIO driver and E-Series host type setting for the system, reboot to make sure it doesn’t work only until the next reboot</li>
</ul>

<p>Install these packages, and make the services (multipathd, multipath-tools) enabled and running.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt-get <span class="nb">install</span> <span class="nt">-y</span> multipath-tools multipath-tools-boot
</code></pre></div></div>

<p>Blacklist the OS root device from MPIO if need be. The official iSER guide for E-Series and Ubuntu documentation suggest to create an initial image with MPIO enabled (<code class="language-plaintext highlighter-rouge">update-initramfs -u -k all</code>) and this should be repeated every time multipath.conf is changed.</p>

<p>The E-Series docs tell you to use an empty /etc/multipath.conf because the defaults are already correct for E/EF-Series, but you can also modify those values if you want to see what they are or need to accommodate other storage such as ONTAP iSCSI or such. This alone won’t change the driver type loaded (see further below).</p>

<pre><code class="language-raw">defaults {
  failback "manual"
  # See NetApp TR-4737, sometimes you may want "immediate"
}
blacklist {
  devnode "!^(sd[a-z]|dasd[a-z]|nvme[0-9])"
  # maybe you want (or not?) to blacklist other devices, including boot disks, from MPIO
  device {
  }
}
devices {
  # any other devices that use a different algo
}
overrides {
}
</code></pre>

<p>You can read the <a href="https://www.netapp.com/media/17144-tr4737.pdf">NetApp TR-4737</a> for the details about various MPIO options. Clearly - in my mind at least - that TR contradicts the official SANtricity documentation (<a href="https://docs.netapp.com/us-en/e-series/config-linux/iser-ib-configure-multipath-software-task.html">Multipathing section in Linux Express configuration</a> for v11.80 and all v11.70 releases):</p>

<blockquote>
  <p>Use the default multipath settings by leaving the multipath.conf file blank.</p>
</blockquote>

<p>I don’t think so. I had to use <code class="language-plaintext highlighter-rouge">rdloaddriver="scsi_dh_alua"</code> in GRUB which - after a reboot - loaded the generic Linux SCSI ALUA driver and allowed me to set “Host operating system type” to Linux DP 3.10+ without getting warnings about suboptimal paths.</p>

<p>Now if you log in to your iSER targets, you should be able to see multiple paths to volumes (in theory, you should see <code class="language-plaintext highlighter-rouge">hwhandler="1 alua"</code> in <code class="language-plaintext highlighter-rouge">multipath -ll</code> output; otherwise you might see <code class="language-plaintext highlighter-rouge">"1 rdac"</code> which is either <code class="language-plaintext highlighter-rouge">scsi_dh_rdac</code> or the deprecated RDAC driver).</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>multipath <span class="nt">-ll</span>
3600a098000e3c1b000002be3620b681e dm-3 NETAPP,INF-01-00
<span class="nv">size</span><span class="o">=</span>399G <span class="nv">features</span><span class="o">=</span><span class="s1">'3 queue_if_no_path pg_init_retries 50'</span> <span class="nv">hwhandler</span><span class="o">=</span><span class="s1">'1 alua'</span> <span class="nv">wp</span><span class="o">=</span>rw
|-+- <span class="nv">policy</span><span class="o">=</span><span class="s1">'service-time 0'</span> <span class="nv">prio</span><span class="o">=</span>50 <span class="nv">status</span><span class="o">=</span>active
| <span class="sb">`</span>- 15:0:0:1 sdb 8:16 active ready running
<span class="sb">`</span>-+- <span class="nv">policy</span><span class="o">=</span><span class="s1">'service-time 0'</span> <span class="nv">prio</span><span class="o">=</span>10 <span class="nv">status</span><span class="o">=</span>enabled
  <span class="sb">`</span>- 16:0:0:1 sdq 65:0 active ready running
3600a098000e3c1b000002d14634f47c9 dm-0 NETAPP,INF-01-00
<span class="nv">size</span><span class="o">=</span>100G <span class="nv">features</span><span class="o">=</span><span class="s1">'3 queue_if_no_path pg_init_retries 50'</span> <span class="nv">hwhandler</span><span class="o">=</span><span class="s1">'1 alua'</span> <span class="nv">wp</span><span class="o">=</span>rw
|-+- <span class="nv">policy</span><span class="o">=</span><span class="s1">'service-time 0'</span> <span class="nv">prio</span><span class="o">=</span>50 <span class="nv">status</span><span class="o">=</span>active
| <span class="sb">`</span>- 15:0:0:5 sdf 8:80  active ready running
<span class="sb">`</span>-+- <span class="nv">policy</span><span class="o">=</span><span class="s1">'service-time 0'</span> <span class="nv">prio</span><span class="o">=</span>10 <span class="nv">status</span><span class="o">=</span>enabled
  <span class="sb">`</span>- 16:0:0:5 sdu 65:64 active ready running
...
</code></pre></div></div>

<p>In theory, <code class="language-plaintext highlighter-rouge">scsi_dh_alua</code> is loaded by default. In practice, I think this doesn’t happen because RDAC is still in default multpath.conf and it gets loaded before the ALUA driver. I had to add <code class="language-plaintext highlighter-rouge">rdloaddriver="scsi_dh_alua"</code> to kernel boot command to get it to load before RDAC. That step is also suggested in <a href="https://docs.netapp.com/us-en/ontap-sm-classic/iscsi-config-rhel/task_configuring_dm_multipath.html">ONTAP SAN documentation</a>, while people from the E-Series Team explicitly told me it’s not required.</p>

<p>To force-load both supported types use <a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/6.3_technical_notes/kernel_issues">this</a> syntax: <code class="language-plaintext highlighter-rouge">rdloaddriver=scsi_dh_rdac,scsi_dh_alua</code>. But if you can’t control which gets eventually used, pick one.</p>

<p>And finally, if you don’t care about artsy-fartsy (also known as “user-friendly”) device names, you could format and mount these devices (or add mount points to /etc/fstab).</p>

<p>Example:</p>

<pre><code class="language-raw">/dev/mapper/3600a098000f637140000284763a83f44 /mount/data xfs4 _netdev 0 0
</code></pre>

<h3 id="target-discovery">Target discovery</h3>

<p>Before we mount and use any iSCSI (and also iSER) we need to discover them and login to iSCSI portal. But remember, here we’re not discovering iSCSI, but iSER targets (“the other iSCSI”).</p>

<p>Two things to say about that:</p>

<p>1) One funny thing that kept happening (although it didn’t bother me much) is the stupid 192.168.130.x/24 IPs which were not E-Series controller IPs. I think the reason is the IB HCAs have BlueField functionality included, so it’s coming from that. (I haven’t used BlueField, but I want to learn enough to be able to disable it!)</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>iscsiadm <span class="nt">-m</span> discovery <span class="nt">-t</span> st <span class="nt">-p</span> 192.168.100.1 <span class="nt">-I</span> iser
192.168.130.101:3260,1 iqn.1992-08.com.netapp:5700.600a098000f63714000000005e79c17c
192.168.131.101:3260,1 iqn.1992-08.com.netapp:5700.600a098000f63714000000005e79c17c
192.168.100.1:3260,1 iqn.1992-08.com.netapp:5700.600a098000f63714000000005e79c17c
192.168.101.1:3260,1 iqn.1992-08.com.netapp:5700.600a098000f63714000000005e79c17c
192.168.130.102:3260,2 iqn.1992-08.com.netapp:5700.600a098000f63714000000005e79c17c
192.168.131.102:3260,2 iqn.1992-08.com.netapp:5700.600a098000f63714000000005e79c17c
192.168.100.2:3260,2 iqn.1992-08.com.netapp:5700.600a098000f63714000000005e79c17c
192.168.101.2:3260,2 iqn.1992-08.com.netapp:5700.600a098000f63714000000005e79c17c
</code></pre></div></div>

<p>Anyway, the main thing here is you don’t want to see any unexpected errors during discovery and login to iSER.</p>

<p>2) The other noteworthy detail - is “<code class="language-plaintext highlighter-rouge">-I iser</code>” should supposedly be used to scan iSER targets. Using device from /etc/iscsi/ifaces/ worked for me as well (“<code class="language-plaintext highlighter-rouge">-I iface-ibs5f1</code>”, for example). I think that’s because iSER is configured in those interface files.</p>

<p>With <code class="language-plaintext highlighter-rouge">-I iser</code> (and the stupid BlueField IPs that I didn’t have time to get rid of):</p>

<p><img src="/assets/images/eseries-iser-03-configure-client-iser-ip-and-login.png" alt="" /></p>

<p>With full iface names:</p>

<p><img src="/assets/images/eseries-iser-05-scan-from-limited-ifaces.png" alt="" /></p>

<p>In the four iface files that I created, I only modified two values in each, <code class="language-plaintext highlighter-rouge">iface.initiatorname</code> (I used the OS-set iSCSI initiator name for all 4 files) and <code class="language-plaintext highlighter-rouge">iface.net_ifacename</code> (use iface name from “<code class="language-plaintext highlighter-rouge">ip address</code>” output, which is different for each interface:  <code class="language-plaintext highlighter-rouge">ibs1f0</code>,  <code class="language-plaintext highlighter-rouge">ibs1f1</code>,  <code class="language-plaintext highlighter-rouge">ibs5f0</code>, and <code class="language-plaintext highlighter-rouge">ibs5f1</code> for the last one). The latter I think ensures that only IB transport is used during discovery.</p>

<p>I don’t know if iSER would work better with more detailed settings (HWADDR, MTU, etc.) in iface files, but it seems it works fine without them.</p>

<p>Anyway, this is another good chance to reboot and see if everything (starting with the OS) can come up again. Have those OS rescue mode instructions ready!</p>

<p>Don’t panic if it takes a while for stuff to come up. This step takes long enough to get you start thinking about rescue mode.</p>

<p><img src="/assets/images/eseries-iser-07-mellanox-ofed.png" alt="" /></p>

<h3 id="volume-partitioning-and-sanlun-command">Volume partitioning and sanlun command</h3>

<p>You can see the <a href="https://docs.netapp.com/us-en/e-series/config-linux/fc-create-partition-file-task.html">sanlun</a> command mentioned in several places, including iSER-related pages.</p>

<p>First, that command is from one of the optional “utility” packages that NetApp provides for the main SAN products (ONTAP and E-Series). I seem to recall it can’t even be natively installed on Ubuntu or Debian, so don’t even bother.</p>

<p>Second, you don’t really need it anyway.</p>

<p>Maybe it was needed in 2007 when Linux was crappier than it is today, but what it does now is fairly limited. Example:</p>

<p><img src="/assets/images/eseries-iser-11-netapp-san-utilities.png" alt="" /></p>

<p>Why should I care about /dev/sd* names?</p>

<p>What I really want is something that helps me map E-Series volume names to mount points (which should not use /dev/sd* devices). For the rest I can simply use a handful of OS commands.</p>

<p>At first I skipped this scripting stuff alltogether as setups are mostly one-off things, but later on I had to create, delete and configure a whole bunch of LUNs, so eventually I did create a script that does that I want.</p>

<p><img src="/assets/images/eseries-iser-10-diy-san-script.png" alt="" /></p>

<ul>
  <li>Use the SANtricity API to get volume details</li>
  <li>Use OS (<code class="language-plaintext highlighter-rouge">multipath -ll</code>) to get path details</li>
  <li>Use shell commands to optionally create mount points</li>
  <li>Use shell commands to create format and mount commands (my use case is “lab testing”, otherwise I wouldn’t even show those force-mount commands - just copying them is dangerous enough!)</li>
</ul>

<p>This lets me go to the SANtricity UI, create a bunch of volumes, run the script and in 10 seconds have use all volumes with at pre-determined mount points. Some notes:</p>

<ul>
  <li>WWNs don’t change, dm-* names can, so WWNs is what we should use in /etc/fstab, but for temporary testing Device Mapper names are easier to inspect</li>
  <li>The last column shows current controller and it’s 1 throughout, that’s because I assigned all LUNs to Controller A to eliminate path flipping between controllers (i.e. MPIO is not behaving correctly, maybe because of network issues)</li>
</ul>

<p>What about volume partitioning?</p>

<p>My view is on flash disks I wouldn’t even partition LUNs unless I test and see partitioned disks are faster or have some other advantage over non-partitioned (I can’t think of any). Maybe they are when they’re NL-SAS, but for flash I doubt. So I wouldn’t bother with that step. For flash-based disks, I’d just run <code class="language-plaintext highlighter-rouge">mkfs</code> on the device and move on.</p>

<p>You can try and create partitioned devices and run a simple synthetic benchmark that resembles your workload to see if it matters.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Most E-Series users who want to use Ubuntu shouldn’t care that it’s not supported.</p>

<p>If you ask NetApp to qualify it for your environment, I suggest asking for Ubuntu LTS <em>with Mellanox OFED</em>.</p>

<p>One of the annoying areas is the poorly documented MPIO configuration for Linux - not for iSER or Ubuntu (which isn’t documented at all because it’s not officially supported although it’s known to work), but in general.</p>

<p>I will use this system in coming weeks and months so I may have new findings, but I don’t think there’s much that can go wrong here:</p>

<ul>
  <li>the mature and well-known components are iSCSI initiator and libraries, Multipath / Device Mapper, and kernel in general</li>
  <li>the sensitive parts are IB and RDMA and with OFED that’s solved for you by NVIDIA who, by the way, are <strong>heavy</strong> users of Ubuntu, so it’s not like this is something very risky or new</li>
</ul>

<p>Ubuntu and NVIDIA / Mellanox OFED solve your client-side challenges, and storage-side (E-Series IB uses Mellanox HCAs, as far as I remember) then becomes easy.</p>

<p>This approach lets you use Ubuntu and iSER with E-Series. While the DIY approach may involve mild shortcuts, this gives you LXD, ZFS and some other solutions and features which work best on Ubuntu. I think it’s a decent tradeoff if you need those features or standardize on Ubuntu or Debian-like distros that Mellanox OFED supports.</p>

<p>You could use Ubuntu easier without iSER (FC or 25Gb/s iSCSI), but iSER is fast and may be the right choice for AI, analytics and other environments.</p>

      </article>
    </div>
    
      <div class="categories">
    <span><p>Categories:</p>
    
    
      <a href="
      /categories/#storage">storage</a>
       
    
  </span>
</div>
    

  
    <div>
      <h3>Related Posts</h3>
      <ul>
      
        <li><a href="/2023/10/29/consistency-group-monitoring-in-eseries.html">Monitor Snapshot Consistency Groups of NetApp E-Series SANtricity OS</a></li>
      
        <li><a href="/2023/10/12/snapshot-clone-repository-monitoring-in-eseries.html">Monitor snapshot and clone repositories of NetApp E-Series SANtricity OS</a></li>
      
        <li><a href="/2023/10/08/raid1-in-netapp-eseries-ddp.html">Benefits of RAID 1 in E-Series DDP</a></li>
      
        <li><a href="/2023/11/06/netapp-eseries-sizing-for-splunk-smartstore.html">NetApp E-Series sizing for Splunk 9 with SmartStore</a></li>
      
        <li><a href="/2023/11/04/eseries-perf-analyzer-epa-330.html">E-Series Perf Analyzer (EPA) v3.3.0</a></li>
      
      </ul>
    </div>
  

    
  </div><footer class= "footer">
    <p>2025-01-24 01:15 </p>
    <p>Copyright © 2025 scaleoutSean. Content is released under the CC BY license. Design: Alessio Franceschi</p>
</footer>

</body>
</html>
