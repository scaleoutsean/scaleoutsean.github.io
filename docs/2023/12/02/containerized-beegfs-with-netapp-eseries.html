<!doctype html>
<html lang="en">
  <head>
        <meta charset="UTF-8">
    <meta http-equiv="content-language" content="en">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="googlebot" content="noarchive">
    <meta name="googlebot" content="max-snippet:60">
    <meta name="googlebot" content="max-image-preview:small">
    <meta name="bingbot" content="noarchive">
    <meta name="bingbot" content="max-snippet:60">
    <meta name="bingbot" content="max-image-preview:small">
    <meta name="robots" content="noarchive">
    <meta name="robots" content="max-snippet:60">
    <meta name="google-site-verification" content="F6q7vIwQ2G0j8tk-KL9rAXOMLXMDMMUkEz4fRs1Nnnc" />
    <title>
        
            Containerized BeeGFS with NetApp E-Series | Acting Technologist
      
    </title>
    <meta name="description" content="
     How E-Series owners can take advantage of containerized BeeGFS
     ">

    <!-- LINK TO ATOM FEED FOR SEO -->
    <link rel="alternate" type="application/atom+xml" href="https://scaleoutsean.github.io/feed.xml" />

    <!-- FAVICON -->
    <link rel="icon" href="/assets/favicon.ico" type="image/x-icon">
    <link rel="shortcut icon" href="/assets/favicon.ico" type="image/x-icon">

    <!-- CSS -->
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="stylesheet" href="/assets/css/syntax.css">

    <!-- Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css">

    <link href="/_pagefind/pagefind-ui.css" rel="stylesheet">
    <script src="/_pagefind/pagefind-ui.js" type="text/javascript"></script>
    <div id="search"></div>
    <script src="https://cdn.counter.dev/script.js" data-id="83dc700a-b821-4e57-9425-02b8336a9456" data-utcoffset="0"></script>
    <script>
        window.addEventListener('DOMContentLoaded', (event) => {
            new PagefindUI({ element: "#search" });
        });
    </script>

    <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Containerized BeeGFS with NetApp E-Series | Acting Technologist</title>
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="Containerized BeeGFS with NetApp E-Series" />
<meta name="author" content="scaleoutSean" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="How E-Series owners can take advantage of containerized BeeGFS" />
<meta property="og:description" content="How E-Series owners can take advantage of containerized BeeGFS" />
<link rel="canonical" href="https://scaleoutsean.github.io/2023/12/02/containerized-beegfs-with-netapp-eseries.html" />
<meta property="og:url" content="https://scaleoutsean.github.io/2023/12/02/containerized-beegfs-with-netapp-eseries.html" />
<meta property="og:site_name" content="Acting Technologist" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-12-02T00:00:00+08:00" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://scaleoutsean.github.io/2023/12/02/containerized-beegfs-with-netapp-eseries.html"},"author":{"@type":"Person","name":"scaleoutSean"},"description":"How E-Series owners can take advantage of containerized BeeGFS","@type":"BlogPosting","url":"https://scaleoutsean.github.io/2023/12/02/containerized-beegfs-with-netapp-eseries.html","headline":"Containerized BeeGFS with NetApp E-Series","dateModified":"2023-12-02T00:00:00+08:00","datePublished":"2023-12-02T00:00:00+08:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  </head>

<body>

  <div class="container">
    <header id="header">
	<div id = site_title>
		<a href="https://scaleoutsean.github.io/">
			<h2 style="font-size:1.8em;">Acting Technologist</h2>
		</a>
		
		<h3>
			civilizations are created by individuals
		</h3>
		
	</div>

	<div id="subheader">
		
		<nav class="pages">
<a href="/about.html">About</a>

<a href="/archive.html">Archive</a>

<a href="/categories/">Categories</a>

<a href="/projects.html">Projects</a>
</nav>
		
		
		<nav class="social">
			
  
    <a href="https://www.github.com/scaleoutsean" target="_blank" id="github"><i class="fab fa-github" aria-hidden="true"></i></a>
  
   

  
    <a href="https://twitter.com/scaleoutsean" target="_blank" id="twitter"><i class="fab fa-twitter" aria-hidden="true"></i></a>
  
   

  
  
    <a href="/feed.xml" target="_blank" id="rss"><i class="fas fa-rss" aria-hidden="true"></i></a>
   

		</nav>
		
	</div>
</header>


    <div class="post-container">
      <article id = "post">
        <h1 id = "post-title">Containerized BeeGFS with NetApp E-Series</h1>

  
  <div class = "post-info">
    <span>
        <i class="far fa-calendar" aria-hidden="true"></i> <span>02 Dec 2023</span> - <i class="far fa-clock"></i> 


  
  
    9 minute read
  

    </span>
  </div>
  
        <h2 id="introduction">Introduction</h2>

<p>Sometimes we want to run BeeGFS in containers.</p>

<p>It’s possible and, since the recent efforts by the BeeGFS vendor ThinkParQ, easy.</p>

<h2 id="why">Why</h2>

<p>Why deploy containerized BeeGFS?</p>

<p>As I demonstrated in <a href="/2020/12/31/beegfs-on-netapp-hci-and-ef-series.html">BeeGFS in VMs</a>:</p>

<ul>
  <li>BeeGFS in VMs can be deployed in minutes (including VMs, 10 minutes)</li>
  <li>Virtualized BeeGFS can deliver a decent performance even on VMFS</li>
  <li>vSphere can be used to provide HA to BeeGFS services</li>
  <li>If you need 1-20 GB/s, you may be able to get away with Ethernet-based, virtualized BeeGFS on a VI platform</li>
</ul>

<p>BeeGFS in containers is useful for the same reasons.</p>

<p>We expect that deployment would be faster, HA probably less good, and the cost lower because there’s no vSphere in the picture.</p>

<p>If you run analytics, AI or similar workloads based on Kubernetes workflows, you may not even have VMware in your environment.</p>

<p><a href="/2023/09/08/beegfs-csi-driver-lives-on.html">ThinkParQ took over development of BeeGFS CSI driver</a>, so client-side provisioning is supported by them as well.</p>

<h2 id="how">How</h2>

<p>Basically it’s three big steps: deploy underlying infra, deploy BeeGFS, and connect clients.</p>

<h3 id="deploy-and-configure-os-storage-and-network">Deploy and configure OS, storage and network</h3>

<p>Deploy OS, E-Series, and a high speed network (25 Gbps or better).</p>

<p>In a Kubernetes environment we’d have to use static host paths or a CSI driver for the storage array.</p>

<p>We don’t want BeeGFS containers (or VMs with BeeGFS containers) to randomly move around among compute nodes, so static host paths should be enough, but HA among compute hosts would have to be manual (unmount storage on one host, mount on another, fire up the migrated container).</p>

<p>There are <a href="/2022/12/09/directpv-topolvm-csi-lvm-das-k8s-with-eseries.html">CSI drivers</a> that we can use with E-Series, but they also don’t support HA failover among compute nodes. And with some other storage arrays you may be able to h handle compute node failures without hypervisor failover, and just with CSI drivers.</p>

<p>If you run Docker in VMs on on bare metal hosts, you wouldn’t need Kubernetes but only Docker, so we’d just create regular filesystem mount points that BeeGFS needs.</p>

<p>For optimal HA, using Docker containers with VMware HA is still appropriate for those who need enterprise-level uptime. If BeeGFS is deployed for temp or scratch space, manual HA with Kubernetes or Docker may be good enough.</p>

<p>Each BeeGFS host would <a href="https://doc.beegfs.io/latest/quick_start_guide/quick_start_guide.html">mount and format a volume or volumes for persistence</a>, and containers would use those persistent volumes to store data that survives restarts.</p>

<p>Normally, with a dynamic CSI provisioner those volumes would be created on the fly, but if you use Kubernetes with E-Series the ones I mentioned above would have to be created semi-automatically as they don’t talk to the SANtricity API and cannot create LUNs and present them to hosts without administrator’s involvement. It’s the same for Docker.</p>

<h3 id="containerized-beegfs">Containerized BeeGFS</h3>

<p><a href="https://github.com/ThinkParQ/beegfs-containers/">Clone this repository</a>, read the instructions (including <a href="https://doc.beegfs.io/latest/advanced_topics/containers.html">the BeeGFS documentation</a>) and start BeeGFS containers. Note that the BeeGFS documentation has its own docker-compose.yml which is slightly different from the one on Github. Maybe take a look at the both and pick whichever you like.</p>

<p>The only thing you may need to create is one or two Docker bridge interfaces, if you don’t want to use the default one. If you use Docker Compose on a single host, that should be automatic. If you use “<code class="language-plaintext highlighter-rouge">docker run</code>” or use several hosts, then you may need to create them separately.</p>

<p>Based on the simple example from the repository, my host mount points were as follows and all the BeeGFS containers were deployed with docker-compose in this VM:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">df</span> 
/dev/vdf                           270M   25k  226M   1% /mnt/meta_01_tgt_0101
/dev/vdg                           1.1G   42M  1.1G   4% /mnt/stor_01_tgt_101
/dev/vdh                           1.1G   42M  1.1G   4% /mnt/stor_01_tgt_102
/dev/vde                           511M   25k  474M   1% /mnt/mgmt_tgt_mgmt01
<span class="c"># double-check path for docker-compose.yml management volume path - it may be /mnt/mgmt_tgt (Github version)</span>
</code></pre></div></div>

<h3 id="deploy-and-configure-beegfs-clients">Deploy and configure BeeGFS clients</h3>

<p>BeeGFS client(s) wouldn’t run in containers.</p>

<p>Inside of application/workload containers we could use BeeGFS CSI driver (find the link at the top) for dynamic, or host paths for static, PVCs.</p>

<p>We’d deploy BeeGFS client(s) in VMs or physical hosts and connect to BeeGFS running in containers.</p>

<p>docker-compose.yml from the Github repository does not map host ports to any containers as it’s meant for local host experimentation. You’d have to add ports that you want to expose over network(s).</p>

<p>If you map container ports to the host (see the product documentation), you should see a couple of ports from the 8003-8008 range open like this (all containers are on one host, so management, metadata and storage nodes are all present).</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>netstat <span class="nt">-ant</span> 
Active Internet connections <span class="o">(</span>servers and established<span class="o">)</span>
Proto Recv-Q Send-Q Local Address           Foreign Address         State      
tcp        0      0 127.0.0.1:2019          0.0.0.0:<span class="k">*</span>               LISTEN     
tcp        0      0 0.0.0.0:22              0.0.0.0:<span class="k">*</span>               LISTEN     
tcp        0      0 0.0.0.0:8005            0.0.0.0:<span class="k">*</span>               LISTEN     
tcp        0      0 0.0.0.0:8003            0.0.0.0:<span class="k">*</span>               LISTEN     
tcp        0      0 0.0.0.0:8008            0.0.0.0:<span class="k">*</span>               LISTEN     
tcp6       0      0 :::80                   :::<span class="k">*</span>                    LISTEN     
tcp6       0      0 :::22                   :::<span class="k">*</span>                    LISTEN     
tcp6       0      0 :::8005                 :::<span class="k">*</span>                    LISTEN     
tcp6       0      0 :::8003                 :::<span class="k">*</span>                    LISTEN     
tcp6       0      0 :::8008                 :::<span class="k">*</span>                    LISTEN     
tcp6       0      0 :::8086                 :::<span class="k">*</span>                    LISTEN     
</code></pre></div></div>

<p>Obviously you couldn’t run two of the same services on the same node without creating port conflicts, but you wouldn’t want to anyway.</p>

<p>We’d want to spread storage node containers around different physical hosts or maybe VMs (probably not more than two per physical server).</p>

<p>We could also change BeeGFS configuration files to eliminate collision, but done properly each BeeGFS storage node would be on a different host. Small clusters would have just one metadata server, so that and management node wouldn’t be a problem.</p>

<p>BeeGFS clients would run on hosts or VMs different from those where BeeGFS servers run. Mixing multiple clients and servers across several shared physical hosts may be possible, but I wouldn’t do it - it could be difficult to troubleshoot. If you have just one host with all BeeGFS containers on it, and one client (the host), that works fine with “host” type network in docker-compose.yml.</p>

<h2 id="other-notes">Other notes</h2>

<p>Networking-wise, RDMA is supported, but I didn’t try it because I don’t have it and I’d think this containerized approach wouldn’t be targeting extreme levels of performance in any case.</p>

<p>But RDMA should work without any changes - just use the compose example from the official documentation to map network hardware to the containers - there’s nothing else to “enable” if your host is properly configured. You may <em>disable</em> RDMA if it’s available on the host and you don’t want it - the same as with a non-containerized BeeGFS.</p>

<p>Startup time for a small cluster (1 x Mgmt, 1 x MD, 1 x Storage (2 disks)) is mere seconds.</p>

<ul>
  <li>Docker start time: 2023-12-02 08:50:44</li>
  <li>Services up and running: 08:50:47</li>
</ul>

<p>For this I used docker-compose.yml from Github.</p>

<p><img src="/assets/images/beegfs-docker-00.png" alt="Docker Compose with BeeGFS up and running" /></p>

<p>Although the examples from Github and BeeGFS documentation technically do work, some of it is left as “an exercise for the reader”. For example, the client part is missing, and when I tried to configure it I encountered various network-related problems that I wasn’t able to solve for several hours.</p>

<p>In part that’s because <code class="language-plaintext highlighter-rouge">beegfs-utils</code> isn’t available inside of the containers, so if you want to look from <em>within</em> any Docker container, that’s inconvenient. I eventually spent hours building a customized management container, changing network configuration, etc. This <a href="https://doc.beegfs.io/latest/advanced_topics/containers.html#running-beegfs-in-containers-using-docker-run">part from the official documentation</a> is very relevant:</p>

<blockquote>
  <p>For containers it is important to always use -S and specify the string ID. By default BeeGFS uses the hostname, which for containers can change when they are recreated, or if the network mode is set to “host” will match the base OS hostname which would break things if multiple containers of the same type are running on the same server.</p>
</blockquote>

<p><img src="/assets/images/beegfs-docker-01.png" alt="Docker Compose with BeeGFS client connected" /></p>

<p>Another odd thing I spotted is that I created very small volumes for the host (because I don’t have much free space left), but <code class="language-plaintext highlighter-rouge">beegfs-df</code> reported a larger capacity. I don’t recall if that’s the maximum size possible or a bug.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>root@a3cf0ae2dc85:/# beegfs-df
METADATA SERVERS:
TargetID   Cap. Pool        Total         Free    %      ITotal       IFree    %
<span class="o">========</span>   <span class="o">=========</span>        <span class="o">=====</span>         <span class="o">====</span>    <span class="o">=</span>      <span class="o">======</span>       <span class="o">=====</span>    <span class="o">=</span>
       1         low      22.5GiB       5.9GiB  26%        1.5M        1.2M  81%

STORAGE TARGETS:
TargetID   Cap. Pool        Total         Free    %      ITotal       IFree    %
<span class="o">========</span>   <span class="o">=========</span>        <span class="o">=====</span>         <span class="o">====</span>    <span class="o">=</span>      <span class="o">======</span>       <span class="o">=====</span>    <span class="o">=</span>
     101   emergency      22.5GiB       5.9GiB  26%        1.5M        1.2M  81%
     102   emergency      22.5GiB       5.9GiB  26%        1.5M        1.2M  81%
</code></pre></div></div>

<p>My disks (management, metadata, storage) were much smaller (I should have used &gt; 2 GiB to avoid hitting the emergency limit).</p>

<pre><code class="language-raw">Disk /dev/vde: 512 MiB, 536870912 bytes, 1048576 sectors
Disk /dev/vdf: 614.4 MiB, 644245504 bytes, 1258292 sectors
Disk /dev/vdg: 1 GiB, 1073741824 bytes, 2097152 sectors
Disk /dev/vdh: 1 GiB, 1073741824 bytes, 2097152 sectors
</code></pre>

<p>For the EF300 I would start with 2-4 storage node containers with 1 target each and for the EF600, 4 storage nodes with 2-4 targets each, but since it’s easy to experiment in this environment it would be best to try several scenarios that emulate your workload and pick the most suitable one.</p>

<p>Backup and restore can be done in many ways - see some older posts on this topic. The same goes for monitoring, replication and other topics I’ve written about.</p>

<p>Data synchronization and copying from and to BeeGFS can be done with <a href="/2022/06/14/batch-copy-files-beegfs.html">any POSIX-compatible utility</a>. Users who use NFS or S3 for persistence and use BeeGFS as temp/scratch space can take advantage of this.</p>

<p>ARM64 is also supported, but as of now container images aren’t posted, so you need to use the Github approach which builds them from scratch, rather than the official documentation which downloads ThinkParQ-made images.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Containerized BeeGFS has the same benefits - some more pronounced, some less - as BeeGFS in VMs.</p>

<p>Users who run BeeGFS this way and need data persistence would benefit from VMware HA for workers/Docker nodes, and those who use BeeGFS for scratch space could run with manual HA and simply restart upon failure.</p>

<p>Small E-Series-based deployments could use this approach with 4 directly-attached uniprocessor servers.</p>

<p>I should have created some drawings to make this post easier to understand, but I guess this will have to do for the first one. I’ll write more about containerized BeeGFS if I come across customers interested in this approach.</p>

      </article>
    </div>
    
      <div class="categories">
    <span><p>Categories:</p>
    
    
      <a href="
      /categories/#storage">storage</a>
       
    
  </span>
</div>
    

  
    <div>
      <h3>Related Posts</h3>
      <ul>
      
        <li><a href="/2022/03/28/nomad-democratic-csi.html">HashiCorp Nomad CSI with NetApp SolidFire and E-Series back-ends</a></li>
      
        <li><a href="/2023/09/08/beegfs-csi-driver-lives-on.html">ThinkParQ takes over NetApp-created BeeGFS CSI driver</a></li>
      
        <li><a href="/2023/01/14/eseries-performance-analyzer-container-orchestrator-kubernetes.html">NetApp E-Series Performance Analyzer in orchestrated container environments</a></li>
      
        <li><a href="/2024/04/11/netapp-eseries-containerized-beegfs-nfs-s3-all-in-one.html">NetApp E-Series with containerized BeeGFS, NFS, S3</a></li>
      
        <li><a href="/2024/01/24/kubernetes-keda-netapp-solidfire-eseries.html">Kubernetes KEDA with NetApp SolidFire and E-Series</a></li>
      
      </ul>
    </div>
  

    
  </div><footer class= "footer">
    <p>2025-04-16 15:41 </p>
    <p>Copyright © 2025 scaleoutSean. Content is released under the CC BY license. Design: Alessio Franceschi</p>
</footer>

</body>
</html>
