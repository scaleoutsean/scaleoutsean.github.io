<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.27.3 by Michael Rose
  Copyright 2013-2025 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Linux, NVMe/RoCE and HA NetApp EF-Series NVMe-oF storage | Acting Technologist</title>
<meta name="description" content="Connecting to NetApp EF-Series storage from NVMe/RoCE (RDMA)-capable Linux servers">


  <meta name="author" content="Sean">
  
  <meta property="article:author" content="Sean">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Acting Technologist">
<meta property="og:title" content="Linux, NVMe/RoCE and HA NetApp EF-Series NVMe-oF storage">
<meta property="og:url" content="https://scaleoutsean.github.io/2026/02/19/linux-nvme-roce-ef-series.html">


  <meta property="og:description" content="Connecting to NetApp EF-Series storage from NVMe/RoCE (RDMA)-capable Linux servers">



  <meta property="og:image" content="https://scaleoutsean.github.io/assets/images/eseries-santricity-ef600-200g-das.png">



  <meta name="twitter:site" content="@scaleoutSean">
  <meta name="twitter:title" content="Linux, NVMe/RoCE and HA NetApp EF-Series NVMe-oF storage">
  <meta name="twitter:description" content="Connecting to NetApp EF-Series storage from NVMe/RoCE (RDMA)-capable Linux servers">
  <meta name="twitter:url" content="https://scaleoutsean.github.io/2026/02/19/linux-nvme-roce-ef-series.html">

  
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:image" content="https://scaleoutsean.github.io/assets/images/eseries-santricity-ef600-200g-das.png">
  

  



  <meta property="article:published_time" content="2026-02-19T00:00:00+08:00">






<link rel="canonical" href="https://scaleoutsean.github.io/2026/02/19/linux-nvme-roce-ef-series.html">







  <meta name="google-site-verification" content="F6q7vIwQ2G0j8tk-KL9rAXOMLXMDMMUkEz4fRs1Nnnc" />


  <meta name="msvalidate.01" content="7cf5b7d96a77410a8ad035f764dc81b3">





<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Acting Technologist Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  window.enable_copy_code_button = true;
</script>

<script src="https://cdn.counter.dev/script.js" data-id="83dc700a-b821-4e57-9425-02b8336a9456" data-utcoffset="0"></script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <link href="https://fonts.googleapis.com/css?family=PT+Sans+Narrow:400,700|PT+Serif:400,700,400italic" rel="stylesheet" type="text/css">


  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
          <a class="site-logo" href="/"><img src="/assets/images/scaleoutsean-acting-technologist.png" alt="Acting Technologist"></a>
        
        <a class="site-title" href="/">
          Acting Technologist
          <span class="site-subtitle">human action through technology</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/archive/"
                
                
              >Archive</a>
            </li><li class="masthead__menu-item">
              <a
                href="/categories/"
                
                
              >Categories</a>
            </li><li class="masthead__menu-item">
              <a
                href="/tags/"
                
                
              >Tags</a>
            </li><li class="masthead__menu-item">
              <a
                href="/projects.html"
                
                
              >Projects</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main">
  


  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Linux, NVMe/RoCE and HA NetApp EF-Series NVMe-oF storage">
    <meta itemprop="description" content="Linux NVMe/RoCE clients with NetApp EF-Series NVMe-oF">
    <meta itemprop="datePublished" content="2026-02-19T00:00:00+08:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://scaleoutsean.github.io/2026/02/19/linux-nvme-roce-ef-series.html" itemprop="url">Linux, NVMe/RoCE and HA NetApp EF-Series NVMe-oF storage
</a>
          </h1>
          

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2026-02-19T00:00:00+08:00">2026-02-19 00:00</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          15 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#introduction">Introduction</a></li><li><a href="#network">Network</a></li><li><a href="#hosts">Hosts</a><ul><li><a href="#ubuntu-24044">Ubuntu 24.04.4</a></li><li><a href="#rocky-linux-101">Rocky Linux 10.1</a></li><li><a href="#proxmox-91-debian-trixie">Proxmox 9.1 (Debian Trixie)</a></li><li><a href="#suse-enterprise-linux-sles">SuSE Enterprise Linux (SLES)</a></li><li><a href="#discover-and-connect">Discover and connect</a></li><li><a href="#multipathing">Multipathing</a><ul><li><a href="#nvme-ana">NVMe ANA</a></li><li><a href="#multipath-tools-dm-mp">multipath-tools (DM-MP)</a></li></ul></li></ul></li><li><a href="#automation">Automation</a><ul><li><a href="#ansible">Ansible</a></li><li><a href="#other-automation">Other automation</a></li></ul></li><li><a href="#conclusion">Conclusion</a></li></ul>
            </nav>
          </aside>
        
        <h2 id="introduction">Introduction</h2>

<p>The EF-Series documentation related to NVMe/RoCE is <a href="/2026/02/19/the-shocking-truth-about-ef600-200g-ports.html">too basic</a> and, similarly to the old <a href="/2023/09/22/ubuntu-lts-netapp-eseries-iser.html">iSER-related post</a>, this post aims help E-Series users get started with NVMe/RoCE.</p>

<p>This post several will likely be updated several times until it's complete.</p>

<h2 id="network">Network</h2>

<p>Two main things I'd like to mention here:</p>

<ul>
  <li>DAS is supported and you can run highly available applications on hosts connected to E/EF Series without storage switches</li>
  <li>E/EF uses dual storage fabric. Each controller has targets available on at least two logically separate networks (e.g. 1.0/24 on Controller 1 Port A, and 2.0/24 on Controller 1 Port B)</li>
</ul>

<p>This example depicts a DAS setup of three servers with EF600 (200G NVMe/RoCE). The tricky part (described at the first link of this post) is we don't have enough NICs on the clients to fully utilize 200G, so this is workable but doesn't always fully utilize EF600 controller resources. (The green port is the "virtual" NIC - see the linked post for more).</p>

<p><img src="/assets/images/eseries-santricity-multipath-ef600-200g-switched.png" alt="NVMe/RoCE quad fabric SAN" /></p>

<p>With dual-ported NICs (or enough client NICs in any case), we would be able to do better.</p>

<p><img src="/assets/images/eseries-santricity-multipath-ef600-100g-das.png" alt="NVMe/ROCE with DAS" /></p>

<p>Whether or not you need to "max it out" really "depends" on objectives. Notice how you can't direct-attach all three hosts to storage in the second (supposedly "better") approach. So what's really better is debatable.</p>

<p>Next, I want to highlight a few things from the Web UI because the E-Series documentation for the Web UI has no screenshots (!).</p>

<p>First, remember to click on "Show" to show extra settings where you will find the MTU setting. One applies to both (physical and virtual NIC on a EF600 with 200G).</p>

<p><img src="/assets/images/eseries-nvme-of-santricity-ui-nvme-port-config-01.png" alt="SANtricity UI wizard for NVMe/RoCE configuration step 1" /></p>

<p>Second, this is the part that I blogged about in the other post. I'd always override these suggestions with "adjacent" networks and shorter IP addresses, but it's up to you.</p>

<p><img src="/assets/images/eseries-nvme-of-santricity-ui-nvme-port-config-02.png" alt="SANtricity UI wizard for NVMe/RoCE configuration step 2" /></p>

<h2 id="hosts">Hosts</h2>

<p>EF-Series has a fixed list of supported operating systems which is presently "RPM-centric" (Red Hat, SLES, Rocky Linux).</p>

<p>This doesn't mean other distributions don't work.</p>

<p>For any Linux distribution, I'd recommend the following approach:</p>

<ul>
  <li>Reference <a href="https://docs.netapp.com/us-en/e-series/config-linux/nvme-roce-discover-connect-storage-host-task.html">the official steps</a></li>
  <li>Reference steps from your Linux distribution <strong>and</strong> switch vendor (if you use switch(es))</li>
</ul>

<p>Usually OS documentation is more detailed and enterprise network switch vendors' even better. Neither of these need special "EF-Series-related" instructions. Dual storage fabric mention above is handled automatically thanks to NVMe ANA: unlike with SCSI, where multipathing is usually handled by Device Mapper, NVMe handles it in kernel.</p>

<p>You can still use DM-MP as a "unified" multipathing management tool for SCSI and NVMe, but built-in multipathing is better.</p>

<p>Special cases where DM-MP may still be marginally more useful today include failback, as DM-MP can force it with <code class="language-plaintext highlighter-rouge">immediate</code>, and ANA is currently less customizable and path failback (to a recovered controller) isn't easy to force although that should improve with time. Personally, I think immediate failback isn't worth the trade-offs if you're on a modern Linux distribution such as Ubuntu 24.04 or RHEL 10. If your NVMe stack is old (e.g. RHEL 8) - then maybe.</p>

<p>To start, install NVMe CLI and make sure it works:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nvme show-hostnqn
modprobe nvme_rdma <span class="o">&amp;&amp;</span> <span class="nb">echo</span> <span class="s2">"nvme_rdma"</span> <span class="o">&gt;</span> /etc/modules-load.d/nvme_rdma.conf
</code></pre></div></div>

<p>Configure NVMe/RoCE on E-Series and gather network details from the array.</p>

<p>This example is a weird one (EF600 with 200G NIC, and only one port per controller). Yours will likely be different.</p>

<p>Controller A:</p>

<ul>
  <li>Physical: Channel 3, Port: 2a, Location: HIC 2 Port 2a, speed: 40 Gb/s Full duplex
    <ul>
      <li>IP 192.168.1.1/24, MAC D0:39:EA:44:xx:xx</li>
    </ul>
  </li>
  <li>Virtual: Channel 1, Port: 2a, Location HIC 2 Port: 2a
    <ul>
      <li>IP 192.168.2.1/24, MAC D0:39:EA:44:xx:xx</li>
    </ul>
  </li>
</ul>

<p>Controller B:</p>

<ul>
  <li>Physical: Channel 3, Port: 2a, Location: HIC 2 Port 2a, speed: 40 Gb/s Full duplex
    <ul>
      <li>IP 192.168.1.2/24, MAC D0:39:EA:44:xx:xx</li>
    </ul>
  </li>
  <li>Virtual: Channel 1, Port: 2a, Location HIC 2 Port 2a
    <ul>
      <li>IP 192.168.2.2/24, MAC D0:39:EA:44:xx:xx</li>
    </ul>
  </li>
</ul>

<p>I'm not using Port B on each controller and you probably would (if you purchased that hardware).</p>

<p>My deviation from the default IP configuration offered by SANtricity's wizard:</p>
<ul>
  <li>I use simpler, shorter IPv4 addresses that are easier to remember</li>
  <li>I have only two storage fabrics (1.x/24 and 2.x/24) with a single path to each</li>
</ul>

<p>You may have more than just 2 NICs on each host. I have just two, which corresponds to the E-Series controllers' configuration, and they're presently unconfigured (no IP address, MTU is 1500, etc.):</p>

<pre><code class="language-raw">4: ens3f0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000
    link/ether b8:59:9f:37:82:68 brd ff:ff:ff:ff:ff:ff
    altname enp134s0f0
5: ens3f1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000
    link/ether b8:59:9f:37:82:69 brd ff:ff:ff:ff:ff:ff
</code></pre>

<p>NIC names may differ across distributions and versions, but all the rest is common to all.</p>

<p>I'll provide some working examples for everyone's reference.</p>

<h3 id="ubuntu-24044">Ubuntu 24.04.4</h3>

<p>Ubuntu wasn't (and still isn't) on the list of supported iSER clients, but it works fine with EF-Series iSER as far as I'm concerned. There's no reason it wouldn't work well here, especially considering NVIDIA's participation in the Ubuntu ecosystem.</p>

<p>How to connect? Just follow the steps from the official documentation.</p>

<p>I created <code class="language-plaintext highlighter-rouge">99-eseries-ansible-&lt;nic&gt;.yaml</code> files, one for each NIC (ens3f0np0, ens3f1np1), in <code class="language-plaintext highlighter-rouge">/etc/netplan/</code>. For ens3f0np0:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Managed by NetApp E-Series Ansible</span>
<span class="na">network</span><span class="pi">:</span>
  <span class="na">version</span><span class="pi">:</span> <span class="m">2</span>
  <span class="na">renderer</span><span class="pi">:</span> <span class="s">networkd</span>
  <span class="na">ethernets</span><span class="pi">:</span>
    <span class="na">ens3f0np0</span><span class="pi">:</span>
      <span class="na">addresses</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">192.168.1.12/24</span>
      <span class="na">mtu</span><span class="pi">:</span> <span class="m">9000</span>
</code></pre></div></div>

<p>For ens3f1np1:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Managed by NetApp E-Series Ansible</span>
<span class="na">network</span><span class="pi">:</span>
  <span class="na">version</span><span class="pi">:</span> <span class="m">2</span>
  <span class="na">renderer</span><span class="pi">:</span> <span class="s">networkd</span>
  <span class="na">ethernets</span><span class="pi">:</span>
    <span class="na">ens3f1np1</span><span class="pi">:</span>
      <span class="na">addresses</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">192.168.2.12/24</span>
      <span class="na">mtu</span><span class="pi">:</span> <span class="m">9000</span>
</code></pre></div></div>

<p>I configured Ubuntu 24.04 for the E-Series NVMe/RoCE environment above and it was able to connect without any issues.</p>

<h3 id="rocky-linux-101">Rocky Linux 10.1</h3>

<p>Rocky Linux is even easier, as the official instructions for Red Hat Enterprise Linux work out of the box copy-paste style.</p>

<p>My setup was the same (dual-ported ConnectX-6 per host, connected to dual storage fabric).</p>

<p>Use <code class="language-plaintext highlighter-rouge">nmcli</code> (or GUI, or Ansible, etc.) to create a connection for each storage network. Example for one of two <code class="language-plaintext highlighter-rouge">nmconnection</code> files (one per NIC):</p>

<div class="language-ini highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">[connection]</span><span class="w">
</span><span class="py">id</span><span class="p">=</span><span class="s">ens3f0</span>
<span class="py">uuid</span><span class="p">=</span><span class="s">&lt;GENERATE-YOUR-OWN&gt; # recommended</span>
<span class="py">type</span><span class="p">=</span><span class="s">ethernet</span>
<span class="py">interface-name</span><span class="p">=</span><span class="s">ens3f0</span>
<span class="w">
</span><span class="nn">[ethernet]</span><span class="w">
</span><span class="py">mtu</span><span class="p">=</span><span class="s">9000</span>
<span class="w">
</span><span class="nn">[ipv4]</span><span class="w">
</span><span class="py">address1</span><span class="p">=</span><span class="s">192.168.1.11/24</span>
<span class="py">method</span><span class="p">=</span><span class="s">manual</span>
<span class="w">
</span><span class="nn">[ipv6]</span><span class="w">
</span><span class="py">addr-gen-mode</span><span class="p">=</span><span class="s">default</span>
<span class="py">method</span><span class="p">=</span><span class="s">auto</span>
<span class="w">
</span><span class="nn">[proxy]</span><span class="w">
</span></code></pre></div></div>

<p>If you use GUI or TUI to configure, that's certainly possible, but creates ugly <code class="language-plaintext highlighter-rouge">'Profile &lt;number&gt;.nmconnection'</code> files.</p>

<p><img src="/assets/images/eseries-nvme-of-server_tui_nic_1.png" alt="Network configuration in TUI" /></p>

<p>MTU 9000 is entirely optional. If you use storage switches, you may need a few extra bytes on switch MTU for VLAN and other overheads (or you may need to set a lower MTU here, such as 8800).</p>

<p><img src="/assets/images/eseries-nvme-of-server_tui_nic_2.png" alt="Network adapter configuration in TUI" /></p>

<p>Notice the above subnet is 23, which is a workaround for "multiple fabrics per NIC" described in the referenced post. I actually used /24 in this environment.</p>

<h3 id="proxmox-91-debian-trixie">Proxmox 9.1 (Debian Trixie)</h3>

<p>There's nothing special or unique here - just the old school network config format.</p>

<p>I used "pin network interfaces" in Proxmox which gave the NICs nice names (<code class="language-plaintext highlighter-rouge">nic1</code> and <code class="language-plaintext highlighter-rouge">nic2</code> were my public network NICs, and <code class="language-plaintext highlighter-rouge">nic3</code> and <code class="language-plaintext highlighter-rouge">nic4</code> were for storage, although they could allow Proxmox cluster traffic as well).</p>

<pre><code class="language-raw">auto nic3
iface nic3 inet static
        address 192.168.1.13/24
        mtu 9000

auto nic4
iface nic4 inet static
        address 192.168.2.13/24
        mtu 9000
</code></pre>

<p>I configured these using the Web UI.</p>

<p><img src="/assets/images/eseries-nvme-of-proxmox-nvme-nic-01.png" alt="NVMe/RoCE configuration in Proxmox VE 9" /></p>

<p>When you edit these in Proxmox UI, remember to set them to auto-start (that <code class="language-plaintext highlighter-rouge">auto nic&lt;number&gt;</code> in the Proxmox configuration file above).  Apply this configuration to bring them up.</p>

<p><img src="/assets/images/eseries-nvme-of-proxmox-nvme-nic-02.png" alt="NVMe/RoCE configuration in Proxmox VE 9" /></p>

<p>Add Proxmox host NQN to a host (and/or host group), create a volume on E-Series, present to the host(s), discover and connect from the hosts. You can do all these things with Ansible or other automation, of course.</p>

<p>I started with a 10G volume that was too small and I had to resize it after I realized it was too small. I deleted LVM and ran <code class="language-plaintext highlighter-rouge">pveresize /dev/nvme1n1</code> which solved the problem. After a refresh in the Web UI I saw the new size (100 GB) was able to see the new size.</p>

<p>For shits-n-gigles, I uploaded Ubuntu 26.04 Snapshot 3 ISO to <code class="language-plaintext highlighter-rouge">local</code> repo and created a VM on the SANtricity volume. One of the nice things here is you can get QoS on VMs without E-Series doing anything. Sure, it's not as great as it is with SolidFire, but it's simple enough and familiar to PVE admins. It works, and there's nothing new to learn.</p>

<h3 id="suse-enterprise-linux-sles">SuSE Enterprise Linux (SLES)</h3>

<p>I haven't tried SLES (yet), but the good thing about it is SLES has always been well documented and well behaved and that extends to NVMe/RoCE - the best documentation and probably the best QA as well.</p>

<p>And it's officially supported by E-Series.</p>

<h3 id="discover-and-connect">Discover and connect</h3>

<p>You probably don't need <code class="language-plaintext highlighter-rouge">--hostid</code> if you have the file in the default location, but I mention it here becuase that's different from iSCSI where IQN corresponds to NQN (<code class="language-plaintext highlighter-rouge">hostnqn</code>), not <code class="language-plaintext highlighter-rouge">hostid</code> which you shouldn't forget about.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nvme discover <span class="nt">-t</span> rdma <span class="nt">-a</span> 192.168.1.2 <span class="nt">--hostid</span><span class="o">=</span>/etc/nvme/hostid
nvme connect-all <span class="nt">-t</span> rdma <span class="nt">-a</span> 192.168.1.2 <span class="nt">--hostid</span><span class="o">=</span>/etc/nvme/hostid
</code></pre></div></div>

<p>You need to have both <code class="language-plaintext highlighter-rouge">hostnqn</code> and <code class="language-plaintext highlighter-rouge">hostid</code> in <code class="language-plaintext highlighter-rouge">/etc/nvme/</code>. And here it's <code class="language-plaintext highlighter-rouge">-t rdma</code>, not <code class="language-plaintext highlighter-rouge">-t tcp</code> (which is for NVMe/TCP, the poor man's NVMe/RDMA) and will eventually get you if you blindly copy stuff off the Internet like I did.</p>

<p>If <code class="language-plaintext highlighter-rouge">ping</code> works but discovery doesn't, you may have a MTU (or RoCEv2) problem. NVMe CLI may give misleading error messages (I know because I had a MTU problem and was mislead about it with some nonsense error messages about <code class="language-plaintext highlighter-rouge">hostid</code>).</p>

<p>Pay attention to the <code class="language-plaintext highlighter-rouge">--ctrl-loss-tmo</code> parameter. The NetApp documentation suggests <code class="language-plaintext highlighter-rouge">3600</code> (seconds), while <code class="language-plaintext highlighter-rouge">-1</code> is allowed and means "forever".  After that timeout limit is hit without, the (dis)connected device flips to read-only mode. One hour is reasonable, but so is eight or 24.</p>

<p>Next, auto-connect service. Add all targets if you want the host to reconnect.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">echo</span> <span class="s2">"-t rdma -a 192.168.1.1"</span> | <span class="nb">sudo tee</span> <span class="nt">-a</span> /etc/nvme/discovery.conf
<span class="c"># discover on all NVMe/RoCE ports</span>
<span class="c"># enable NVMe-oF auto-connect service</span>
<span class="nb">sudo </span>systemctl <span class="nb">enable </span>nvmf-autoconnect.service
</code></pre></div></div>

<p>You wouldn't need this in a Kubernetes environment with dynamic PVCs that are scheduled on demand, and don't need to survive reboots on their own.</p>

<p>Also note the modern way is <code class="language-plaintext highlighter-rouge">/etc/nvme/discovery.json</code>, while <code class="language-plaintext highlighter-rouge">discovery.conf</code> still works as of now.</p>

<h3 id="multipathing">Multipathing</h3>

<h4 id="nvme-ana">NVMe ANA</h4>

<p>Use <code class="language-plaintext highlighter-rouge">nvme show-topology</code> to to view paths to storage targets.</p>

<p>With multiple paths, some (or one, if you have just two) will likely be <code class="language-plaintext highlighter-rouge">optimized</code> and other(s) <code class="language-plaintext highlighter-rouge">non-optimized</code>. (Theoretically you may see two optimized if they're both connected to the same controller that's optimal for the volume, but most people would connect to two controlles.)</p>

<p>I've mentioned earlier that failback in NVMe/RoCE seems hard to control. You may need to fiddle with these to push a volume from the non-preferred controller to the preferred. I haven't played with this yet.</p>

<p>This could probably be automated with a "watcher" service, but I'd rather not touch that unless the controller with failed over LUNs was maxed out. <a href="https://github.com/scaleoutsean/eseries-santricity-collector">E-Series SANtricity Collector</a> collects volume location (preferred vs. non-preferred), among other things, and I'd rather set alerts in Grafana or InfluxDB 3 and deal with these on a case-by-cases basis.</p>

<h4 id="multipath-tools-dm-mp">multipath-tools (DM-MP)</h4>

<p>DM-MP is not recommended for NVMe-oF as native kernel support is preferred, but for the sake of completeness:</p>
<ul>
  <li>E-Series support was added in multipath-tools <a href="https://github.com/opensvc/multipath-tools/blob/0.12.1/NEWS.md#other-5">0.9.4 (2022/12)</a></li>
  <li>Enable DM-MP for NVMe with <code class="language-plaintext highlighter-rouge">nvme_core.multipath=N</code> kernel parameter (which makes it manageable by DM-MP)</li>
  <li>Use <code class="language-plaintext highlighter-rouge">multipath -T</code> to show the default package settings for connected devices and avoid using deprecated options</li>
  <li>Even if you use DM-MP for some NVMe targets, you can blacklist selected products</li>
</ul>

<p>I have not tested DM-MP with NVMe/RoCE yet, given it's "legacy" label, but you may consider these if you try.</p>

<p>Whitelist E-Series NVMe (not SCSI!) storage in DM-MP:</p>

<pre><code class="language-raw">devices {
  device {
    vendor "NVME"
    product "NetApp E-Series*"
    path_grouping_policy group_by_prio
    failback immediate # it may be better to consider: manual, &lt;timeout_seconds&gt;, followover
    no_path_retry 30
    # no_path_retry fail # recommended for SLES 15 for cluster file system clients
    # SLES proposes "multi-queue" with the following kernel parameters: scsi_mod.use_blk_mq=1 dm_mod.use_blk_mq=1
  }
}
</code></pre>

<p>Blacklist E-Series NVMe from DM-MP:</p>

<pre><code class="language-raw">blacklist {
  device {
    vendor  "NVME"
    product "NetApp E-Series*"
  }
}
</code></pre>

<h2 id="automation">Automation</h2>

<p>From a SANtricity admin's host management perspective NVMe is quite similar to iSCSI, it's just hosts identified by an NQN.</p>

<p>The slightly tricky part (at least it was for me, as I had nowhere to copy this from) is the "<code class="language-plaintext highlighter-rouge">PUT /hosts</code>" part for NVMe/RoCE host configuration steps, which is easier to do in Swagger than in Ansible (where it's supposed to be "easy").</p>

<p>This is how to create a stand-alone Linux host:</p>

<ul>
  <li>The host name for me was <code class="language-plaintext highlighter-rouge">h3</code></li>
  <li>Port needs a <code class="language-plaintext highlighter-rouge">&lt;hostLabel&gt;_&lt;portNumber&gt;</code> value, i.e. <code class="language-plaintext highlighter-rouge">h3_1</code>, a <code class="language-plaintext highlighter-rouge">port</code> string (landmine!) which isn't at all a port but an NQN, and a type (<code class="language-plaintext highlighter-rouge">nvmeof</code> - at least one value that is as one might expect).</li>
</ul>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"h3"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"hostType"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"index"</span><span class="p">:</span><span class="w"> </span><span class="mi">28</span><span class="w">
  </span><span class="p">},</span><span class="w">
  </span><span class="nl">"ports"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
    </span><span class="p">{</span><span class="w">
      </span><span class="nl">"label"</span><span class="p">:</span><span class="w"> </span><span class="s2">"h3_1"</span><span class="p">,</span><span class="w">
      </span><span class="nl">"port"</span><span class="p">:</span><span class="w"> </span><span class="s2">" nqn.2014-08.org.nvmexpress:uuid:b6087fac-aef6-4e75-85c1-abd7078c94f9"</span><span class="p">,</span><span class="w">
      </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"nvmeof"</span><span class="w">
    </span><span class="p">}</span><span class="w">
  </span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>Once you have that in place, the rest is almost the same as with iSCSI: create a volume, map it to the host(s) identified <code class="language-plaintext highlighter-rouge">hostRef</code> (or <code class="language-plaintext highlighter-rouge">clusterRef</code> if mapping to a cluster of hosts), go to your host(s) to discover, connect and start cracking!</p>

<p>If you automate, you probably don't want to manually configure NVMe/RoCE adapters - especially not on Red Hat and Rocky Linux where that results in ugly NM connection files. My preferred approach is to configure storage NICs via automation and not use Network Manager TUI/GUI at all.</p>

<h3 id="ansible">Ansible</h3>

<p>For this, use the official E-Series Ansible Collection.</p>

<p>In this playbook, I use an EF600 array with 200G NICs. Why's that relevant? You may see the previous post (the one mentioned at the top) - the odd part is NIC speed is set only one one of the adapters, because the other one is "virtual". The less odd part is I had a 40G cable. Most real-life configurations won't look exactly like this.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>TASK <span class="o">[</span>Configure Controller A NVMe Interfaces] <span class="k">********************************************</span>
skipping: <span class="o">[</span>server1] <span class="o">=&gt;</span> <span class="o">(</span><span class="nv">item</span><span class="o">={</span><span class="s1">'channel'</span>: 3, <span class="s1">'address'</span>: <span class="s1">'192.168.1.1'</span>, <span class="s1">'speed'</span>: 40<span class="o">})</span>
skipping: <span class="o">[</span>server1] <span class="o">=&gt;</span> <span class="o">(</span><span class="nv">item</span><span class="o">={</span><span class="s1">'channel'</span>: 1, <span class="s1">'address'</span>: <span class="s1">'192.168.2.1'</span><span class="o">})</span>
skipping: <span class="o">[</span>server1]
skipping: <span class="o">[</span>server2] <span class="o">=&gt;</span> <span class="o">(</span><span class="nv">item</span><span class="o">={</span><span class="s1">'channel'</span>: 3, <span class="s1">'address'</span>: <span class="s1">'192.168.1.1'</span>, <span class="s1">'speed'</span>: 40<span class="o">})</span>
skipping: <span class="o">[</span>server2] <span class="o">=&gt;</span> <span class="o">(</span><span class="nv">item</span><span class="o">={</span><span class="s1">'channel'</span>: 1, <span class="s1">'address'</span>: <span class="s1">'192.168.2.1'</span><span class="o">})</span>
skipping: <span class="o">[</span>server2]
changed: <span class="o">[</span>array1 -&gt; localhost] <span class="o">=&gt;</span> <span class="o">(</span><span class="nv">item</span><span class="o">={</span><span class="s1">'channel'</span>: 3, <span class="s1">'address'</span>: <span class="s1">'192.168.1.1'</span>, <span class="s1">'speed'</span>: 40<span class="o">})</span>
changed: <span class="o">[</span>array1 -&gt; localhost] <span class="o">=&gt;</span> <span class="o">(</span><span class="nv">item</span><span class="o">={</span><span class="s1">'channel'</span>: 1, <span class="s1">'address'</span>: <span class="s1">'192.168.2.1'</span><span class="o">})</span>

TASK <span class="o">[</span>Configure Controller B NVMe Interfaces] <span class="k">********************************************</span>
skipping: <span class="o">[</span>server1] <span class="o">=&gt;</span> <span class="o">(</span><span class="nv">item</span><span class="o">={</span><span class="s1">'channel'</span>: 3, <span class="s1">'address'</span>: <span class="s1">'192.168.1.2'</span>, <span class="s1">'speed'</span>: 40<span class="o">})</span>
skipping: <span class="o">[</span>server1] <span class="o">=&gt;</span> <span class="o">(</span><span class="nv">item</span><span class="o">={</span><span class="s1">'channel'</span>: 1, <span class="s1">'address'</span>: <span class="s1">'192.168.2.2'</span><span class="o">})</span>
skipping: <span class="o">[</span>server2] <span class="o">=&gt;</span> <span class="o">(</span><span class="nv">item</span><span class="o">={</span><span class="s1">'channel'</span>: 3, <span class="s1">'address'</span>: <span class="s1">'192.168.1.2'</span>, <span class="s1">'speed'</span>: 40<span class="o">})</span>
skipping: <span class="o">[</span>server1]
skipping: <span class="o">[</span>server2] <span class="o">=&gt;</span> <span class="o">(</span><span class="nv">item</span><span class="o">={</span><span class="s1">'channel'</span>: 1, <span class="s1">'address'</span>: <span class="s1">'192.168.2.2'</span><span class="o">})</span>
skipping: <span class="o">[</span>server2]
changed: <span class="o">[</span>array1 -&gt; localhost] <span class="o">=&gt;</span> <span class="o">(</span><span class="nv">item</span><span class="o">={</span><span class="s1">'channel'</span>: 3, <span class="s1">'address'</span>: <span class="s1">'192.168.1.2'</span>, <span class="s1">'speed'</span>: 40<span class="o">})</span>
changed: <span class="o">[</span>array1 -&gt; localhost] <span class="o">=&gt;</span> <span class="o">(</span><span class="nv">item</span><span class="o">={</span><span class="s1">'channel'</span>: 1, <span class="s1">'address'</span>: <span class="s1">'192.168.2.2'</span><span class="o">})</span>
</code></pre></div></div>

<p>Unlike with iSCSI, here we use NQNs to create (register) hosts on SANtricity.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>TASK <span class="o">[</span>Debug Host NQN] <span class="k">********************************************************************</span>
ok: <span class="o">[</span>server1] <span class="o">=&gt;</span> <span class="o">{</span>
    <span class="s2">"msg"</span>: <span class="s2">"Host NQN for server1 is nqn.2014-08.org.nvmexpress:uuid:b6087fac-aef6-4e75-85c1-abd7078c94f9"</span>
<span class="o">}</span>
ok: <span class="o">[</span>server2] <span class="o">=&gt;</span> <span class="o">{</span>
    <span class="s2">"msg"</span>: <span class="s2">"Host NQN for server2 is nqn.2014-08.org.nvmexpress:uuid:cd525b26-df3b-11e9-b457-3a68dd166ea7"</span>
<span class="o">}</span>
skipping: <span class="o">[</span>array1]
</code></pre></div></div>

<p>The entire workflow works fine with NVMe, including volume creation and mapping to hosts.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>TASK <span class="o">[</span>Create Host Volumes on E-Series Array] <span class="k">*********************************************</span>
skipping: <span class="o">[</span>array1] <span class="o">=&gt;</span> <span class="o">(</span><span class="nv">item</span><span class="o">={</span><span class="s1">'name'</span>: <span class="s1">'data'</span>, <span class="s1">'size'</span>: 10, <span class="s1">'unit'</span>: <span class="s1">'gb'</span>, <span class="s1">'lun'</span>: 1, <span class="s1">'raid_level'</span>: <span class="s1">'raid1'</span><span class="o">})</span>
skipping: <span class="o">[</span>array1]
changed: <span class="o">[</span>server1 -&gt; localhost] <span class="o">=&gt;</span> <span class="o">(</span><span class="nv">item</span><span class="o">={</span><span class="s1">'name'</span>: <span class="s1">'data'</span>, <span class="s1">'size'</span>: 10, <span class="s1">'unit'</span>: <span class="s1">'gb'</span>, <span class="s1">'lun'</span>: 1, <span class="s1">'raid_level'</span>: <span class="s1">'raid1'</span><span class="o">})</span>
changed: <span class="o">[</span>server2 -&gt; localhost] <span class="o">=&gt;</span> <span class="o">(</span><span class="nv">item</span><span class="o">={</span><span class="s1">'name'</span>: <span class="s1">'data'</span>, <span class="s1">'size'</span>: 10, <span class="s1">'unit'</span>: <span class="s1">'gb'</span>, <span class="s1">'lun'</span>: 1, <span class="s1">'raid_level'</span>: <span class="s1">'raid1'</span><span class="o">})</span>

TASK <span class="o">[</span>Map Volumes to Host] <span class="k">***************************************************************</span>
skipping: <span class="o">[</span>array1] <span class="o">=&gt;</span> <span class="o">(</span><span class="nv">item</span><span class="o">={</span><span class="s1">'name'</span>: <span class="s1">'data'</span>, <span class="s1">'size'</span>: 10, <span class="s1">'unit'</span>: <span class="s1">'gb'</span>, <span class="s1">'lun'</span>: 1, <span class="s1">'raid_level'</span>: <span class="s1">'raid1'</span><span class="o">})</span>
skipping: <span class="o">[</span>array1]
changed: <span class="o">[</span>server2 -&gt; localhost] <span class="o">=&gt;</span> <span class="o">(</span><span class="nv">item</span><span class="o">={</span><span class="s1">'name'</span>: <span class="s1">'data'</span>, <span class="s1">'size'</span>: 10, <span class="s1">'unit'</span>: <span class="s1">'gb'</span>, <span class="s1">'lun'</span>: 1, <span class="s1">'raid_level'</span>: <span class="s1">'raid1'</span><span class="o">})</span>
changed: <span class="o">[</span>server1 -&gt; localhost] <span class="o">=&gt;</span> <span class="o">(</span><span class="nv">item</span><span class="o">={</span><span class="s1">'name'</span>: <span class="s1">'data'</span>, <span class="s1">'size'</span>: 10, <span class="s1">'unit'</span>: <span class="s1">'gb'</span>, <span class="s1">'lun'</span>: 1, <span class="s1">'raid_level'</span>: <span class="s1">'raid1'</span><span class="o">})</span>

PLAY RECAP <span class="k">*******************************************************************************</span>
array1                     : <span class="nv">ok</span><span class="o">=</span>3    <span class="nv">changed</span><span class="o">=</span>2    <span class="nv">unreachable</span><span class="o">=</span>0    <span class="nv">failed</span><span class="o">=</span>0    <span class="nv">skipped</span><span class="o">=</span>14   <span class="nv">rescued</span><span class="o">=</span>0    <span class="nv">ignored</span><span class="o">=</span>0
server1                    : <span class="nv">ok</span><span class="o">=</span>42   <span class="nv">changed</span><span class="o">=</span>3    <span class="nv">unreachable</span><span class="o">=</span>0    <span class="nv">failed</span><span class="o">=</span>0    <span class="nv">skipped</span><span class="o">=</span>35   <span class="nv">rescued</span><span class="o">=</span>0    <span class="nv">ignored</span><span class="o">=</span>0
server2                    : <span class="nv">ok</span><span class="o">=</span>41   <span class="nv">changed</span><span class="o">=</span>5    <span class="nv">unreachable</span><span class="o">=</span>0    <span class="nv">failed</span><span class="o">=</span>0    <span class="nv">skipped</span><span class="o">=</span>33   <span class="nv">rescued</span><span class="o">=</span>0    <span class="nv">ignored</span><span class="o">=</span>0
</code></pre></div></div>

<p>Some observations:</p>

<ul>
  <li>Lookup methods (pools, volumes, hosts) are all weak, as if the collection was created to configure sutff and not manage it day to day</li>
  <li>Create volume module doesn't return volume object (!?)</li>
  <li>NVMe-related features are weak (EF600 was launched in 2019!). Yes, you can create NVMe-oF hosts, but you can't look up targets and you have to build own Ansible tasks for that, which is disappointing</li>
  <li>No <code class="language-plaintext highlighter-rouge">raid_level</code> support for volume create task in DDP (RAID setting was added in SANtricity 11.7 years ago!)</li>
  <li>Working around these quirks and adding own tasks that do what the Collection doesn't is fine if you have very repetitive tasks. Otherwise you end up doing what you could create and run faster in a proper language without these extra layers</li>
</ul>

<p>But, it works.</p>

<p>I created a playbook that configures NICs and storage for Rocky 10.1, creates volumes on storage array, and deploys PostgreSQL to Podman that uses these volumes. It was an interesting experience because Trident CSI also supports Docker in non-HA storage mode (because Docker Swam doesn't support it).</p>

<p>Compared to Trident with Docker, this is actually a better experience because Ansible gives you the same storage experience (non-HA provisioning to host-bound containers) with some extras that you'd probably use in any case: with Trident for Docker you'd still have some tools for container management, and here you already have them in Ansible. And you don't have to deal with a storage provisioner just to create volumes and present them to a host.</p>

<h3 id="other-automation">Other automation</h3>

<p>Frequent readers of this blog may recall I recently released other integrations for SANtricity (Python, PowerShell, Go), so Ansible is is just one example.</p>

<p>They all "work", but need support for NVMe/RoCE (related to creating NVMe/RoCE host and host-groups).</p>

<p>As an example, <code class="language-plaintext highlighter-rouge">Get-SANtricityMappingsReport</code> from my PowerShell module shows this environment, but <code class="language-plaintext highlighter-rouge">New-SANtricityHost</code> needed modifications.</p>

<p><img src="/assets/images/eseries-nvme-of-santricity-powershell-report.png" alt="Get-SANtricityMappingsReport" /></p>

<p>After those modifications, now we can add <code class="language-plaintext highlighter-rouge">h3</code>, that Proxmox NVMe-oF host, like this.</p>

<p><img src="/assets/images/eseries-nvme-of-santricity-powershell-new-nvmeof-host-proxmox.png" alt="New-SANtricityHost" /></p>

<p>The <code class="language-plaintext highlighter-rouge">Get</code> cmdlet shows configured NVMe hosts (and filtering and formatting is up to you).</p>

<p><img src="/assets/images/eseries-nvme-of-santricity-powershell-hosts.png" alt="Get-SANtricityHost" /></p>

<p>And now we have everything we need to create a SANtricity version of <a href="https://github.com/scaleoutsean/firemox">Firemox</a>.</p>

<p>My SANtricity Go client, Terraform Provider SANtricity and SANtricity CSI have all been updated. And because it works, we SANtricity Go and Terraform Provider can configure <code class="language-plaintext highlighter-rouge">hosts</code> object, which is where a host's port(s) are defined. And SANtricity CSI is NVMe-oF aware.</p>

<p><img src="/assets/images/santricity-csi-v1-02-startup.png" alt="CSI container startup" /></p>

<p>My <code class="language-plaintext highlighter-rouge">santricity-client</code>, a Python client for SANtricity, has also been updated to handle NVMe/RDMA host objects (added in version 0.1.1). Subsequently I also added volume expand support since that wasn't implemented before.</p>

<p>There is an opportunity to add custom NVMe-focused commands/cmdlets to better serve NVMe/RoCE users because existing approaches (see my comments about Ansible Collection above) aren't optimal. Also, I may also need to update E-Series Performance Analyzer and E-Series SANtricity Collector to make sure NVMe/RoCE is not neglected. I'll blog about that later if I get it done.</p>

<h2 id="conclusion">Conclusion</h2>

<p>The simplicity, performance and (now) improving automationâ€¦ How nice! Automation used to be the weakest spot, but that's no longer the case. Now it's becoming a strength.</p>

<p>NVMe/RoCE is the best choice for NVMe-over-Fabric - it's not as "different" as Infiniband, it's more cost effective and works equally well for AI, Analytics and virtualization, all of which happen to be strong points of EF-Series. NVMe/TCP is even cheaper, but also slower (to me it's always been "a complicated version of iSCSI" that couldn't justify the switch).</p>

<p>I know, "virtualization" mentioned just above seems out of place, but let's remember that not all VI users expect storage to do everythig for them; many expect it to work reliably, be good value for money, perform well and behave. E-Series does all that, so yes, I say it's a very good choice for virtualization, too!</p>


        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/ef-series" class="page__taxonomy-item p-category" rel="tag">ef-series</a><span class="sep">, </span>
    
      <a href="/tags/ef600" class="page__taxonomy-item p-category" rel="tag">ef600</a><span class="sep">, </span>
    
      <a href="/tags/hardware" class="page__taxonomy-item p-category" rel="tag">hardware</a><span class="sep">, </span>
    
      <a href="/tags/netapp" class="page__taxonomy-item p-category" rel="tag">netapp</a><span class="sep">, </span>
    
      <a href="/tags/networking" class="page__taxonomy-item p-category" rel="tag">networking</a><span class="sep">, </span>
    
      <a href="/tags/nvme" class="page__taxonomy-item p-category" rel="tag">nvme</a><span class="sep">, </span>
    
      <a href="/tags/rdma" class="page__taxonomy-item p-category" rel="tag">rdma</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/e-series" class="page__taxonomy-item p-category" rel="tag">e-series</a><span class="sep">, </span>
    
      <a href="/categories/storage" class="page__taxonomy-item p-category" rel="tag">storage</a>
    
    </span>
  </p>


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time class="dt-published" datetime="2026-02-19T00:00:00+08:00">2026-02-19 00:00</time></p>

      </footer>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://x.com/intent/tweet?via=scaleoutSean&text=Linux%2C+NVMe%2FRoCE+and+HA+NetApp+EF-Series+NVMe-oF+storage%20https%3A%2F%2Fscaleoutsean.github.io%2F2026%2F02%2F19%2Flinux-nvme-roce-ef-series.html" class="btn btn--x" aria-label="Share on X" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on X">
    <i class="fab fa-fw fa-x-twitter" aria-hidden="true"></i><span> X</span>
  </a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://scaleoutsean.github.io/2026/02/19/linux-nvme-roce-ef-series.html" class="btn btn--linkedin" aria-label="Share on LinkedIn" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn">
    <i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span>
  </a>

</section>


      
  <nav class="pagination">
    
      <a href="/2026/02/15/updated-terraform-provider-solidfire.html" class="pagination--pager" title="Terraform provider for SolidFire update for Feb 2026">Previous</a>
    
    
      <a href="/2026/02/19/the-shocking-truth-about-ef600-200g-ports.html" class="pagination--pager" title="Connecting to 200G HICs on E-Series EF600">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
    
<div class="page__related">
  
  <h2 class="page__related-title">You may also enjoy</h2>
  <div class="grid__wrapper">
    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/2026/02/26/ibm-block-storage-cis-driver-santricity-fork.html" rel="permalink">Fork of IBM Block CSI driver for NetApp-E-Series
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2026-02-26T00:00:00+08:00">2026-02-26 00:00</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          10 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Unofficial fork of IBM Block CSI driver for your SANtricity storage
</p>
  </article>
</div>

    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/2026/02/19/the-shocking-truth-about-ef600-200g-ports.html" rel="permalink">Connecting to 200G HICs on E-Series EF600
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2026-02-19T00:00:00+08:00">2026-02-19 00:00</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          2 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Connecting to EF-Series EF600 200G HIC in DAS mode
</p>
  </article>
</div>

    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/2026/02/15/updated-terraform-provider-solidfire.html" rel="permalink">Terraform provider for SolidFire update for Feb 2026
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2026-02-15T00:00:00+08:00">2026-02-15 00:00</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          3 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Latest and greatest Terraform provider for SolidFire status update (2026/02)
</p>
  </article>
</div>

    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/2026/02/14/weak-security-ssh-client.html" rel="permalink">Overcome SSH invalid key length, SSH security annoyances
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2026-02-14T00:00:00+08:00">2026-02-14 00:00</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          3 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Fix for the annoying SSH invalid key error
</p>
  </article>
</div>

    
  </div>
</div>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        

<div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
          <li><a href="https://twitter.com/scaleoutsean" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/scaleoutsean" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>


<div class="page__footer-copyright">&copy; 2020-2026 scaleoutSean.github.io | <a href="https://scaleoutsean.github.io">Acting Technologist</a> | Terms: <a href="https://creativecommons.org/licenses/by/4.0/deed.en" target="_blank">CC BY 4.0</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>






  </body>
</html>
