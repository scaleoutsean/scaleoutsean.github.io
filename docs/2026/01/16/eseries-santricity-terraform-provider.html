<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.27.3 by Michael Rose
  Copyright 2013-2025 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>NetApp E-Series SANtricity Terraform Provider | Acting Technologist</title>
<meta name="description" content="SANtricity provider for modern pettle (pets &amp; cattle) volumes">


  <meta name="author" content="Sean">
  
  <meta property="article:author" content="Sean">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Acting Technologist">
<meta property="og:title" content="NetApp E-Series SANtricity Terraform Provider">
<meta property="og:url" content="https://scaleoutsean.github.io/2026/01/16/eseries-santricity-terraform-provider.html">


  <meta property="og:description" content="SANtricity provider for modern pettle (pets &amp; cattle) volumes">



  <meta property="og:image" content="https://scaleoutsean.github.io/assets/images/eseries-datalake-storage-layout-01.png">



  <meta name="twitter:site" content="@scaleoutSean">
  <meta name="twitter:title" content="NetApp E-Series SANtricity Terraform Provider">
  <meta name="twitter:description" content="SANtricity provider for modern pettle (pets &amp; cattle) volumes">
  <meta name="twitter:url" content="https://scaleoutsean.github.io/2026/01/16/eseries-santricity-terraform-provider.html">

  
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:image" content="https://scaleoutsean.github.io/assets/images/eseries-datalake-storage-layout-01.png">
  

  



  <meta property="article:published_time" content="2026-01-16T00:00:00+08:00">






<link rel="canonical" href="https://scaleoutsean.github.io/2026/01/16/eseries-santricity-terraform-provider.html">







  <meta name="google-site-verification" content="F6q7vIwQ2G0j8tk-KL9rAXOMLXMDMMUkEz4fRs1Nnnc" />


  <meta name="msvalidate.01" content="7cf5b7d96a77410a8ad035f764dc81b3">





<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Acting Technologist Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  window.enable_copy_code_button = true;
</script>

<script src="https://cdn.counter.dev/script.js" data-id="83dc700a-b821-4e57-9425-02b8336a9456" data-utcoffset="0"></script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <link href="https://fonts.googleapis.com/css?family=PT+Sans+Narrow:400,700|PT+Serif:400,700,400italic" rel="stylesheet" type="text/css">


  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
          <a class="site-logo" href="/"><img src="/assets/images/scaleoutsean-acting-technologist.png" alt="Acting Technologist"></a>
        
        <a class="site-title" href="/">
          Acting Technologist
          <span class="site-subtitle">human action through technology</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/archive/"
                
                
              >Archive</a>
            </li><li class="masthead__menu-item">
              <a
                href="/categories/"
                
                
              >Categories</a>
            </li><li class="masthead__menu-item">
              <a
                href="/tags/"
                
                
              >Tags</a>
            </li><li class="masthead__menu-item">
              <a
                href="/projects.html"
                
                
              >Projects</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main">
  


  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="NetApp E-Series SANtricity Terraform Provider">
    <meta itemprop="description" content="Provision storage in seconds with Terraform and E-Series">
    <meta itemprop="datePublished" content="2026-01-16T00:00:00+08:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://scaleoutsean.github.io/2026/01/16/eseries-santricity-terraform-provider.html" itemprop="url">NetApp E-Series SANtricity Terraform Provider
</a>
          </h1>
          

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2026-01-16T00:00:00+08:00">2026-01-16 00:00</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          15 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#introduction">Introduction</a></li><li><a href="#bbbut-e-series">Bbbut E-Series…</a></li><li><a href="#santricity-provider-for-terraform">SANtricity Provider for Terraform</a><ul><li><a href="#why-only-ddp">Why only DDP?</a></li><li><a href="#disk-failover-for-nosql-and-ceph-like-applications">Disk failover for NoSQL and Ceph-like applications</a></li></ul></li><li><a href="#hands-on-with-santricity-provider">Hands-on with SANtricity Provider</a></li><li><a href="#demo">Demo</a></li><li><a href="#conclusion">Conclusion</a></li></ul>
            </nav>
          </aside>
        
        <h2 id="introduction">Introduction</h2>

<p>As <a href="/2026/01/15/santricity-go-language-library-cli.html">I've been saying</a>, we need to build building blocks first and then we can have fun.</p>

<p>If you read that post and some of the other recent posts, the building blocks are now in place and the hard part is done.</p>

<h2 id="bbbut-e-series">Bbbut E-Series…</h2>

<p>"As we all know, E-Series is hard to automate and not suitable for modern applications." Right?</p>

<p>Wrong!</p>

<p>I'll borrow this image from another post to show why. This is an example - not a recipe - of a modern storage services stack from my perspective:</p>

<ul>
  <li>S3 as the source of truth - everything that needs protection at rest is there. Maybe there's an async replica in a low-cost public cloud or another site</li>
  <li>Block storage services
    <ul>
      <li>Databases - data that can't be lost, but moves to S3 buckets when it cools down
        <ul>
          <li>Logs - durable, fastest tier, protected, and replicated (by application or storage)</li>
          <li>Data - hot or warm, fast tier, stored on protected external storage</li>
        </ul>
      </li>
      <li>Temp/scratch/KV cache - fast(est) tier, used by analytics and inferencing - there's a copy in S3 or databases</li>
      <li>Other (front-end apps) - small capacity, <strong>big churn</strong> of many small volumes with non-persistent data. Put them anywhere - replicated ZFS on RAID 10 disks, VM-based HA NAS on RAID 6…</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/images/eseries-datalake-storage-layout-02.png" alt="Why E-Series" /></p>

<p>Modern applications compress data before it's uploaded to S3. They deal with HA on their own (barely any rely on traditional clusterware) and can backup to and restore from S3. In this situation, the main requirements for block services are:</p>

<ul>
  <li>RAID 1-like performance for logs</li>
  <li>RAID 6- or RAID 1-like performance for data that stays on protected storage</li>
  <li>RAID 1 or RAID 0 for ephemeral cache and scratch/temp filesystems (needed for analytics jobs which can be re-run if storage or host fail)</li>
  <li>"Other" applications have very simple requirements because data persistence isn't required. Containerize and scale them out if HA or rack redundancy is required. There's nothing to protect or backup - most are read-only (stateless). Create a Linux VM with a RAID 6 volume and use CSI LVM or TopoLVM, or form a small Ceph (three VMs, can be spread across racks) cluster for rack redundancy, etc.</li>
</ul>

<p>In this situation, having a single global pool of capacity from which we can carve volumes is a great feature. Does E-Series have that? Yes, DDP. I've blogged about it many times. In short:</p>

<ul>
  <li>Great performance for both random and sequential workloads</li>
  <li>Expansion in single disk increments without violating best practices</li>
  <li>Supports RAID 6 and RAID 1</li>
  <li>Simple choices - durability vs performance (or both, with application replicas or EC)</li>
</ul>

<p>DDP isn't the only way, but it's a great choice. If you have massive workloads you understand very well, you can benefit from traditional layout ("classic" RAID groups), or mix two in the same storage or across different arrays.</p>

<p>So I'd say E-Series is highly relevant for modern workloads.</p>

<p>One issue is that its automation and integrations are lacking. What can we do about that?</p>

<p>Well, we need to build them. That's not great, but it's happening. A Go library for Day 1+ management was posted to Github this week. Now we also have a SANtricity Provider!</p>

<h2 id="santricity-provider-for-terraform">SANtricity Provider for Terraform</h2>

<p>Imagine if we could use Terraform to take care of storage provisioning… Wait, I got this to work yesterday!</p>

<p>For data that needs persistence and performance, create a DDP and simply provision RAID 1 or RAID 6 depending on whether it's application-replicated or needs to rely on storage array.</p>

<p>For data that needs just performance, consider RAID 1.</p>

<p>A tiny minority of users may need something else, but I'd say 95% need just DDP.</p>

<p>Now consider that an entire storage system's capacity can be in a single DDP (pool) and all volumes are "owned" by both controllers. And we know almost everyone needs just RAID 1 or RAID 6 protection for their volumes. What does that mean?</p>

<p>It means the entire Day 1+ lifecycle consists of just two main operations:</p>

<ul>
  <li>Create volume</li>
  <li>Map volume to a host or cluster (host group)</li>
</ul>

<p>Now we see E-Series is very easy to automate. Yes, we may also want to remove mappings and delete volumes from time to time, but that's the same thing only in reverse.</p>

<p>That's the idea behind this provider:</p>

<ul>
  <li>Stick with DDP</li>
  <li>Pick RAID 1 or RAID 6</li>
  <li>Use E-Series Performance Analyzer or E-Series SANtricity Collector to monitor performance, capacity, events</li>
</ul>

<p>In case you wonder how exactly that works: DDP (11+ disks) always writes in 10 stripes; those can be 5+5 (R1 volumes) or 8+2 (R6 volumes). That's it! When you query the DDP, you see it keeps track of total extents allocated to RAID 1 and RAID 6 volumes. See extents key from this pool I used: <code class="language-plaintext highlighter-rouge">raidDiskPool</code> has <code class="language-plaintext highlighter-rouge">raid6</code> and <code class="language-plaintext highlighter-rouge">raid1</code> volumes in it:</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nl">"extents"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
  </span><span class="p">{</span><span class="w">
    </span><span class="nl">"sectorOffset"</span><span class="p">:</span><span class="w"> </span><span class="s2">"0"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"rawCapacity"</span><span class="p">:</span><span class="w"> </span><span class="s2">"4312147165184"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"raidLevel"</span><span class="p">:</span><span class="w"> </span><span class="s2">"raidDiskPool"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"volumeGroupRef"</span><span class="p">:</span><span class="w"> </span><span class="s2">"04000000600A098000E3C1B000002CED62CF874D"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"freeExtentRef"</span><span class="p">:</span><span class="w"> </span><span class="s2">"03010000600A098000E3C1B000002CED62CF874D"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"reserved1"</span><span class="p">:</span><span class="w"> </span><span class="s2">"000000000000000000000000"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"reserved2"</span><span class="p">:</span><span class="w"> </span><span class="s2">""</span><span class="p">,</span><span class="w">
    </span><span class="nl">"ddpRAIDCapacities"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
      </span><span class="p">{</span><span class="w">
        </span><span class="nl">"ddpVolRAIDLevel"</span><span class="p">:</span><span class="w"> </span><span class="s2">"raid6"</span><span class="p">,</span><span class="w">
        </span><span class="nl">"usableCapacity"</span><span class="p">:</span><span class="w"> </span><span class="s2">"4312147165184"</span><span class="p">,</span><span class="w">
        </span><span class="nl">"allocGranularity"</span><span class="p">:</span><span class="w"> </span><span class="s2">"4294967296"</span><span class="w">
      </span><span class="p">},</span><span class="w">
      </span><span class="p">{</span><span class="w">
        </span><span class="nl">"ddpVolRAIDLevel"</span><span class="p">:</span><span class="w"> </span><span class="s2">"raid1"</span><span class="p">,</span><span class="w">
        </span><span class="nl">"usableCapacity"</span><span class="p">:</span><span class="w"> </span><span class="s2">"2695091978240"</span><span class="p">,</span><span class="w">
        </span><span class="nl">"allocGranularity"</span><span class="p">:</span><span class="w"> </span><span class="s2">"2684354560"</span><span class="w">
      </span><span class="p">}</span><span class="w">
    </span><span class="p">]</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">]</span><span class="w">
</span></code></pre></div></div>

<p>There's a RAID 1 vs 6 breakdown calculator in my E-Config Web app (see <a href="/projects/">Projects</a>), but it's not hard to manually calculate expected usable capacity from a DDP based on estimates of R1 and R6 shares: R1 takes 100% more DDP space (1/1), R6 takes 25% (2/8). Remember that DDP itself has a reserve (1 or more disks) that can allow it to tolerate several consecutive failures of up to 2 disks.</p>

<p>This image depicts two target workloads for SANtricity Provider:</p>

<ul>
  <li>Modern databases - each node uses own disk(s) (hosts <strong>do not belong to a host group</strong>)</li>
  <li>Traditional HA clusters (shared disk(s) - mapped to a joint host group)</li>
</ul>

<p><img src="/assets/images/santricity-go-02-terraform-provider.png" alt="Storage provisioning with SANtricity Provider" /></p>

<p>There's no difference to the provider if the pool is DDP or not or whether volumes are mapped one-per-node or shared to a host group (shared storage cluster). You can configure and use it either way.</p>

<p>The difference to the <em>administrator</em> (and only because I didn't want to implement that, because I don't think it should be done) is modern databases (pattern on the left) need at least one disk (RAID 0 in the schematic) per application and the provider does not touch storage pools. That means you'd need to create and manage several classic RAID disk groups.</p>

<p>On the right, we start with a pre-created DDP (11 or more disks) and just use it and grow it when we need more capacity. The bigger it gets, the faster it rebuilds. You could have more than one DDP, of course. The point is the provider takes a pool ID and works with it. You just let the provider manage volumes, hosts and mappings.</p>

<h3 id="why-only-ddp">Why only DDP?</h3>

<p>I think consolidating on DDP is a great idea. The provider can already do almot everything we need (create and delete volumes). We also want to be able to grow (extend) volumes on storage array although we can also add new ones (the provider can do that).</p>

<p>One feature (or a limitation, depending on your view point) of this approach is SANtricity Provider does not attempt to micro-manage storage pools and physical disks. You don't either: create one pool and just use it.</p>

<p>The provider doesn't know if the pool ID you told it to use is a classic RAID disk group or DDP, so you could have multiple and use each as own "island". But I think it's hard to justify complicating things that way.</p>

<p>Of course, the provider can be improved in an attempt to make it become "smart". I don't like that idea because those who don't think the DDP approach is smart enough surely won't like a program's attempt to be smart about choosing between hundreds of permutations while having no idea about one's business and other requirements. At this time even an AI wouldn't necessarily outperform you with this DDP approach and three entry level EF arrays in three racks.</p>

<p>Why is that?</p>

<p>Assume you have 24 disks (full controller shelf). At any given time, maybe 1/3 of them is available for custom RAID groups. Which means you can build perhaps one RAID 1 or 0 and one small RAID 5. Once. And then you're out of disks and have stranded capacity in each pool-island. Compare that to having one DDP with consolidated capacity <em>and</em> the ability to create RAID 1 and RAID 6 volumes.</p>

<p>So, users with very large workloads (many racks) may be able to do better than DDP, but as far as this provider is concerned, it'd have to be improved as follows:</p>

<ul>
  <li>Enumerate and analyze physical disks (size, location)</li>
  <li>Understand differences between one, two, three or more racks (how?)</li>
  <li>Build new classic disk groups or DDPs in smart way (how?)</li>
  <li>Take pool ID as optional. Consider more variables to become "smart" (rack redundancy, etc.)</li>
  <li>Select the right pool (based on new inputs - space, performance, etc.)</li>
  <li>Choose to create a new pool or use existing to place your data (would you let it do that?)</li>
  <li>Have a way to deal with unnecessary pools (consolidate into fewer to free physical disks) which can use SANtricity volume copy and destruction of evacuated pools (would you let it?)</li>
  <li>Assign and unassign hot spares or decide whether or not to use global hot spare (would you let it?)</li>
</ul>

<p>In other words, that's probably a stupid idea. No MCP server can "solve" this today (and this "problem" doesn't really exist in the first place).</p>

<p>Then, secondly, if you take a step back you may notice the above points are simply a shortlist on how to re-invent Ceph. Why would we want to do that? Use the first pattern to create volumes for a bunch of servers and <a href="/2025/12/28/ceph-with-netapp-eseries.html#storage-layout">deploy Ceph on E-Series</a>.</p>

<p>It doesn't even take <a href="/2025/12/28/ceph-with-netapp-eseries.html#deploy-in-seconds">one minute</a> to deploy it with SANtricity Provider and MicroCeph. This is simply the first pattern on the left and - related to "other" workloads - if we needed this flexibility (RF2, RF3, EC) for "other" (stateless, "high churn") application workloads we would use a layout from the Ceph blog post and all volume and all <strong>CSI</strong> action would happen in Ceph plugins - you would not even need to use this provider to do that right (there's also Ansible and now three CLIs!).</p>

<p><a href="/2024/02/28/incus-zfs-netapp-eseries.html">ZFS</a> replicated over two DDPs or Ceph with EC is decent enough for "other" applications, while micro-managing RAID groups or creating a DIY Ceph (the list above) is not.</p>

<h3 id="disk-failover-for-nosql-and-ceph-like-applications">Disk failover for NoSQL and Ceph-like applications</h3>

<p>This was already mentioned (rely on RF or EC), but there's one other thing that I need to highlight: like most other managed storage, SANtricity lets you "remap" an existing volume to another host.</p>

<p>That gives you two options to handle server failures (not storage failures):</p>

<ul>
  <li>Remove the host and volume, create a new host and volume, and let RF or EC take care of a failed server/host, or</li>
  <li>Replace the host with another, and remap the failed host's volumes to the new host</li>
</ul>

<p>The second approach spares you from rebuilding TBs of data. Is it worth it? Maybe not, because if you do this once a year you could destroy everything. But if we wanted to do it:</p>

<ul>
  <li>Use the provider to create a new host with new IQN (remember to include CHAP if you use it) or NQN</li>
  <li>Remap the volume(s) from the dead host to this new host</li>
</ul>

<p>The question is: can we use SANtricity provider for that?</p>

<p>And the answer is: I don't know, but likely yes. In an emergency you could do it manually in the SANtricity Web UI.</p>

<p>The key here - from the application side - is whether the application can handle this. That's why I haven't really paid much attention to this so far. Overall, the safest way to do it is let the application do its thing.</p>

<p>Sometimes the application <em>won't let you</em> do this re-assignment. Other times it won't rebuild for minutes or hours, but then it will start, and you won't be able to stop it. So I don't see this "dead host replacement" workflow as essential, but it would be nice to be able to do it (assuming the application supports it) for <em>planned</em> host replacement (server hardware upgrade or refresh), so I will look into this later.</p>

<h2 id="hands-on-with-santricity-provider">Hands-on with SANtricity Provider</h2>

<p>I currently have no plans to publish this to Terraform community plugins - it has to be installed locally.</p>

<div class="language-hcl highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">cat</span> <span class="o">&lt;&lt;</span><span class="nx">EOF</span> <span class="o">&gt;</span> <span class="err">~</span><span class="o">/</span><span class="err">.</span><span class="nx">terraformrc</span>
<span class="nx">provider_installation</span> <span class="p">{</span>
  <span class="nx">dev_overrides</span> <span class="p">{</span>
    <span class="s2">"local/scaleoutsean/santricity"</span> <span class="p">=</span> <span class="s2">"/home/sean/santricity-go"</span>
  <span class="p">}</span>
  <span class="nx">direct</span> <span class="p">{}</span>
<span class="p">}</span>
<span class="nx">EOF</span>
</code></pre></div></div>

<p>Build as per README and configure credentials (or use a <a href="/2022/11/08/eseries-santricity-jwt-bearer-tokens.html">JWT Bearer Token</a>, which probably can be stored in Hashicorp Vault although I haven't checked).</p>

<div class="language-hcl highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">terraform</span> <span class="p">{</span>
  <span class="nx">required_providers</span> <span class="p">{</span>
    <span class="nx">santricity</span> <span class="o">=</span> <span class="p">{</span>
      <span class="nx">source</span>  <span class="o">=</span> <span class="s2">"local/scaleoutsean/santricity"</span>
      <span class="nx">version</span> <span class="o">=</span> <span class="s2">"1.0.0"</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>

<span class="nx">provider</span> <span class="s2">"santricity"</span> <span class="p">{</span>
  <span class="nx">endpoint</span> <span class="o">=</span> <span class="s2">"10.1.1.1"</span>   <span class="c1"># Replace with your Controller IP</span>
  <span class="nx">username</span> <span class="o">=</span> <span class="s2">"admin"</span>          <span class="c1"># Optional if token is used</span>
  <span class="nx">password</span> <span class="o">=</span> <span class="s2">"letsGoBrandon"</span>  <span class="c1"># Optional if token is used</span>
  <span class="c1"># token    = "eyJ..."       # Optional if username/password is used</span>
  <span class="nx">insecure</span> <span class="o">=</span> <span class="kc">true</span>
<span class="p">}</span>

</code></pre></div></div>

<p>Then we define volumes. Let's say I'm building a PostgreSQL HA cluster. One LUN (disk) for data (RAID 6), one disk for database log files (RAID 1). We simply use one DDP (identified by its pool ID) for everything. You can get these IDs from the UI, <code class="language-plaintext highlighter-rouge">santricity-client</code>, <code class="language-plaintext highlighter-rouge">santricity-powershell</code>, Swagger (<code class="language-plaintext highlighter-rouge">GET storage-pools</code>), Ansible (<code class="language-plaintext highlighter-rouge">gather_facts</code>), etc.</p>

<div class="language-hcl highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">resource</span> <span class="s2">"santricity_volume"</span> <span class="s2">"pg_data"</span> <span class="p">{</span>
  <span class="nx">name</span>       <span class="o">=</span> <span class="s2">"pg_data_vol"</span>
  <span class="nx">pool_id</span>    <span class="o">=</span> <span class="nx">var</span><span class="p">.</span><span class="nx">pool_id</span>
  <span class="nx">size_gb</span>    <span class="o">=</span> <span class="mi">50</span>
  <span class="nx">raid_level</span> <span class="o">=</span> <span class="s2">"raid6"</span>
<span class="p">}</span>

<span class="nx">resource</span> <span class="s2">"santricity_volume"</span> <span class="s2">"pg_log"</span> <span class="p">{</span>
  <span class="nx">name</span>       <span class="o">=</span> <span class="s2">"pg_log_vol"</span>
  <span class="nx">pool_id</span>    <span class="o">=</span> <span class="nx">var</span><span class="p">.</span><span class="nx">pool_id</span>
  <span class="nx">size_gb</span>    <span class="o">=</span> <span class="mi">10</span>
  <span class="nx">raid_level</span> <span class="o">=</span> <span class="s2">"raid1"</span>
<span class="p">}</span>


<span class="nx">variable</span> <span class="s2">"pool_id"</span> <span class="p">{</span>
  <span class="nx">type</span>        <span class="o">=</span> <span class="nx">string</span>
  <span class="nx">description</span> <span class="o">=</span> <span class="s2">"The DDP Pool ID (Ref) to provision volumes in."</span>
  <span class="nx">default</span>     <span class="o">=</span> <span class="s2">"04000000600A098000E3C1B000002CED62CF874D"</span>
<span class="p">}</span>
</code></pre></div></div>
<p>We also need to create a host group for our two PostgreSQL servers and then we can map the new disks to a "host group" of these two iSCSI clients.</p>

<div class="language-hcl highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="nx">resource</span> <span class="s2">"santricity_host"</span> <span class="s2">"pg_host_01"</span> <span class="p">{</span>
  <span class="nx">name</span> <span class="o">=</span> <span class="s2">"pg-01"</span>
  <span class="nx">type</span> <span class="o">=</span> <span class="s2">"linux_dm_mp"</span>
  <span class="nx">host_group_id</span> <span class="o">=</span> <span class="nx">santricity_host_group</span><span class="p">.</span><span class="nx">pg_cluster</span><span class="p">.</span><span class="nx">id</span>
  <span class="nx">ports</span> <span class="p">{</span>
    <span class="nx">type</span>  <span class="o">=</span> <span class="s2">"iscsi"</span>
    <span class="nx">port</span>  <span class="o">=</span> <span class="s2">"iqn.1993-08.org.debian:01:postgres01"</span>
    <span class="nx">label</span> <span class="o">=</span> <span class="s2">"pg01-iscsi"</span>
  <span class="p">}</span>
<span class="p">}</span>

<span class="nx">resource</span> <span class="s2">"santricity_host"</span> <span class="s2">"pg_host_02"</span> <span class="p">{</span>
  <span class="nx">name</span> <span class="o">=</span> <span class="s2">"pg-02"</span>
  <span class="nx">type</span> <span class="o">=</span> <span class="s2">"linux_dm_mp"</span>
  <span class="nx">host_group_id</span> <span class="o">=</span> <span class="nx">santricity_host_group</span><span class="p">.</span><span class="nx">pg_cluster</span><span class="p">.</span><span class="nx">id</span>
  <span class="nx">ports</span> <span class="p">{</span>
    <span class="nx">type</span>  <span class="o">=</span> <span class="s2">"iscsi"</span>
    <span class="nx">port</span>  <span class="o">=</span> <span class="s2">"iqn.1993-08.org.debian:01:postgres02"</span>
    <span class="nx">label</span> <span class="o">=</span> <span class="s2">"pg02-iscsi"</span>
  <span class="p">}</span>
<span class="p">}</span>

<span class="nx">resource</span> <span class="s2">"santricity_mapping"</span> <span class="s2">"pg_data_map"</span> <span class="p">{</span>
  <span class="nx">volume_id</span> <span class="o">=</span> <span class="nx">santricity_volume</span><span class="p">.</span><span class="nx">pg_data</span><span class="p">.</span><span class="nx">id</span>
  <span class="c1"># Because pg-01 is in a group, this will automatically map to the Group (Cluster).</span>
  <span class="nx">host_id</span>   <span class="o">=</span> <span class="nx">santricity_host</span><span class="p">.</span><span class="nx">pg_host_01</span><span class="p">.</span><span class="nx">id</span>
  <span class="nx">lun</span>       <span class="o">=</span> <span class="mi">10</span> <span class="c1"># Arbitrary LUN</span>
<span class="p">}</span>

<span class="nx">resource</span> <span class="s2">"santricity_mapping"</span> <span class="s2">"pg_log_map"</span> <span class="p">{</span>
  <span class="nx">volume_id</span> <span class="o">=</span> <span class="nx">santricity_volume</span><span class="p">.</span><span class="nx">pg_log</span><span class="p">.</span><span class="nx">id</span>
  <span class="c1"># We can also map explicitly to the Group ID</span>
  <span class="nx">host_group_id</span> <span class="o">=</span> <span class="nx">santricity_host_group</span><span class="p">.</span><span class="nx">pg_cluster</span><span class="p">.</span><span class="nx">id</span>
  <span class="nx">lun</span>       <span class="o">=</span> <span class="mi">11</span>
<span class="p">}</span>

</code></pre></div></div>

<p>LUN IDs can be left out and SANtricity will assign them automatically. Since you can get WWNs out of Terraform, you don't really need to rely on LUN numbers, but you can. It may be useful for folks who build VMs or physical hosts with many disks and want to micromanage that aspect.</p>

<p>Run <code class="language-plaintext highlighter-rouge">terraform apply</code>.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>santricity_host_group.pg_cluster: Creating...
santricity_volume.pg_log: Creating...
santricity_volume.pg_data: Creating...
santricity_host_group.pg_cluster: Creation <span class="nb">complete </span>after 0s <span class="o">[</span><span class="nb">id</span><span class="o">=</span>85000000600A098000E3C1B00036367C69688A93]
santricity_host.pg_host_02: Creating...
santricity_host.pg_host_01: Creating...
santricity_host.pg_host_02: Creation <span class="nb">complete </span>after 1s <span class="o">[</span><span class="nb">id</span><span class="o">=</span>84000000600A098000E3C1B00030367E69688A94]
santricity_host.pg_host_01: Creation <span class="nb">complete </span>after 1s <span class="o">[</span><span class="nb">id</span><span class="o">=</span>84000000600A098000E3C1B00030368269688A94]
santricity_volume.pg_log: Creation <span class="nb">complete </span>after 2s <span class="o">[</span><span class="nb">id</span><span class="o">=</span>02000000600A098000F63714000038B469689007]
santricity_mapping.pg_log_map: Creating...
santricity_mapping.pg_log_map: Creation <span class="nb">complete </span>after 1s <span class="o">[</span><span class="nb">id</span><span class="o">=</span>8800000090000000000000000000000000000000]
santricity_volume.pg_data: Creation <span class="nb">complete </span>after 3s <span class="o">[</span><span class="nb">id</span><span class="o">=</span>02000000600A098000E3C1B00000368B69688A95]
santricity_mapping.pg_data_map: Creating...
santricity_mapping.pg_data_map: Creation <span class="nb">complete </span>after 0s <span class="o">[</span><span class="nb">id</span><span class="o">=</span>8800000093000000000000000000000000000000]
</code></pre></div></div>

<p>Done in seconds! If you build a more complete workflow (with Terraform or other) now you could rescan iSCSI, login to these targets, configure DM-MP, deploy PostgreSQL and configure Pacemaker and Corosync.</p>

<p>If you use PostgreSQL's built-in replication, deploy independent hosts (similar to pattern on the left, but with DDP).</p>

<p>If other people build the compute side, they can use <code class="language-plaintext highlighter-rouge">santricity-client</code> (Python) or <code class="language-plaintext highlighter-rouge">santricity-powershell</code> to inspect disks host-side.</p>

<p><code class="language-plaintext highlighter-rouge">terraform destroy</code> is also fast, as usual for Terraform.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>santricity_mapping.pg_log_map: Destroying... <span class="o">[</span><span class="nb">id</span><span class="o">=</span>8800000090000000000000000000000000000000]
santricity_mapping.pg_data_map: Destroying... <span class="o">[</span><span class="nb">id</span><span class="o">=</span>8800000093000000000000000000000000000000]
santricity_host.pg_host_02: Destroying... <span class="o">[</span><span class="nb">id</span><span class="o">=</span>84000000600A098000E3C1B00030367E69688A94]
santricity_mapping.pg_data_map: Destruction <span class="nb">complete </span>after 0s
santricity_mapping.pg_log_map: Destruction <span class="nb">complete </span>after 0s
santricity_host.pg_host_01: Destroying... <span class="o">[</span><span class="nb">id</span><span class="o">=</span>84000000600A098000E3C1B00030368269688A94]
santricity_volume.pg_data: Destroying... <span class="o">[</span><span class="nb">id</span><span class="o">=</span>02000000600A098000E3C1B00000368B69688A95]
santricity_volume.pg_log: Destroying... <span class="o">[</span><span class="nb">id</span><span class="o">=</span>02000000600A098000F63714000038B469689007]
santricity_volume.pg_data: Destruction <span class="nb">complete </span>after 1s
santricity_host.pg_host_02: Destruction <span class="nb">complete </span>after 1s
santricity_host.pg_host_01: Destruction <span class="nb">complete </span>after 1s
santricity_host_group.pg_cluster: Destroying... <span class="o">[</span><span class="nb">id</span><span class="o">=</span>85000000600A098000E3C1B00036367C69688A93]
santricity_volume.pg_log: Destruction <span class="nb">complete </span>after 1s
santricity_host_group.pg_cluster: Destruction <span class="nb">complete </span>after 0s
</code></pre></div></div>

<p>You may add new disks by editing and re-applying your Terraform plan, and soon you'll be able to expand them (which usually requires a rescan (both) and a filesystem expansion on the (attached) client as usual).</p>

<p>If you want to replicate this to another site or rack, just create another Terraform plan and run it against the remote SANtricity (you may cost-down to just one host and one 60 GiB RAID 6 volume rather than 50 (R6) + 10 (R1), and then setup replication on PostgreSQL). You could also create a snapshot schedule for the remote site using the SANtricity API or Web UI.</p>

<p>Apply:</p>

<p><img src="/assets/images/santricity-go-03-terraform-provider-apply.png" alt="terraform apply with SANtricity Provider" /></p>

<p>Destroy:</p>

<p><img src="/assets/images/santricity-go-04-terraform-provider-destroy.png" alt="terraform apply with SANtricity Provider" /></p>

<p>Remember to backup your Terraform state, etc.</p>

<h2 id="demo">Demo</h2>

<ul>
  <li><a href="https://rumble.com/v76asii-terraform-provider-santricity-for-netapp-e-series-storage-arrays.html">Terraform Provider (for) SANtricity</a> - 3m50s</li>
</ul>

<h2 id="conclusion">Conclusion</h2>

<p>I'm very satisfied how this turned out. A week ago none of this existed. I <a href="/2026/01/15/santricity-go-language-library-cli.html">salvaged abandoned code and published a stand-alone Go library and client</a>, and now we have this SANtricity provider for an <strong>optimal</strong> storage deployment pattern.</p>

<p>Several wish-list items (P1 and P2 will be done, P3 if easy):</p>

<ul>
  <li>P1: Volume expand will be added today (edit: <strong>done</strong>)</li>
  <li>P2: NVMe/RoCEv2 will likely be added soon - it only impacts host creation (and the only reason it's not available now is I don't have any hardware to try it, but I'll build it based on JSON samples (edit: <strong>done</strong>, although I can't test due to no hardware access))</li>
  <li>P3: Planned host replacement with volume move (edit: <strong>done</strong>, also volume-to-host re-mapping in SANtricity Provider)</li>
</ul>

<p>Gaps (real and perceived):</p>

<ul>
  <li>"Other" workloads - mostly lightweight and high-churn, can be deployed in VMs based on the HA pattern (ZFS)</li>
  <li>"Custom" classic RAID groups - out of scope, use Ceph (available in Proxmox and elsewhere) or similar</li>
  <li>CSI (Kuberntes) - the HA pattern works for SDS VMs, the NOSQL pattern also works on DDP and manually; there's no "native" CSI plugin yet as Trident dumped E-SEries years ago</li>
  <li>Snapshots, clones - can be implemented, although the recommended pattern is to do that on application layer (almost all databases have it. I don't think we're not supposed to solve these application management problems with storage)</li>
</ul>

<p>Application-level integrations? I don't think we need those. Currently the repository has a PostgreSQL (storage side) example shared above (the HA pattern). A sample for the NOSQL/NUSQL pattern may be added, but it's really the same thing - just <strong>don't</strong> create a "host group" that lets all hosts see all volumes.</p>

<p>The provider and examples can be found in my <code class="language-plaintext highlighter-rouge">santricity-go</code> repository.</p>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/api" class="page__taxonomy-item p-category" rel="tag">api</a><span class="sep">, </span>
    
      <a href="/tags/automation" class="page__taxonomy-item p-category" rel="tag">automation</a><span class="sep">, </span>
    
      <a href="/tags/e-series" class="page__taxonomy-item p-category" rel="tag">e-series</a><span class="sep">, </span>
    
      <a href="/tags/go" class="page__taxonomy-item p-category" rel="tag">go</a><span class="sep">, </span>
    
      <a href="/tags/golang" class="page__taxonomy-item p-category" rel="tag">golang</a><span class="sep">, </span>
    
      <a href="/tags/netapp" class="page__taxonomy-item p-category" rel="tag">netapp</a><span class="sep">, </span>
    
      <a href="/tags/opentofu" class="page__taxonomy-item p-category" rel="tag">opentofu</a><span class="sep">, </span>
    
      <a href="/tags/pulumi" class="page__taxonomy-item p-category" rel="tag">pulumi</a><span class="sep">, </span>
    
      <a href="/tags/santricity" class="page__taxonomy-item p-category" rel="tag">santricity</a><span class="sep">, </span>
    
      <a href="/tags/terraform" class="page__taxonomy-item p-category" rel="tag">terraform</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/netapp" class="page__taxonomy-item p-category" rel="tag">netapp</a><span class="sep">, </span>
    
      <a href="/categories/storage" class="page__taxonomy-item p-category" rel="tag">storage</a>
    
    </span>
  </p>


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time class="dt-published" datetime="2026-01-16T00:00:00+08:00">2026-01-16 00:00</time></p>

      </footer>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://x.com/intent/tweet?via=scaleoutSean&text=NetApp+E-Series+SANtricity+Terraform+Provider%20https%3A%2F%2Fscaleoutsean.github.io%2F2026%2F01%2F16%2Feseries-santricity-terraform-provider.html" class="btn btn--x" aria-label="Share on X" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on X">
    <i class="fab fa-fw fa-x-twitter" aria-hidden="true"></i><span> X</span>
  </a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://scaleoutsean.github.io/2026/01/16/eseries-santricity-terraform-provider.html" class="btn btn--linkedin" aria-label="Share on LinkedIn" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn">
    <i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span>
  </a>

</section>


      
  <nav class="pagination">
    
      <a href="/2026/01/15/santricity-go-language-library-cli.html" class="pagination--pager" title="E-Series SANtricity Go client for Day 1+ operations">Previous</a>
    
    
      <a href="/2026/01/16/santricity-eseries-datalake-storage.html" class="pagination--pager" title="Big Data &amp; analytics patterns for NetApp object &amp; block">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
    
<div class="page__related">
  
  <h2 class="page__related-title">You may also enjoy</h2>
  <div class="grid__wrapper">
    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/2026/03/01/adblock-andnetapp-docs.html" rel="permalink">Block external docs to improve NetApp docs load time
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2026-03-01T00:00:00+08:00">2026-03-01 00:00</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          less than 1 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Block external domains on https://docs.netapp.com
</p>
  </article>
</div>

    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/2026/02/26/ibm-block-storage-cis-driver-santricity-fork.html" rel="permalink">IBM Block Storage CSI driver patched for NetApp E-Series
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2026-02-26T00:00:00+08:00">2026-02-26 00:00</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          11 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Unofficial patch to IBM Block CSI driver for E-Series SANtricity storage
</p>
  </article>
</div>

    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/2026/02/19/the-shocking-truth-about-ef600-200g-ports.html" rel="permalink">Connecting to 200G HICs on E-Series EF600
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2026-02-19T00:00:00+08:00">2026-02-19 00:00</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          2 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Connecting to EF-Series EF600 200G HIC in DAS mode
</p>
  </article>
</div>

    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/2026/02/19/linux-nvme-roce-ef-series.html" rel="permalink">Linux, NVMe/RoCE and HA NetApp EF-Series NVMe-oF storage
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2026-02-19T00:00:00+08:00">2026-02-19 00:00</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          19 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Linux NVMe/RoCE clients with NetApp EF-Series NVMe-oF
</p>
  </article>
</div>

    
  </div>
</div>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        

<div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
          <li><a href="https://twitter.com/scaleoutsean" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/scaleoutsean" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>


<div class="page__footer-copyright">&copy; 2020-2026 scaleoutSean.github.io | <a href="https://scaleoutsean.github.io">Acting Technologist</a> | Terms: <a href="https://creativecommons.org/licenses/by/4.0/deed.en" target="_blank">CC BY 4.0</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>






  </body>
</html>
