<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.27.3 by Michael Rose
  Copyright 2013-2025 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Proxmox PVE 9.1.1 (Debian) with NetApp EF-Series NVMe/RoCE | Acting Technologist</title>
<meta name="description" content="Let's see how sucky it is!">


  <meta name="author" content="Sean">
  
  <meta property="article:author" content="Sean">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Acting Technologist">
<meta property="og:title" content="Proxmox PVE 9.1.1 (Debian) with NetApp EF-Series NVMe/RoCE">
<meta property="og:url" content="https://scaleoutsean.github.io/2026/03/01/proxmox-pve-with-netapp-eseries.html">


  <meta property="og:description" content="Let's see how sucky it is!">



  <meta property="og:image" content="https://scaleoutsean.github.io/assets/images/proxmox-pve-santricity-sequential-read.png">



  <meta name="twitter:site" content="@scaleoutSean">
  <meta name="twitter:title" content="Proxmox PVE 9.1.1 (Debian) with NetApp EF-Series NVMe/RoCE">
  <meta name="twitter:description" content="Let's see how sucky it is!">
  <meta name="twitter:url" content="https://scaleoutsean.github.io/2026/03/01/proxmox-pve-with-netapp-eseries.html">

  
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:image" content="https://scaleoutsean.github.io/assets/images/proxmox-pve-santricity-sequential-read.png">
  

  



  <meta property="article:published_time" content="2026-03-01T00:00:00+08:00">






<link rel="canonical" href="https://scaleoutsean.github.io/2026/03/01/proxmox-pve-with-netapp-eseries.html">







  <meta name="google-site-verification" content="F6q7vIwQ2G0j8tk-KL9rAXOMLXMDMMUkEz4fRs1Nnnc" />


  <meta name="msvalidate.01" content="7cf5b7d96a77410a8ad035f764dc81b3">





<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Acting Technologist Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  window.enable_copy_code_button = true;
</script>

<script src="https://cdn.counter.dev/script.js" data-id="83dc700a-b821-4e57-9425-02b8336a9456" data-utcoffset="0"></script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <link href="https://fonts.googleapis.com/css?family=PT+Sans+Narrow:400,700|PT+Serif:400,700,400italic" rel="stylesheet" type="text/css">


  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
          <a class="site-logo" href="/"><img src="/assets/images/scaleoutsean-acting-technologist.png" alt="Acting Technologist"></a>
        
        <a class="site-title" href="/">
          Acting Technologist
          <span class="site-subtitle">human action through technology</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/archive/"
                
                
              >Archive</a>
            </li><li class="masthead__menu-item">
              <a
                href="/categories/"
                
                
              >Categories</a>
            </li><li class="masthead__menu-item">
              <a
                href="/tags/"
                
                
              >Tags</a>
            </li><li class="masthead__menu-item">
              <a
                href="/projects.html"
                
                
              >Projects</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main">
  


  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Proxmox PVE 9.1.1 (Debian) with NetApp EF-Series NVMe/RoCE">
    <meta itemprop="description" content="PVE 9.1.1 with NVMe/RoCE on NetApp EF600">
    <meta itemprop="datePublished" content="2026-03-01T00:00:00+08:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://scaleoutsean.github.io/2026/03/01/proxmox-pve-with-netapp-eseries.html" itemprop="url">Proxmox PVE 9.1.1 (Debian) with NetApp EF-Series NVMe/RoCE
</a>
          </h1>
          

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2026-03-01T00:00:00+08:00">2026-03-01 00:00</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          10 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#introduction">Introduction</a></li><li><a href="#verifying-proxmox-pve-performance">Verifying Proxmox PVE performance</a></li><li><a href="#video-walk-through">Video walk-through</a></li><li><a href="#other-examples">Other examples</a><ul><li><a href="#simple-random-workloads">Simple random workloads</a></li><li><a href="#simple-sequential-workload">Simple sequential workload</a></li><li><a href="#summary">Summary</a></li></ul></li><li><a href="#tuning-pve-with-e-series">Tuning PVE with E-Series</a></li><li><a href="#conclusion">Conclusion</a></li></ul>
            </nav>
          </aside>
        
        <h2 id="introduction">Introduction</h2>

<p>Today I wanted to work on something useful - such as host-side interface reporting for various SANtricity-related libraries I maintain - but instead I spent hours dealing with <a href="/2026/03/01/adblock-and-netapp-docs.html">custom URL blocking filters</a> and "clarifying" my take on this <a href="https://kb.netapp.com/on-prem/E-Series/Hardware-KBs/Very_Low_iSCSI_Read_Performance_on_NetApp_EF300_with_Proxmox_Debian_Hosts">this NetApp KB</a> (see <a href="/2026/02/19/linux-nvme-roce-ef-series.html#proxmox-91-debian-trixie">this Linux NVMe/RoCE post for additional details</a>), which is what follows below.</p>

<h2 id="verifying-proxmox-pve-performance">Verifying Proxmox PVE performance</h2>

<p>Proxmox has <a href="https://pve.proxmox.com/wiki/Benchmarking_Storage">this page</a> with basic <code class="language-plaintext highlighter-rouge">fio</code>-based performance tests. As the page clearly states, those are very basic tests to get you started.</p>

<p>I generally dislike running these aimless tests unless I know what the objective is, but since the KB article is generic and I need a reference point, I followed the commands on the Proxmox Wiki page and did some extra runs to give you an idea of what might be relevant here.</p>

<h2 id="video-walk-through">Video walk-through</h2>

<p>This video isn't a super-complete pro version of PVE benchmarking, but it covers the types of workloads commonly found in VM or Kubernetes environments.</p>

<p>While I repeated the same tests from the Proxmox Wiki, I also did a few more realistic runs. More realistic how?</p>

<ul>
  <li>Filesystems (although this test is about storage, not filesystem, performance, you will have a filesystem on top)</li>
  <li>Write workload (50% write, which is excessive, but you might have that if you use 25% write on Btrfs or ZFS and use PVE to repelicate within same E-Series system)</li>
  <li>More realistic parameters in request sizes: 8 KiB for a "database-like" random workload and 4 MiB for "streaming-like" sequential workload</li>
  <li>More than one thread, more than minimal Queue Depth</li>
  <li>All E-Series tests done here use protected storage (that is to say, storage controllers, IO paths, and storage pools, and power supplies are all fully redundant). I highlight this from an availability perspective - and without trying to "compare performance" - because these tests <strong>include all overheads you may think of</strong> and the ones done by Proxmox maybe do not in which case you should consider how figures obtained from a 100% read test on a RAID 0 disk may translate to a 30% write workload after Erasure Coding or RF2 replication (including possibly 100% write amplification).</li>
</ul>

<p>I also deliberately used bad settings for "minimally viable" EF600 performance (in this configuration):</p>

<ul>
  <li>Apart from setting the storage NICs to MTU 9000 bytes, both the host and storage had <strong>zero tuning</strong> of any kind. All I wanted is to simulate a basic setup by someone who just followed steps in that NVMe post linked at the top</li>
  <li>End-to-end <strong>disabled</strong> caching on the RAID 1 volume with XFS (totally unrealistic because it should realy be enabled as E-Series by defaults mirrors its write cache and it's battery-backed and fully redundant). For the RAID 6 disk I didn't check cache settings because the overhead of Btrfs was obvious.</li>
</ul>

<p>Highlights:</p>

<ul>
  <li>Slower performance for 4 KiB random read than in the simple test by Proxmox. Why? They likely used internal NVMe, which has no network latency and DAS is especially beneficial for the lowest Queue Depth and the smallest reqeust size possible, i.e. exactly this test</li>
  <li>Faster performance than the Wiki test with higher Queue Depth and the more realistic 8 KiB request size</li>
  <li>Much faster performance with 1 MiB and 4 MiB "streaming" requests, due to more disks on E-Series and better sequential performance of E-Series</li>
</ul>

<p>Operational highlights:</p>

<ul>
  <li>Both RAID 1 (it's RAID 10, but RAID 1 is the config shorthand) and RAID 6 volumes can be consolidated on DDP storage pools</li>
  <li>For optimal price/performance, users would normally use RAID 1 for log-style workloads (e.g. PostgreSQL WAL log) and maybe write-heavy indexes and RAID 6 for the rest</li>
  <li>There's relatively little one has to understand to make most out of E-Series performance. Tools like E-Series SANtricity Collector help you deep dive into performance, but in reality for most users on flash arrays, that is not necessary. If you use hybrid or NL-SAS, then you'll need enough disks to get decent random IO performance on NL-SAS - the same as on any storage system.</li>
</ul>

<h2 id="other-examples">Other examples</h2>

<p>These examples are <strong>not</strong> in the video, which would make the video too long, but illustrate some of the points made above.</p>

<h3 id="simple-random-workloads">Simple random workloads</h3>

<p>This <code class="language-plaintext highlighter-rouge">fio</code> run with random IO uses 8 KiB requests (usually the minimum for databases) on DDP-based RAID 6. The filesystem is Btrfs, we have a 30% write workload a minimal multi-threading and basic Queue Depth, to make it more realistic.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>fio <span class="nt">--ioengine</span><span class="o">=</span>libaio <span class="nt">--direct</span><span class="o">=</span>1 <span class="nt">--sync</span><span class="o">=</span>1 <span class="nt">--rw</span><span class="o">=</span>readwrite <span class="nt">--rwmixwrite</span><span class="o">=</span>30 <span class="se">\</span>
   <span class="nt">--bs</span><span class="o">=</span>8K <span class="nt">--numjobs</span><span class="o">=</span>2 <span class="nt">--iodepth</span><span class="o">=</span>4 <span class="nt">--runtime</span><span class="o">=</span>60 <span class="nt">--size</span> 1G <span class="nt">--time_based</span> <span class="se">\</span>
   <span class="nt">--name</span> seq_read <span class="nt">--filename</span> /mnt/btrfs1/fio-test
seq_read: <span class="o">(</span><span class="nv">g</span><span class="o">=</span>0<span class="o">)</span>: <span class="nv">rw</span><span class="o">=</span>rw, <span class="nv">bs</span><span class="o">=(</span>R<span class="o">)</span> 8192B-8192B, <span class="o">(</span>W<span class="o">)</span> 8192B-8192B, <span class="o">(</span>T<span class="o">)</span> 8192B-8192B, <span class="nv">ioengine</span><span class="o">=</span>libaio, <span class="nv">iodepth</span><span class="o">=</span>4
...
fio-3.39
Starting 2 processes
Jobs: 2 <span class="o">(</span><span class="nv">f</span><span class="o">=</span>2<span class="o">)</span>: <span class="o">[</span>M<span class="o">(</span>2<span class="o">)][</span>100.0%][r<span class="o">=</span>11.9MiB/s,w<span class="o">=</span>5333KiB/s][r<span class="o">=</span>1520,w<span class="o">=</span>666 IOPS][eta 00m:00s]
seq_read: <span class="o">(</span><span class="nv">groupid</span><span class="o">=</span>0, <span class="nb">jobs</span><span class="o">=</span>1<span class="o">)</span>: <span class="nv">err</span><span class="o">=</span> 0: <span class="nv">pid</span><span class="o">=</span>1092801: Sun Mar  1 04:12:28 2026
  <span class="nb">read</span>: <span class="nv">IOPS</span><span class="o">=</span>790, <span class="nv">BW</span><span class="o">=</span>6323KiB/s <span class="o">(</span>6475kB/s<span class="o">)(</span>371MiB/60003msec<span class="o">)</span>
    slat <span class="o">(</span>usec<span class="o">)</span>: <span class="nv">min</span><span class="o">=</span>7, <span class="nv">max</span><span class="o">=</span>555, <span class="nv">avg</span><span class="o">=</span>26.57, <span class="nv">stdev</span><span class="o">=</span> 7.34
    clat <span class="o">(</span>usec<span class="o">)</span>: <span class="nv">min</span><span class="o">=</span>46, <span class="nv">max</span><span class="o">=</span>26642, <span class="nv">avg</span><span class="o">=</span>2646.35, <span class="nv">stdev</span><span class="o">=</span>2289.82
     lat <span class="o">(</span>usec<span class="o">)</span>: <span class="nv">min</span><span class="o">=</span>61, <span class="nv">max</span><span class="o">=</span>26674, <span class="nv">avg</span><span class="o">=</span>2672.92, <span class="nv">stdev</span><span class="o">=</span>2289.97
    clat percentiles <span class="o">(</span>usec<span class="o">)</span>:
     |  1.00th<span class="o">=[</span>   72],  5.00th<span class="o">=[</span>   86], 10.00th<span class="o">=[</span>   88], 20.00th<span class="o">=[</span>  124],
     | 30.00th<span class="o">=[</span>  151], 40.00th<span class="o">=[</span> 2606], 50.00th<span class="o">=[</span> 2802], 60.00th<span class="o">=[</span> 2933],
     | 70.00th<span class="o">=[</span> 3097], 80.00th<span class="o">=[</span> 5342], 90.00th<span class="o">=[</span> 5932], 95.00th<span class="o">=[</span> 6259],
     | 99.00th<span class="o">=[</span> 8979], 99.50th<span class="o">=[</span> 9241], 99.90th<span class="o">=[</span> 9765], 99.95th<span class="o">=[</span>10028],
     | 99.99th<span class="o">=[</span>12518]
   bw <span class="o">(</span>  KiB/s<span class="o">)</span>: <span class="nv">min</span><span class="o">=</span> 4544, <span class="nv">max</span><span class="o">=</span> 8512, <span class="nv">per</span><span class="o">=</span>50.28%, <span class="nv">avg</span><span class="o">=</span>6337.88, <span class="nv">stdev</span><span class="o">=</span>631.62, <span class="nv">samples</span><span class="o">=</span>119
   iops        : <span class="nv">min</span><span class="o">=</span>  568, <span class="nv">max</span><span class="o">=</span> 1064, <span class="nv">avg</span><span class="o">=</span>792.24, <span class="nv">stdev</span><span class="o">=</span>78.95, <span class="nv">samples</span><span class="o">=</span>119
  write: <span class="nv">IOPS</span><span class="o">=</span>338
...
Run status group 0 <span class="o">(</span>all <span class="nb">jobs</span><span class="o">)</span>:
   READ: <span class="nv">bw</span><span class="o">=</span>12.3MiB/s <span class="o">(</span>12.9MB/s<span class="o">)</span>, 6281KiB/s-6323KiB/s <span class="o">(</span>6432kB/s-6475kB/s<span class="o">)</span>, <span class="nv">io</span><span class="o">=</span>739MiB <span class="o">(</span>774MB<span class="o">)</span>, <span class="nv">run</span><span class="o">=</span>60001-60003msec
  WRITE: <span class="nv">bw</span><span class="o">=</span>5415KiB/s <span class="o">(</span>5545kB/s<span class="o">)</span>, 2708KiB/s-2708KiB/s <span class="o">(</span>2773kB/s-2773kB/s<span class="o">)</span>, <span class="nv">io</span><span class="o">=</span>317MiB <span class="o">(</span>333MB<span class="o">)</span>, <span class="nv">run</span><span class="o">=</span>60001-60003msec
</code></pre></div></div>

<p>1.1k (8 KiB) IOPS.</p>

<p>The performance is poor for reasons explained earlier. If I needed more performance, I'd use a filesystem suitable that kind of a workload (not Btrfs, for example) and, if I needed even more, I'd probably use RAID 1 volumes on the same DDP for the DB log file. And I'd tune my storage and system (which I didn't do - it was as explained above and in the video).</p>

<p>All of these tests also test filesystem performance. The Proxmox examples test block devices (which is technially correct for "storage" tests). I run these test with (on) filesystems because I don't need to demonstrate "best possible", but <strong>almost the worst</strong> performance figures in part because of the generic nature of the KB.</p>

<p>Let's see what happens when we make some improvements to both the filesystem and RAID level (XFS on RAID 1) while still using the same DDP storage pool.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>fio <span class="nt">--ioengine</span><span class="o">=</span>libaio <span class="nt">--direct</span><span class="o">=</span>1 <span class="nt">--sync</span><span class="o">=</span>1 <span class="nt">--rw</span><span class="o">=</span>readwrite <span class="nt">--rwmixwrite</span><span class="o">=</span>30 <span class="se">\</span>
  <span class="nt">--bs</span><span class="o">=</span>8K <span class="nt">--numjobs</span><span class="o">=</span>2 <span class="nt">--iodepth</span><span class="o">=</span>4 <span class="nt">--runtime</span><span class="o">=</span>60 <span class="nt">--size</span> 1G <span class="nt">--time_based</span> <span class="se">\</span>
  <span class="nt">--name</span> seq_read <span class="nt">--filename</span> /mnt/ddpr1/fio-test
seq_read: <span class="o">(</span><span class="nv">g</span><span class="o">=</span>0<span class="o">)</span>: <span class="nv">rw</span><span class="o">=</span>rw, <span class="nv">bs</span><span class="o">=(</span>R<span class="o">)</span> 8192B-8192B, <span class="o">(</span>W<span class="o">)</span> 8192B-8192B, <span class="o">(</span>T<span class="o">)</span> 8192B-8192B, <span class="nv">ioengine</span><span class="o">=</span>libaio, <span class="nv">iodepth</span><span class="o">=</span>4
  <span class="nb">read</span>: <span class="nv">IOPS</span><span class="o">=</span>10.6k, <span class="nv">BW</span><span class="o">=</span>83.0MiB/s <span class="o">(</span>87.1MB/s<span class="o">)(</span>4982MiB/60001msec<span class="o">)</span>
    slat <span class="o">(</span>usec<span class="o">)</span>: <span class="nv">min</span><span class="o">=</span>5, <span class="nv">max</span><span class="o">=</span>1693, <span class="nv">avg</span><span class="o">=</span>14.29, <span class="nv">stdev</span><span class="o">=</span>23.91
    clat <span class="o">(</span>usec<span class="o">)</span>: <span class="nv">min</span><span class="o">=</span>20, <span class="nv">max</span><span class="o">=</span>2712, <span class="nv">avg</span><span class="o">=</span>136.74, <span class="nv">stdev</span><span class="o">=</span>56.32
     lat <span class="o">(</span>usec<span class="o">)</span>: <span class="nv">min</span><span class="o">=</span>33, <span class="nv">max</span><span class="o">=</span>2724, <span class="nv">avg</span><span class="o">=</span>151.03, <span class="nv">stdev</span><span class="o">=</span>61.26
    clat percentiles <span class="o">(</span>usec<span class="o">)</span>:
     |  1.00th<span class="o">=[</span>   69],  5.00th<span class="o">=[</span>   75], 10.00th<span class="o">=[</span>   83], 20.00th<span class="o">=[</span>  113],
     | 30.00th<span class="o">=[</span>  119], 40.00th<span class="o">=[</span>  124], 50.00th<span class="o">=[</span>  131], 60.00th<span class="o">=[</span>  137],
     | 70.00th<span class="o">=[</span>  145], 80.00th<span class="o">=[</span>  155], 90.00th<span class="o">=[</span>  172], 95.00th<span class="o">=[</span>  208],
     | 99.00th<span class="o">=[</span>  355], 99.50th<span class="o">=[</span>  400], 99.90th<span class="o">=[</span>  603], 99.95th<span class="o">=[</span>  807],
     | 99.99th<span class="o">=[</span> 1385]
   bw <span class="o">(</span>  KiB/s<span class="o">)</span>: <span class="nv">min</span><span class="o">=</span>58016, <span class="nv">max</span><span class="o">=</span>92352, <span class="nv">per</span><span class="o">=</span>50.18%, <span class="nv">avg</span><span class="o">=</span>85107.09, <span class="nv">stdev</span><span class="o">=</span>4218.38, <span class="nv">samples</span><span class="o">=</span>119
   iops        : <span class="nv">min</span><span class="o">=</span> 7252, <span class="nv">max</span><span class="o">=</span>11544, <span class="nv">avg</span><span class="o">=</span>10638.39, <span class="nv">stdev</span><span class="o">=</span>527.30, <span class="nv">samples</span><span class="o">=</span>119
  write: <span class="nv">IOPS</span><span class="o">=</span>4560
...
fio-3.39
Starting 2 processes
Jobs: 2 <span class="o">(</span><span class="nv">f</span><span class="o">=</span>2<span class="o">)</span>: <span class="o">[</span>M<span class="o">(</span>2<span class="o">)][</span>100.0%][r<span class="o">=</span>170MiB/s,w<span class="o">=</span>72.6MiB/s][r<span class="o">=</span>21.7k,w<span class="o">=</span>9298 IOPS][eta 00m:00s]
seq_read: <span class="o">(</span><span class="nv">groupid</span><span class="o">=</span>0, <span class="nb">jobs</span><span class="o">=</span>1<span class="o">)</span>: <span class="nv">err</span><span class="o">=</span> 0: <span class="nv">pid</span><span class="o">=</span>1093241: Sun Mar  1 04:14:46 2026
  <span class="nb">read</span>: <span class="nv">IOPS</span><span class="o">=</span>10.6k, <span class="nv">BW</span><span class="o">=</span>83.0MiB/s <span class="o">(</span>87.1MB/s<span class="o">)(</span>4982MiB/60001msec<span class="o">)</span>

Disk stats <span class="o">(</span><span class="nb">read</span>/write<span class="o">)</span>:
  nvme1n2: <span class="nv">ios</span><span class="o">=</span>1269786/639560, <span class="nv">sectors</span><span class="o">=</span>20316576/9484696, <span class="nv">merge</span><span class="o">=</span>0/0, <span class="nv">ticks</span><span class="o">=</span>185087/244996, <span class="nv">in_queue</span><span class="o">=</span>430083, <span class="nv">util</span><span class="o">=</span>99.93%
</code></pre></div></div>

<p>From 1.1k to 15k IOPS. Almost 14x faster.</p>

<p>Based on <a href="/2023/10/17/netapp-eseries-raid1-vs-raid6-ddp-comparison.html#tests">prior work</a> I'd guess 50-150% of the improvement is due to the RAID 6 to RAID 1 change, and the rest is due to the change from Btrfs to XFS. (Why Btrfs at all? Because it has good features that XFS doesn't, excels at SQLite, so there's no reason to reject it across the board for PVE workloads.)</p>

<p>Let's try the same run on XFS and RAID 1, but with <code class="language-plaintext highlighter-rouge">numjobs=1</code> and <code class="language-plaintext highlighter-rouge">iodepth=1</code>, to align with the Proxmox random test, with the following exceptions:</p>
<ul>
  <li>Use 8 KiB rather than 4 KiB IO requests to reflect modern database page sizes</li>
  <li>Use a 70% read rather than 100% read, as 10-40% write workloads are common</li>
</ul>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>fio <span class="nt">--ioengine</span><span class="o">=</span>libaio <span class="nt">--direct</span><span class="o">=</span>1 <span class="nt">--sync</span><span class="o">=</span>1 <span class="nt">--rw</span><span class="o">=</span>readwrite <span class="nt">--rwmixwrite</span><span class="o">=</span>30 <span class="se">\</span>
  <span class="nt">--bs</span><span class="o">=</span>8K <span class="nt">--numjobs</span><span class="o">=</span>1 <span class="nt">--iodepth</span><span class="o">=</span>1 <span class="nt">--runtime</span><span class="o">=</span>60 <span class="nt">--size</span> 1G <span class="nt">--time_based</span> <span class="se">\</span>
  <span class="nt">--name</span> seq_read <span class="nt">--filename</span> /mnt/ddpr1/fio-test
seq_read: <span class="o">(</span><span class="nv">g</span><span class="o">=</span>0<span class="o">)</span>: <span class="nv">rw</span><span class="o">=</span>rw, <span class="nv">bs</span><span class="o">=(</span>R<span class="o">)</span> 8192B-8192B, <span class="o">(</span>W<span class="o">)</span> 8192B-8192B, <span class="o">(</span>T<span class="o">)</span> 8192B-8192B, <span class="nv">ioengine</span><span class="o">=</span>libaio, <span class="nv">iodepth</span><span class="o">=</span>1
fio-3.39
Starting 1 process
Jobs: 1 <span class="o">(</span><span class="nv">f</span><span class="o">=</span>1<span class="o">)</span>: <span class="o">[</span>M<span class="o">(</span>1<span class="o">)][</span>100.0%][r<span class="o">=</span>23.8MiB/s,w<span class="o">=</span>10.7MiB/s][r<span class="o">=</span>3047,w<span class="o">=</span>1369 IOPS][eta 00m:00s]
seq_read: <span class="o">(</span><span class="nv">groupid</span><span class="o">=</span>0, <span class="nb">jobs</span><span class="o">=</span>1<span class="o">)</span>: <span class="nv">err</span><span class="o">=</span> 0: <span class="nv">pid</span><span class="o">=</span>1093631: Sun Mar  1 04:17:03 2026
  <span class="nb">read</span>: <span class="nv">IOPS</span><span class="o">=</span>3100, <span class="nv">BW</span><span class="o">=</span>24.2MiB/s <span class="o">(</span>25.4MB/s<span class="o">)(</span>1453MiB/60001msec<span class="o">)</span>
    slat <span class="o">(</span>usec<span class="o">)</span>: <span class="nv">min</span><span class="o">=</span>9, <span class="nv">max</span><span class="o">=</span>556, <span class="nv">avg</span><span class="o">=</span>19.40, <span class="nv">stdev</span><span class="o">=</span> 3.82
    clat <span class="o">(</span>usec<span class="o">)</span>: <span class="nv">min</span><span class="o">=</span>26, <span class="nv">max</span><span class="o">=</span>2242, <span class="nv">avg</span><span class="o">=</span>131.77, <span class="nv">stdev</span><span class="o">=</span>33.56
     lat <span class="o">(</span>usec<span class="o">)</span>: <span class="nv">min</span><span class="o">=</span>86, <span class="nv">max</span><span class="o">=</span>2259, <span class="nv">avg</span><span class="o">=</span>151.17, <span class="nv">stdev</span><span class="o">=</span>33.89
    clat percentiles <span class="o">(</span>usec<span class="o">)</span>:
     |  1.00th<span class="o">=[</span>   79],  5.00th<span class="o">=[</span>   81], 10.00th<span class="o">=[</span>   86], 20.00th<span class="o">=[</span>  119],
     | 30.00th<span class="o">=[</span>  124], 40.00th<span class="o">=[</span>  127], 50.00th<span class="o">=[</span>  133], 60.00th<span class="o">=[</span>  139],
     | 70.00th<span class="o">=[</span>  145], 80.00th<span class="o">=[</span>  153], 90.00th<span class="o">=[</span>  161], 95.00th<span class="o">=[</span>  172],
     | 99.00th<span class="o">=[</span>  188], 99.50th<span class="o">=[</span>  212], 99.90th<span class="o">=[</span>  326], 99.95th<span class="o">=[</span>  611],
     | 99.99th<span class="o">=[</span> 1188]
   bw <span class="o">(</span>  KiB/s<span class="o">)</span>: <span class="nv">min</span><span class="o">=</span>23488, <span class="nv">max</span><span class="o">=</span>26160, <span class="nv">per</span><span class="o">=</span>100.00%, <span class="nv">avg</span><span class="o">=</span>24828.10, <span class="nv">stdev</span><span class="o">=</span>645.24, <span class="nv">samples</span><span class="o">=</span>119
   iops        : <span class="nv">min</span><span class="o">=</span> 2936, <span class="nv">max</span><span class="o">=</span> 3270, <span class="nv">avg</span><span class="o">=</span>3103.51, <span class="nv">stdev</span><span class="o">=</span>80.65, <span class="nv">samples</span><span class="o">=</span>119
  write: <span class="nv">IOPS</span><span class="o">=</span>1331, <span class="nv">BW</span><span class="o">=</span>10.4MiB/s <span class="o">(</span>10.9MB/s<span class="o">)(</span>624MiB/60001msec<span class="o">)</span><span class="p">;</span> 0 zone resets
...
Run status group 0 <span class="o">(</span>all <span class="nb">jobs</span><span class="o">)</span>:
   READ: <span class="nv">bw</span><span class="o">=</span>24.2MiB/s <span class="o">(</span>25.4MB/s<span class="o">)</span>, 24.2MiB/s-24.2MiB/s <span class="o">(</span>25.4MB/s-25.4MB/s<span class="o">)</span>, <span class="nv">io</span><span class="o">=</span>1453MiB <span class="o">(</span>1524MB<span class="o">)</span>, <span class="nv">run</span><span class="o">=</span>60001-60001msec
  WRITE: <span class="nv">bw</span><span class="o">=</span>10.4MiB/s <span class="o">(</span>10.9MB/s<span class="o">)</span>, 10.4MiB/s-10.4MiB/s <span class="o">(</span>10.9MB/s-10.9MB/s<span class="o">)</span>, <span class="nv">io</span><span class="o">=</span>624MiB <span class="o">(</span>655MB<span class="o">)</span>, <span class="nv">run</span><span class="o">=</span>60001-60001msec

Disk stats <span class="o">(</span><span class="nb">read</span>/write<span class="o">)</span>:
  nvme1n2: <span class="nv">ios</span><span class="o">=</span>185685/141282, <span class="nv">sectors</span><span class="o">=</span>2970960/1768400, <span class="nv">merge</span><span class="o">=</span>0/0, <span class="nv">ticks</span><span class="o">=</span>23835/22661, <span class="nv">in_queue</span><span class="o">=</span>46496, <span class="nv">util</span><span class="o">=</span>77.58%
</code></pre></div></div>

<p>The performance dropped from 15k to 4.4k IOPS.</p>

<p>Note that the PVE page gets 21k IOPS with the random workload, but that is with 4 KiB requests <strong>and</strong> with a 100% read workload.</p>

<p>Our second example above (10.6k in 8 KiB requests) achieves better performance. In the case filesystem fragmentation is extreme, we'd get less than 15k in 8 KiB IOPS, but if we use 100% read like the Proxmox test does, we'd more than make up for that.</p>

<p>It'd be nice to see XFS on RAID 6 (with controller cache enabled as that's how everyone would use it), but I don't want to spend even more time on this rather "academic" post that I wouldn't have written, given absence of any real-life problem associated with it.</p>

<h3 id="simple-sequential-workload">Simple sequential workload</h3>

<p>I won't share the sequential read examples to keep the post short, but all those performed very well and you may view them in the video. To cut the suspense, the same <code class="language-plaintext highlighter-rouge">fio</code> command with a 100% read workload on the RAID 6 volume with Btrfs resulted in 1 GiB/s.</p>

<p><img src="/assets/images/proxmox-pve-santricity-sequential-read.png" alt="PVE 9 100pct sequential read with Btrfs on NVMe/RoCE (EF600)" /></p>

<p>To get most out of reads you'd run them multi-threaded and - with E-Series - using a higher Queue Depth and larger request sizes (4 MiB, for example). So this is another example of a non-ideal workload - this time a sequential read - that still gets a decent result.</p>

<h3 id="summary">Summary</h3>

<p>Not one of the tests - I ran about 10-15 variants - produced suspiciously sub-par results.</p>

<p>I encourage you to run the same tests with your storage system to compare (whether you want to disable storage caching is up to you, but disabling it on the client doesn't disable it end-to-end if the storage array has read and/or write cache).</p>

<h2 id="tuning-pve-with-e-series">Tuning PVE with E-Series</h2>

<p>In the case you think "Okay, so this is almost worst, but how do I get the best, performance?"</p>

<p>This post isn't about that, but I'll add a few lines on that.</p>

<p>Check these official <a href="https://pve.proxmox.com/wiki/Performance_Tweaks">PVE performance tuning tweaks</a>.</p>

<p>For the stack used in these (generic) tests, what we could do better:</p>

<ul>
  <li>Tune storage NIC configuration (follow your NIC manufacturer's and distro's guides)</li>
  <li>Perform other OS-level settings (distro's and Linux kernel guides)</li>
  <li>Enable read-write cache on E-Series (factory default: both are enabled)</li>
  <li>Create DDP volumes not just with workload-specific RAID levels, but also other volume options (such as <a href="https://docs.netapp.com/us-en/e-series-santricity/sm-settings/cache-settings-and-performance.html">cache block size</a>). Note that RAID 1 on DDP must be done from the CLI or API (see the rest of this site or SMcli documentation)</li>
  <li>Test several filesystem options (at least one of XFS or ext4, for reference)</li>
  <li>Test classic RAID storage pools. I recommend using DDP for everything rather than breaking storage down into islands, but in extreme cases you might want to do try.</li>
</ul>

<h2 id="conclusion">Conclusion</h2>

<p>Note that the KB refers to who-knows-what kind of storage configuration and host and network (mis)configuration. This post and the video demo show exactly what I did and how using PVE 9.1.1 with NVMe/RoCE storage on EF600 in absence of any tuning and even with some self-sabotage.</p>

<p>PVE with a slower array and NL-SAS will be slower, and if it's misconfigured it will be extremely slow. But that has nothing to do with Proxmox, Debian, iSCSI, NVMe/RoCE or E-Series. PVE (or Debian, or E-Series) are expected to perform exactly as designed and not slower than other hosts or storage systems of similar configuration.</p>

<p>In theory, there may be a software-specific issue that makes PVE with E-Series slower than Red Hat with E-Series. In practice, everyone uses the same software stack (i.e. the Linux kernel, open-iscsi, NIC driversâ€¦) and components and it never happens over extended periods of time. I've never encountered any issues, let alone those that persisted over many months.</p>

<p>But saying that Proxmox or Debian would be slower because they're not validated is almost equal to saying SANtricity is tuned for the validated Linux distributions.</p>

<p>Despite wasting most of today on trivia, I've managed to expose tbe <code class="language-plaintext highlighter-rouge">block_size</code> parameter for volume resource in Terraform Provider for SANtricity. Sadly, nothing else got accomplished.</p>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/debian" class="page__taxonomy-item p-category" rel="tag">debian</a><span class="sep">, </span>
    
      <a href="/tags/e-series" class="page__taxonomy-item p-category" rel="tag">e-series</a><span class="sep">, </span>
    
      <a href="/tags/ef600" class="page__taxonomy-item p-category" rel="tag">ef600</a><span class="sep">, </span>
    
      <a href="/tags/netapp" class="page__taxonomy-item p-category" rel="tag">netapp</a><span class="sep">, </span>
    
      <a href="/tags/nvme" class="page__taxonomy-item p-category" rel="tag">nvme</a><span class="sep">, </span>
    
      <a href="/tags/performance" class="page__taxonomy-item p-category" rel="tag">performance</a><span class="sep">, </span>
    
      <a href="/tags/proxmox" class="page__taxonomy-item p-category" rel="tag">proxmox</a><span class="sep">, </span>
    
      <a href="/tags/pve" class="page__taxonomy-item p-category" rel="tag">pve</a><span class="sep">, </span>
    
      <a href="/tags/rdma" class="page__taxonomy-item p-category" rel="tag">rdma</a><span class="sep">, </span>
    
      <a href="/tags/roce" class="page__taxonomy-item p-category" rel="tag">roce</a><span class="sep">, </span>
    
      <a href="/tags/trixie" class="page__taxonomy-item p-category" rel="tag">trixie</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/e-series" class="page__taxonomy-item p-category" rel="tag">e-series</a><span class="sep">, </span>
    
      <a href="/categories/storage" class="page__taxonomy-item p-category" rel="tag">storage</a><span class="sep">, </span>
    
      <a href="/categories/virtualization" class="page__taxonomy-item p-category" rel="tag">virtualization</a>
    
    </span>
  </p>


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time class="dt-published" datetime="2026-03-01T00:00:00+08:00">2026-03-01 00:00</time></p>

      </footer>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://x.com/intent/tweet?via=scaleoutSean&text=Proxmox+PVE+9.1.1+%28Debian%29+with+NetApp+EF-Series+NVMe%2FRoCE%20https%3A%2F%2Fscaleoutsean.github.io%2F2026%2F03%2F01%2Fproxmox-pve-with-netapp-eseries.html" class="btn btn--x" aria-label="Share on X" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on X">
    <i class="fab fa-fw fa-x-twitter" aria-hidden="true"></i><span> X</span>
  </a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://scaleoutsean.github.io/2026/03/01/proxmox-pve-with-netapp-eseries.html" class="btn btn--linkedin" aria-label="Share on LinkedIn" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn">
    <i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span>
  </a>

</section>


      
  <nav class="pagination">
    
      <a href="/2026/03/01/adblock-and-netapp-docs.html" class="pagination--pager" title="Block external sites to improve loading speed of NetApp docs">Previous</a>
    
    
      <a href="#" class="pagination--pager disabled">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
    
<div class="page__related">
  
  <h2 class="page__related-title">You may also enjoy</h2>
  <div class="grid__wrapper">
    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/2026/03/01/adblock-and-netapp-docs.html" rel="permalink">Block external sites to improve loading speed of NetApp docs
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2026-03-01T00:00:00+08:00">2026-03-01 00:00</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          1 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Block external domains on https://docs.netapp.com
</p>
  </article>
</div>

    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/2026/02/26/ibm-block-storage-cis-driver-santricity-fork.html" rel="permalink">IBM Block Storage CSI driver patched for NetApp E-Series
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2026-02-26T00:00:00+08:00">2026-02-26 00:00</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          11 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Unofficial patch to IBM Block CSI driver for E-Series SANtricity storage
</p>
  </article>
</div>

    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/2026/02/19/the-shocking-truth-about-ef600-200g-ports.html" rel="permalink">Connecting to 200G HICs on E-Series EF600
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2026-02-19T00:00:00+08:00">2026-02-19 00:00</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          2 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Connecting to EF-Series EF600 200G HIC in DAS mode
</p>
  </article>
</div>

    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/2026/02/19/linux-nvme-roce-ef-series.html" rel="permalink">Linux, NVMe/RoCE and HA NetApp EF-Series NVMe-oF storage
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2026-02-19T00:00:00+08:00">2026-02-19 00:00</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          18 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Linux NVMe/RoCE clients with NetApp EF-Series NVMe-oF
</p>
  </article>
</div>

    
  </div>
</div>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        

<div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
          <li><a href="https://twitter.com/scaleoutsean" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/scaleoutsean" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>


<div class="page__footer-copyright">&copy; 2020-2026 scaleoutSean.github.io | <a href="https://scaleoutsean.github.io">Acting Technologist</a> | Terms: <a href="https://creativecommons.org/licenses/by/4.0/deed.en" target="_blank">CC BY 4.0</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>






  </body>
</html>
