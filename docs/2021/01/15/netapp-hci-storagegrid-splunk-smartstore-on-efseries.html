<!doctype html>
<html lang="en">
  <head>
        <meta charset="UTF-8">
    <meta http-equiv="content-language" content="en">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="googlebot" content="noarchive">
    <meta name="googlebot" content="max-snippet:60">
    <meta name="googlebot" content="max-image-preview:small">
    <meta name="bingbot" content="noarchive">
    <meta name="bingbot" content="max-snippet:60">
    <meta name="bingbot" content="max-image-preview:small">
    <meta name="robots" content="noarchive">
    <meta name="robots" content="max-snippet:60">
    <meta name="google-site-verification" content="F6q7vIwQ2G0j8tk-KL9rAXOMLXMDMMUkEz4fRs1Nnnc" />
    <title>
        
            Virtualized StorageGRID for Splunk Smartstore on NetApp HCI and EF-Series array | Acting Technologist
      
    </title>
    <meta name="description" content="
     Virtualized StorageGRID with NetApp HCI and EF-Series
     ">

    <!-- LINK TO ATOM FEED FOR SEO -->
    <link rel="alternate" type="application/atom+xml" href="https://scaleoutsean.github.io/feed.xml" />

    <!-- FAVICON -->
    <link rel="icon" href="/assets/favicon.ico" type="image/x-icon">
    <link rel="shortcut icon" href="/assets/favicon.ico" type="image/x-icon">

    <!-- CSS -->
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="stylesheet" href="/assets/css/syntax.css">

    <!-- Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css">

    <link href="/_pagefind/pagefind-ui.css" rel="stylesheet">
    <script src="/_pagefind/pagefind-ui.js" type="text/javascript"></script>
    <div id="search"></div>
    <script src="https://cdn.counter.dev/script.js" data-id="83dc700a-b821-4e57-9425-02b8336a9456" data-utcoffset="0"></script>
    <script>
        window.addEventListener('DOMContentLoaded', (event) => {
            new PagefindUI({ element: "#search" });
        });
    </script>

    <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Virtualized StorageGRID for Splunk Smartstore on NetApp HCI and EF-Series array | Acting Technologist</title>
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="Virtualized StorageGRID for Splunk Smartstore on NetApp HCI and EF-Series array" />
<meta name="author" content="scaleoutSean" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Virtualized StorageGRID with NetApp HCI and EF-Series" />
<meta property="og:description" content="Virtualized StorageGRID with NetApp HCI and EF-Series" />
<link rel="canonical" href="https://scaleoutsean.github.io/2021/01/15/netapp-hci-storagegrid-splunk-smartstore-on-efseries.html" />
<meta property="og:url" content="https://scaleoutsean.github.io/2021/01/15/netapp-hci-storagegrid-splunk-smartstore-on-efseries.html" />
<meta property="og:site_name" content="Acting Technologist" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-01-15T00:00:00+08:00" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://scaleoutsean.github.io/2021/01/15/netapp-hci-storagegrid-splunk-smartstore-on-efseries.html"},"author":{"@type":"Person","name":"scaleoutSean"},"description":"Virtualized StorageGRID with NetApp HCI and EF-Series","@type":"BlogPosting","url":"https://scaleoutsean.github.io/2021/01/15/netapp-hci-storagegrid-splunk-smartstore-on-efseries.html","headline":"Virtualized StorageGRID for Splunk Smartstore on NetApp HCI and EF-Series array","dateModified":"2021-01-15T00:00:00+08:00","datePublished":"2021-01-15T00:00:00+08:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  </head>

<body>

  <div class="container">
    <header id="header">
	<div id = site_title>
		<a href="https://scaleoutsean.github.io/">
			<h2 style="font-size:1.8em;">Acting Technologist</h2>
		</a>
		
		<h3>
			civilizations are created by individuals
		</h3>
		
	</div>

	<div id="subheader">
		
		<nav class="pages">
<a href="/about.html">About</a>

<a href="/archive.html">Archive</a>

<a href="/categories/">Categories</a>

<a href="/projects.html">Projects</a>
</nav>
		
		
		<nav class="social">
			
  
    <a href="https://www.github.com/scaleoutsean" target="_blank" id="github"><i class="fab fa-github" aria-hidden="true"></i></a>
  
   

  
    <a href="https://twitter.com/scaleoutsean" target="_blank" id="twitter"><i class="fab fa-twitter" aria-hidden="true"></i></a>
  
   

  
  
    <a href="/feed.xml" target="_blank" id="rss"><i class="fas fa-rss" aria-hidden="true"></i></a>
   

		</nav>
		
	</div>
</header>


    <div class="post-container">
      <article id = "post">
        <h1 id = "post-title">Virtualized StorageGRID for Splunk Smartstore on NetApp HCI and EF-Series array</h1>

  
  <div class = "post-info">
    <span>
        <i class="far fa-calendar" aria-hidden="true"></i> <span>15 Jan 2021</span> - <i class="far fa-clock"></i> 


  
  
    15 minute read
  

    </span>
  </div>
  
        <p>If you’d like to deploy Splunk SmartStore with NetApp StorageGRID object storage but also want to “start small”, you can consider using smaller StorageGRID appliances or deploy StorageGRID as virtual machines.</p>

<p>Which one is better? Both are great. Which one is better for <em>you</em> depends on your current, and (expected) future, requirements and preferences.</p>

<p>This post is about the latter - addressing smaller SmartStore requirements which need to be satisfied on-premises by deploying StorageGRID VMs on NetApp HCI.</p>

<ul>
  <li><a href="#assumptions">Assumptions</a></li>
  <li><a href="#netapp-hci-with-mellanox-sn2010-and-ef570">NetApp HCI with Mellanox SN2010 and EF570</a></li>
  <li><a href="#vm-to-storage-mapping">VM-to-storage mapping</a>
    <ul>
      <li><a href="#compute-node-sizing-for-splunk-and-storagegrid">Compute node sizing for Splunk and StorageGRID</a></li>
      <li><a href="#vm-storage-sizing-for-data-on-netapp-hci-storage-nodes">VM storage sizing for data on NetApp HCI storage nodes</a></li>
      <li><a href="#tier-capacity-calculation-on-ef-series-array">Tier capacity calculation on EF-Series array</a></li>
    </ul>
  </li>
  <li><a href="#capacity-and-performance-scaling">Capacity and performance scaling</a></li>
  <li><a href="#transition-to-storagegrid-appliances">Transition to StorageGRID appliances</a></li>
  <li><a href="#reasons-for-smartstore-on-storagegrid-vms-and-ef-series">Reasons for SmartStore on StorageGRID VMs and EF-Series</a>
    <ul>
      <li><a href="#storage-overheads-compared">Storage overheads compared</a></li>
    </ul>
  </li>
  <li><a href="#summary">Summary</a></li>
</ul>

<h2 id="assumptions">Assumptions</h2>

<p>What’s a “smaller” SmartStore requirement? That’s also subjective, but let’s assume that is (currently) somewhere between 50 and 300 TB.</p>

<p>It is likely that this environment doesn’t have a lot of busy Splunk indexers, so let’s also assume four indexers running across four ESXi hosts, for which we may be using either blade servers (H410C, 4 per 2U chassis) or “pizza box” H615C servers, with enough CPU cores for all VMs involved even when one server fails (otherwise, if we wanted to be able to process data with “N-1” ESXi hosts, we’d need one more server).</p>

<p>Since we’ll use a flash array for SmartStore data, we can use the same array for Splunk Hot Tier (and SmartStore cache).</p>

<h2 id="netapp-hci-with-mellanox-sn2010-and-ef570">NetApp HCI with Mellanox SN2010 and EF570</h2>

<p>We’d pick a pair of switches (Mellanox SN2100) and one of all-flash EF-Series arrays such as the EF570 configured with eight 25G iSCSI ports (4 per controller):</p>

<ul>
  <li>For this situation NetApp EF-Series array would suffice both capacity- and performance-wise, but considering that we’ll use the same array for Hot Tier and Smartstore, we pick one of faster models, EF570</li>
  <li>Each Mellanox SN2010 has 18 25G ports, which means we have 36 ports at our disposal (excluding inter-switch links and uplinks)
    <ul>
      <li>Four H410C (4 x 25G on each) compute nodes take 16 ports</li>
      <li>Two H410S (HCI storage nodes used for all VMs including Splunk, StorageGRID, etc.) consume 4 ports</li>
      <li>The EF570 with iSCSI add-on card uses eight ports, so by now we’ve used 28 out of 36, which means we can add two more H410C compute nodes at a later time. If we expect to need more than that before next h/w refresh, we could start with a larger Mellanox Ethernet switch (SN2100)</li>
    </ul>
  </li>
</ul>

<p>That would look approximately like this (with 16 rather than the pictured 4 network cables from NetApp HCI (top) to Mellanox SN2010 switches):</p>

<p><img src="/assets/images/splunk-smartstore-storagegrid-vm-netapp-hci-ef-01.png" alt="NetApp HCI with Mellanox SN2010 and EF570" /></p>

<h2 id="vm-to-storage-mapping">VM-to-storage mapping</h2>

<p>If we wanted to start with a very small configuration, we could use the EF570 which can fit up to 24 SSD drives in the controller shelf (same as other currently available EF-Series models) and scale to 5 shelves (including the controller shelf).</p>

<ul>
  <li>For VMs running Splunk indexers attached to Hot tier and SmartStore cache, we want volumes on a RAID 10 group because we don’t need a lot of capacity, we want a fast performance and we expect that in the future SmartStore will grow much faster than this tier so RAID 10 is perfect for that</li>
  <li>StorageGRID storage VMs can store their data on a more redundant (in terms of tolerance to disk failures) disk group such as RAID 6 or DDP pool (<a href="/2021/01/04/elasticsearch-on-netapp-h615c-ef280.html#elasticsearch-data-protection-with-ef-series">this post explains</a> main differences between the two, but let’s just say RAID 6 and DDP are similar)
    <ul>
      <li>Because we have four ESXi hosts and sufficient resources to tolerate the failure of one ESXi host, we chose to use four storage VMs and store objects using Erasure Coding 2+1, which requires N+M+1 VMs (2+1+1=4)</li>
      <li>Considering the small initial size of this StorageGRID, we take 20 disks to create one DDP pool and on top of it create four volumes (for four VMware datastores), so we have just one volume per each StorageGRID storage node</li>
    </ul>
  </li>
  <li>Splunk, StorageGRID and all other VM OS disks live on NetApp HCI storage - we have just two of them (H610S-1) because that’s enough both capacity- and redundancy-wise (we can tolerate the loss of one HCI storage node). If we had over 100 VMs in this environment we might need more capacity and perofrmance and add additional nodes.</li>
</ul>

<p>Showing Splunk-related data on the EF array (StorageGRID 4 storage node VMs on the left and Splunk indexer volumes on the right with only two VMs to declutter the image) would look similar to this:</p>

<p><img src="/assets/images/splunk-smartstore-storagegrid-vm-netapp-hci-ef-02.png" alt="EF-Series storage layout with single shelf for StorageGRID and Splunk indexers" /></p>

<p>Splunk indexers running Linux could use LVM to combine two physical volumes into one larger logical volume, but we could also create large volumes on the array to avoid having to do that. LVM would be desirable if we expected Hot Tier may need to grow without new indexers.</p>

<p>If we expect our data retention requirements will soon increase or need more disks for SmartStore, we can start with two partially (or fully) populated shelves. Only one VM of each service (StorageGRID storage node, Splunk indexer) represents each workload.</p>

<p><img src="/assets/images/splunk-smartstore-storagegrid-vm-netapp-hci-ef-04.png" alt="EF-Series storage layout with two shelves for StorageGRID and Splunk indexers" /></p>

<h3 id="compute-node-sizing-for-splunk-and-storagegrid">Compute node sizing for Splunk and StorageGRID</h3>

<p>Sources:</p>

<ul>
  <li>Splunk <a href="https://docs.splunk.com/Documentation/Splunk/8.1.0/Capacity/Referencehardware">recommendations</a> for v8.1</li>
  <li>StorageGRID <a href="http://docs.netapp.com/sgws-114/index.jsp?topic=%2Fcom.netapp.doc.sg-admin%2FGUID-B9B9FB7B-76FA-4C85-99A7-4310E3F24F1C.html">requirements</a> for v11.4 VMs on vSphere</li>
  <li>NetApp HCI <a href="https://www.netapp.com/media/7977-ds-3881.pdf">datasheet</a> as of now</li>
</ul>

<p>Following those requirements and recommendations I picked 16 cores (between basic and medium recommendation) for Splunk indexers.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Item</th>
      <th style="text-align: left">Detail</th>
      <th style="text-align: center">VM x Cores</th>
      <th style="text-align: right">Total Cores</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Splunk</td>
      <td style="text-align: left"> </td>
      <td style="text-align: center"> </td>
      <td style="text-align: right"><strong>82</strong></td>
    </tr>
    <tr>
      <td style="text-align: left"> </td>
      <td style="text-align: left">Indexer</td>
      <td style="text-align: center">4 x 16</td>
      <td style="text-align: right">64</td>
    </tr>
    <tr>
      <td style="text-align: left"> </td>
      <td style="text-align: left">Search Head</td>
      <td style="text-align: center">1 x 16</td>
      <td style="text-align: right">16</td>
    </tr>
    <tr>
      <td style="text-align: left">StorageGRID</td>
      <td style="text-align: left"> </td>
      <td style="text-align: center"> </td>
      <td style="text-align: right"><strong>56</strong></td>
    </tr>
    <tr>
      <td style="text-align: left"> </td>
      <td style="text-align: left">Admin</td>
      <td style="text-align: center">1 x 8</td>
      <td style="text-align: right">8</td>
    </tr>
    <tr>
      <td style="text-align: left"> </td>
      <td style="text-align: left">Backup Admin</td>
      <td style="text-align: center">1 x 8</td>
      <td style="text-align: right">8</td>
    </tr>
    <tr>
      <td style="text-align: left"> </td>
      <td style="text-align: left">Storage Node</td>
      <td style="text-align: center">4 x 8</td>
      <td style="text-align: right">32</td>
    </tr>
    <tr>
      <td style="text-align: left"> </td>
      <td style="text-align: left">Gateway Node</td>
      <td style="text-align: center">1 x 8</td>
      <td style="text-align: right">8</td>
    </tr>
    <tr>
      <td style="text-align: left">Other</td>
      <td style="text-align: left">VCSA, HCI Mgmt, Splunk UFW</td>
      <td style="text-align: center">2 x 8</td>
      <td style="text-align: right"><strong>16</strong></td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>TOTAL</strong></td>
      <td style="text-align: left"> </td>
      <td style="text-align: center"> </td>
      <td style="text-align: right"><strong>154</strong></td>
    </tr>
  </tbody>
</table>

<p>At a 75% average utilization, 115 cores would be required.</p>

<p>StorageGRID has a lot of CPUs allocated to it, but we know its only hard workload is searches for non-cached data, therefore with one ESXi down only searches that download non-cached SmartStore data could be slightly, if at all, impacted.</p>

<p>Using H410C compute nodes:</p>

<ul>
  <li>4 x H410C with 2 x Xeon Gold 6138 (20 cores) = 160 cores
    <ul>
      <li>120 cores with one ESXi node down - enough to run Splunk at ~90% and the rest at ~50%</li>
    </ul>
  </li>
</ul>

<p>If using H615C compute nodes (these need just 2 x 10/25G ports per server, but require vSphere Enterprise Plus):</p>

<ul>
  <li>4 x H615C with 2 x Xeon Gold 6252 (24 cores) = 192 cores
    <ul>
      <li>144 cores with one ESXi node down</li>
      <li>Because these 1U nodes require just two SFP28 ports each, we can double Splunk cluster size without adding additional ethernet switches</li>
    </ul>
  </li>
</ul>

<p>This sizing exercise is obviously just an example that should be further refined.</p>

<p>I did not consider RAM requirements because NetApp HCI compute nodes come with plenty of RAM and we could probably beef up RAM resources of Splunk search head and indexers above the Splunk-recommended values.</p>

<h3 id="vm-storage-sizing-for-data-on-netapp-hci-storage-nodes">VM storage sizing for data on NetApp HCI storage nodes</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Item</th>
      <th style="text-align: right">Size (GB)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">OS for all Splunk VMs</td>
      <td style="text-align: right">200</td>
    </tr>
    <tr>
      <td style="text-align: left">OS and Data for SG Admin &amp; Backup</td>
      <td style="text-align: right">1,000</td>
    </tr>
    <tr>
      <td style="text-align: left">OS for SG Storage Nodes</td>
      <td style="text-align: right">400</td>
    </tr>
    <tr>
      <td style="text-align: left">OS for SG Gateway Node</td>
      <td style="text-align: right">100</td>
    </tr>
    <tr>
      <td style="text-align: left">OS and Data for NetApp HCI Management Node</td>
      <td style="text-align: right">500</td>
    </tr>
    <tr>
      <td style="text-align: left">Everything else</td>
      <td style="text-align: right">500</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>TOTAL</strong></td>
      <td style="text-align: right"><strong>2,700</strong></td>
    </tr>
  </tbody>
</table>

<p>NetApp HCI usually gets 3x storage efficiency, some of this stuff would deduplicate really well (e.g. all OS) and Management Node can be thin-provisioned. It’s possible only 1TiB of usable capacity might be enough - especially if we moved StorageGRID Admin Nodes out to that DDP pool on EFF (that would take away 1TB from that pool later so keep that in mind when sizing).</p>

<p>Two H610S-1 (12 x 960 GB SSD each) are more than enough for this. If we wanted to use H410C compute nodes (which use the same chassis as H410S storage nodes), we could consider using two or three of the smallest H410S (6 x 480GB SSD each) instead, but it would be wise to give a try in the lab to avoid surprises.</p>

<p>If you’re interested in the details of deploying StorageGRID on NetApp HCI, see <a href="https://www.netapp.com/pdf.html?item=/media/17114-tr4734pdf.pdf">NetApp TR-4734</a> but remember that with storage node data on EF-Series, some of the detailed tuning advice for NetApp HCI storage would not be required.</p>

<h3 id="tier-capacity-calculation-on-ef-series-array">Tier capacity calculation on EF-Series array</h3>

<p>How large would these volumes be? That depends on disk size(s) (from 1.9 to 15.3 TB). They could all have the same size, or we could pick a larger capacity for SSDs in DDP pool, and use a smaller size for the 4 disks in RAID 10 (or the other way around). We also don’t have to populate all 24 slots at once.</p>

<p>With 3.8 TB disks in all the 24 slots we’d have:</p>

<ul>
  <li>Approximately 60 TB for SmartPool data which, after Erasure Coding applied by StorageGRID, translates into 45 TB. If by “50 TB” we meant “50 TB for Splunk SmartStore”, we could use 12 (or thereabout) 7.6 TB disks instead. Each StorageGRID node would have one or two volumes smaller than 20TB.</li>
  <li>Approximately 7 TB for Hot/Cache Tier which translates to almost 2 TB per each of 4 indexers and enough for 3-4 days of data (with RF=2, daily volume 1,000 GB/day). To make that a week, we’d use the same number of 7.6 TB disks. Using RF=2 on hosts that all have their data on the same RAID 10 doesn’t enhance protection from disk failures, but can help prevent downtime to issues with individual indexer OS, filesystem corruption and such.</li>
</ul>

<p>It doesn’t have to be done exactly this way. The idea is to find a balance in data protection, performance, manageability (avoid complexity, avoid too many volumes) and other things we care about.</p>

<p>Larger disks in DDP pool translate into a larger SmartStore capacity, while larger disks in RAID 10 normally result in a better performance for search and reporting because relatively more data would be available for immediate queries (without having to fetch it from SmartStore).</p>

<p>Because the optimal size of each tier greatly varies between use cases, applications and user behaviors, even within one Splunk cluster there could be order-of-magnitude differences between optimal settings for each index (such as 3 days on Hot Tier for one index vs. 30 days for another).</p>

<h2 id="capacity-and-performance-scaling">Capacity and performance scaling</h2>

<p>Compute resources could scale to dozens of ESXi servers as common with VMware. But - as mentioned above - if we started with the smallest network switch model (SN2010) we’d have to add additional switches for that, or start with one of larger models (SN2100, SN2700) to avoid the hassle of having to make modifications too soon.</p>

<p>Storage-wise, we could add capacity - and quite a bit of performance - by adding additional disk shelves.</p>

<p>In picture below, to make it easier to on the eye, only one of four Splunk indexers is shown and only some disk groups are highlighted:</p>

<ul>
  <li>StorageGRID capacity is now carved out of independent RAID 6 disk groups compared to one DDP in the earlier image</li>
  <li>Splunk Hot/Cache is across two independent RAID 10 disk groups (LVM with concatenated logical volumes, Splunk RF still 2)</li>
</ul>

<p><img src="/assets/images/splunk-smartstore-storagegrid-vm-netapp-hci-ef-03.png" alt="Fully populated EF570 array for Splunk Smartstore and Hot/Cache Tier" /></p>

<p>Above illustrations aren’t necessarily “best” ways to lay out storage (for Splunk or in general) but they are “close enough” and indend to show there are different ways to get the capacity, performance, and redundancy for various requirements.</p>

<p>We could increase SmartStore capacity at least fivefold (by adding 4 additional shelves to the initial deployment with one, controller, shelf) before we’d had to add another storage array or StorageGRID appliances. Why “at least” fivefold?</p>

<ul>
  <li>We could use larger disk drives in other shelves</li>
  <li>If we didn’t have to grow Hot/Cache tier shelves #2 to #5 could use more than 20 drives per shelf for StorageGRID VMs serving SmartStore</li>
  <li>If it turned out RAID 10 was too generous performance-wise (i.e. low IOPS observed on Hot/Cache), we could use DDP across the board</li>
</ul>

<h2 id="transition-to-storagegrid-appliances">Transition to StorageGRID appliances</h2>

<p>What if we <em>didn’t</em> want to go beyond one EF570, but instead decided it was time to move SplunkStore to StorageGRID appliances? No problem!</p>

<p>Because StorageGRID clusters can be asymmetric and heterogeneous, it’s possible to further expand this SmartStore deployment not just beyond the size and performance of multiple EF570 arrays, but also with StorageGRID appliances.</p>

<p>We could expand our VM-based StorageGRID cluster with physical StorageGRID appliances, and even gracefully remove StorageGRID storage node VMs from the cluster, effectively migrating SmartStore data out to StorageGRID appliances without downtime. Other, “storage-less” StorageGRID VMs (administrative VMs, for example) would still live on NetApp HCI but if you wanted to move everything out to StorageGRID appliances, you could (using the StorageGRID SG models which provide network load balancing as well as administrative service).</p>

<p>That would release some or all of the capacity on the EF570, which could be reused for additional Hot/Cache data, or other applications in your NetApp HCI environment.</p>

<p>If we had no immediate need for the freed DDP capacity, we could let StorageGRID run across both VMs and appliances, also transparently to Splunk.</p>

<h2 id="reasons-for-smartstore-on-storagegrid-vms-and-ef-series">Reasons for SmartStore on StorageGRID VMs and EF-Series</h2>

<p>Some may wonder: why complicate things and use SmartStore when Erasure Coding 2+1 on top of DDP only decreases (by 33%) the amount of capacity that could otherwise be used for Splunk Warm Tier?</p>

<p>First, SmartStore has several operational benefits and if you think that’s worth the extra cost, this is one of less expensive ways to deploy SmartStore on SSDs. Most object storage appliances start at more than 50 TB so you’d probably start with not just 10-20 TB of overhead (compared to Warm Tier on DDP, for example) in any case.</p>

<p>Second, deploying a smaller SmartStore to better understand how it fits your use case (search and reporting patterns, retention requirements, and so on) can help you save money when you deploy SmartStore at scale. You may discover there are indexes that you don’t want to put on SmartStore and decide to add a Warm Tier on a new RAID 5 disk group, for example.</p>

<p>Third, if your SmartStore continues to grow, you can recover capacity allocated to StorageGRID storage VMs.</p>

<p>Four, suppose you realize you need to retain SmartStore data for 365 days, but your search and reporting timelines go back only up to 60 days. Maybe you don’t need 500 TB of all-flash SmartStore capacity? You could expand this EF-based StorageGRID cluster with HDD-based StorageGRID appliances, and use StorageGRID ILM to store SmartStore data in two object store tiers: (1) fast SSD tier on EF-Series, and (2) lower cost tier on StorageGRID appliances. StorageGRID can also use object storage in Public Cloud as its low cost tier.</p>

<p>Five, if you needed an off-site copy of SmartStore data, you could expand StorageGRID with HDD-based appliances at a remote location (it’s never searched, so maybe HDDs will do?) and use StorageGRID ILM policies to replicate SmartStore data to it.</p>

<p>Six, my earlier statement about losing 33% capacity was not entirely correct: SmartStore saves just one copy of data to Remote Tier, so with tiering to StorageGRID, Splunk RF changes from 2 to 1. Two copies from RAID 10 become one copy on DDP pool with a 50% overhead due to erasure coding used to protect StorageGRID data.</p>

<p>There’s more, but my point is: most Splunk users may not care about some of these reasons, but everyone starting with SmartStore should be able relate to at least one or two, and that can matter enough to consider merits of this approach.</p>

<h4 id="storage-overheads-compared">Storage overheads compared</h4>

<p>To elaborate on point six, here’s a look at the overheads involved in our approach with the first, controller shelf (this doesn’t concern itself with Splunk compression and metadata):</p>

<table>
  <thead>
    <tr>
      <th>Tier</th>
      <th style="text-align: center">DDP</th>
      <th style="text-align: center">RAID 10</th>
      <th style="text-align: center">Splunk RF</th>
      <th style="text-align: center">Object Store (EC 2+1)</th>
      <th style="text-align: center">TOTAL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Hot/Cache</td>
      <td style="text-align: center">-</td>
      <td style="text-align: center">2x</td>
      <td style="text-align: center">2x</td>
      <td style="text-align: center">-</td>
      <td style="text-align: center">4.0x</td>
    </tr>
    <tr>
      <td>SmartStore</td>
      <td style="text-align: center">1.1x</td>
      <td style="text-align: center">-</td>
      <td style="text-align: center">1x</td>
      <td style="text-align: center">1.5x</td>
      <td style="text-align: center">1.7x</td>
    </tr>
  </tbody>
</table>

<p>If we used DDP for all data or had more (or less) disks in DDP pool, we’d get different results. Hot/Cache Tier has 4x overhead but its cost is minimal (it’s just four drives for the entire deployment, not per each indexer).</p>

<p>SmartStore on EF-Series doesn’t necessarily have a negative impact on usable storage capacity available to Splunk.</p>

<h2 id="summary">Summary</h2>

<p>Splunk users on NetApp HCI with EF-Series arrays can take advantage of SmartStore by using StorageGRID VMs backed by an EF-Series array.</p>

<p>After the initial capacity requirements are confirmed, we select a suitable disk size for DDP pool and from that moment it is very easy to increase SmartStore capacity in <em>single drive</em> increments and scale it to hundreds of TBs without complications.</p>

<p>If you’re interested in starting small with an on-premises SmartStore, I hope this post motivates you to evaluate this approach.</p>

      </article>
    </div>
    
      <div class="categories">
    <span><p>Categories:</p>
    
    
      <a href="
      /categories/#virtualization">virtualization</a>
      &nbsp; 
    
      <a href="
      /categories/#analytics">analytics</a>
       
    
  </span>
</div>
    

  
    <div>
      <h3>Related Posts</h3>
      <ul>
      
        <li><a href="/2021/01/04/elasticsearch-on-netapp-h615c-ef280.html">Elasticsearch on NetApp HCI H615C with EF-Series EF280</a></li>
      
        <li><a href="/2023/11/06/netapp-eseries-sizing-for-splunk-smartstore.html">NetApp E-Series sizing for Splunk 9 with SmartStore</a></li>
      
        <li><a href="/2023/02/18/epa-eseries-monitor-sensors-psu-power-consumption.html">Gather and visualize E-Series temperature and power consumption metrics with EPA</a></li>
      
        <li><a href="/2023/01/14/eseries-performance-analyzer-container-orchestrator-kubernetes.html">NetApp E-Series Performance Analyzer in orchestrated container environments</a></li>
      
        <li><a href="/2023/01/08/eseries-flash-ssd-wear-level-monitoring.html">Monitor wear level of NetApp E-Series flash drives via the API and from the CLI</a></li>
      
      </ul>
    </div>
  

    
  </div><footer class= "footer">
    <p>2025-05-20 20:32 </p>
    <p>Copyright © 2025 scaleoutSean. Content is released under the CC BY license. Design: Alessio Franceschi</p>
</footer>

</body>
</html>
