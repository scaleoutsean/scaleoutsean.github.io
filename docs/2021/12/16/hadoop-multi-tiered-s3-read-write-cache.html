<!doctype html>
<html lang="en">
  <head>
        <meta charset="UTF-8">
    <meta http-equiv="content-language" content="en">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="googlebot" content="noarchive">
    <meta name="googlebot" content="max-snippet:60">
    <meta name="googlebot" content="max-image-preview:small">
    <meta name="bingbot" content="noarchive">
    <meta name="bingbot" content="max-snippet:60">
    <meta name="bingbot" content="max-image-preview:small">
    <meta name="robots" content="noarchive">
    <meta name="robots" content="max-snippet:60">
    <meta name="google-site-verification" content="F6q7vIwQ2G0j8tk-KL9rAXOMLXMDMMUkEz4fRs1Nnnc" />
    <title>
        
            Hadoop with Multi-Tiered Read-Write S3 cache using Alluxio and NetApp StorageGRID | Acting Technologist
      
    </title>
    <meta name="description" content="
     Eliminate legacy storage approach without compromises by using multi-tiered read-write cache with StorageGRID
     ">

    <!-- LINK TO ATOM FEED FOR SEO -->
    <link rel="alternate" type="application/atom+xml" href="https://scaleoutsean.github.io/feed.xml" />

    <!-- FAVICON -->
    <link rel="icon" href="/assets/favicon.ico" type="image/x-icon">
    <link rel="shortcut icon" href="/assets/favicon.ico" type="image/x-icon">

    <!-- CSS -->
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="stylesheet" href="/assets/css/syntax.css">

    <!-- Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css">

    <link href="/_pagefind/pagefind-ui.css" rel="stylesheet">
    <script src="/_pagefind/pagefind-ui.js" type="text/javascript"></script>
    <div id="search"></div>
    <script src="https://cdn.counter.dev/script.js" data-id="83dc700a-b821-4e57-9425-02b8336a9456" data-utcoffset="0"></script>
    <script>
        window.addEventListener('DOMContentLoaded', (event) => {
            new PagefindUI({ element: "#search" });
        });
    </script>

    <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Hadoop with Multi-Tiered Read-Write S3 cache using Alluxio and NetApp StorageGRID | Acting Technologist</title>
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="Hadoop with Multi-Tiered Read-Write S3 cache using Alluxio and NetApp StorageGRID" />
<meta name="author" content="scaleoutSean" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Eliminate legacy storage approach without compromises by using multi-tiered read-write cache with StorageGRID" />
<meta property="og:description" content="Eliminate legacy storage approach without compromises by using multi-tiered read-write cache with StorageGRID" />
<link rel="canonical" href="https://scaleoutsean.github.io/2021/12/16/hadoop-multi-tiered-s3-read-write-cache.html" />
<meta property="og:url" content="https://scaleoutsean.github.io/2021/12/16/hadoop-multi-tiered-s3-read-write-cache.html" />
<meta property="og:site_name" content="Acting Technologist" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-12-16T00:00:00+08:00" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://scaleoutsean.github.io/2021/12/16/hadoop-multi-tiered-s3-read-write-cache.html"},"author":{"@type":"Person","name":"scaleoutSean"},"description":"Eliminate legacy storage approach without compromises by using multi-tiered read-write cache with StorageGRID","@type":"BlogPosting","url":"https://scaleoutsean.github.io/2021/12/16/hadoop-multi-tiered-s3-read-write-cache.html","headline":"Hadoop with Multi-Tiered Read-Write S3 cache using Alluxio and NetApp StorageGRID","dateModified":"2021-12-16T00:00:00+08:00","datePublished":"2021-12-16T00:00:00+08:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  </head>

<body>

  <div class="container">
    <header id="header">
	<div id = site_title>
		<a href="https://scaleoutsean.github.io/">
			<h2 style="font-size:1.8em;">Acting Technologist</h2>
		</a>
		
		<h3>
			civilizations are created by individuals
		</h3>
		
	</div>

	<div id="subheader">
		
		<nav class="pages">
<a href="/about.html">About</a>

<a href="/archive.html">Archive</a>

<a href="/categories/">Categories</a>

<a href="/projects.html">Projects</a>
</nav>
		
		
		<nav class="social">
			
  
    <a href="https://www.github.com/scaleoutsean" target="_blank" id="github"><i class="fab fa-github" aria-hidden="true"></i></a>
  
   

  
    <a href="https://twitter.com/scaleoutsean" target="_blank" id="twitter"><i class="fab fa-twitter" aria-hidden="true"></i></a>
  
   

  
  
    <a href="/feed.xml" target="_blank" id="rss"><i class="fas fa-rss" aria-hidden="true"></i></a>
   

		</nav>
		
	</div>
</header>


    <div class="post-container">
      <article id = "post">
        <h1 id = "post-title">Hadoop with Multi-Tiered Read-Write S3 cache using Alluxio and NetApp StorageGRID</h1>

  
  <div class = "post-info">
    <span>
        <i class="far fa-calendar" aria-hidden="true"></i> <span>16 Dec 2021</span> - <i class="far fa-clock"></i> 


  
  
    14 minute read
  

    </span>
  </div>
  
        <!-- TOC -->

<ul>
  <li><a href="#introduction">Introduction</a></li>
  <li><a href="#setup">Setup</a>
    <ul>
      <li><a href="#underlay-storage-s3">Underlay Storage (S3)</a></li>
      <li><a href="#multi-tiered-s3-cache">Multi-tiered S3 Cache</a></li>
      <li><a href="#hadoop-mapreduce-compute-layer-configuration">Hadoop (MapReduce) Compute Layer Configuration</a></li>
    </ul>
  </li>
  <li><a href="#results">Results</a>
    <ul>
      <li><a href="#hot-read-cache-including-pinned-and-pre-loaded-content">Hot Read Cache (including pinned and pre-loaded content)</a></li>
      <li><a href="#cold-cache-including-freed-and-evicted">Cold Cache (including free’d and evicted)</a></li>
      <li><a href="#observations-related-to-ssd-cache">Observations Related to SSD Cache</a></li>
    </ul>
  </li>
  <li><a href="#benefits-and-conclusion">Benefits and Conclusion</a></li>
  <li><a href="#demo">Demo</a></li>
</ul>

<!-- /TOC -->

<p><strong>NOTICE</strong>: all credentials and tokens on this page are samples, not leaked.</p>

<h2 id="introduction">Introduction</h2>

<p>If you are interested in “problem statement” and details related to each approach, you can refer to these:</p>

<ul>
  <li><a href="/2021/11/12/alluxio-storagegrid-s3.html">Alluxio and StorageGRID</a> - in-RAM cache for StorageGRID S3</li>
  <li><a href="/2021/11/21/alluxio-ontap.html">Alluxio and ONTAP NFS</a> - in-RAM cache for ONTAP NFS</li>
</ul>

<p>The purpose of this post is to show how both can be used together in the Hadoop compute (MapReduce) context.</p>

<p>Note that we will <em>not</em> be using two back-end storages (NFS and S3); we’ll use just one - StorageGRID (S3) - but we’ll set up Alluxio to use a <em>two-tiered</em> cache: in-RAM and NFS. With this approach we can take advantage of SSD-based NFS shares (or block storage devices, as ONTAP supports multiple block protocols) to augment RAM-based Alluxio cache.</p>

<p>So while ONTAP NFS will be used, it won’t be used as underlay file system (for that, see the post above). It’s not that it can’t be used at the same time - it can, and nested Alluxio mounts are possible - but that we chose to make our cache larger than the amount of RAM we have at our disposal.</p>

<h2 id="setup">Setup</h2>

<ul>
  <li>Local
    <ul>
      <li>Ubuntu 20.04 LTS
        <ul>
          <li>Alluxio 2.7.1</li>
          <li>Hadoop 3.3.0</li>
        </ul>
      </li>
      <li>ONTAP 9.10.1 (NFS v4)</li>
    </ul>
  </li>
  <li>Remote
    <ul>
      <li>NetApp StorageGRID 11.5</li>
    </ul>
  </li>
</ul>

<p>The following screenshots can be opened in new tab or separate window for better clarity.</p>

<p>vSphere testbed with Hadoop and ONTAP:</p>

<p><img src="/assets/images/alluxio-hadoop-multitier-01-vmware.png" alt="Compute-side resources" /></p>

<p>ONTAP NFS share:</p>

<p><img src="/assets/images/alluxio-hadoop-multitier-02-ontap.png" alt="ONTAP Select NFS v4 share" /></p>

<p>Remote StorageGRID bucket used as sole underlay:</p>

<p><img src="/assets/images/alluxio-hadoop-multitier-03-storagegrid.png" alt="StorageGRID S3 underlay" /></p>

<p>In-memory tier (can be sized differently on each worker node):</p>

<p><img src="/assets/images/alluxio-hadoop-multitier-04-alluxio-mem-tier.png" alt="Alluxio MEM caching tier" /></p>

<p>Alluxio on-disk tier (ONTAP NFS, framed blue at the bottom of this screenshot):</p>

<p><img src="/assets/images/alluxio-hadoop-multitier-04-alluxio-nfs-tier.png" alt="Alluxio NFS caching tier" /></p>

<p>Alluxio overview (Web UI running on singleton master/worker node): we can see faster tiers (here MEM) are prioritized and therefore fuller.</p>

<p><img src="/assets/images/alluxio-hadoop-multitier-05-alluxio-overview.png" alt="Alluxio overview" /></p>

<p>Cached objects: see the green rectangle, not pinned, as we just let them load and get evicted naturally based on the default settings. The red rectangle shows that the default Alluxio block size can be wasteful for tiny objects. Normally you wouldn’t have tiny objects in Hadoop but if you did, you could adjust block size downwards, so it’s not a problem.</p>

<p><img src="/assets/images/alluxio-hadoop-multitier-06-alluxio-in-memory.png" alt="Alluxio cache" /></p>

<p>In-Alluxio cache objects can be browsed and files/objects downloaded. Notice how there’s just one Location in the screenshot because (a) there <em>is</em> just one location (one worker) and (b) I didn’t instruct Alluxio to make two (or more) copies, which I could have done (if I had more workers) for even faster cache sharing <em>within</em> Alluxio cluster.</p>

<p><img src="/assets/images/alluxio-hadoop-multitier-07-alluxio-browser.png" alt="Browseable cache" /></p>

<p>Alluxio MountTable:</p>

<p><img src="/assets/images/alluxio-hadoop-multitier-08-alluxio-mounttable.png" alt="StorageGRID bucket in Alluxio MounTable" /></p>

<p>I’ll skip the configuration details of configuration files because I had them in the previous Alluxio-related posts, and also Hadoop and Java in general are a PITA and depending on your version of OS, or Java, or Alluxio, you’d have to adjust them anyway. (Did I mention that I don’t like Java?)</p>

<p>Alluxio is latest version released and while Hadoop 3.3.1 is available, Alluxio’s recommended Hadoop version (with Alluxio 2.7.1) is Hadoop 3.3.0, so that’s what I used. If you use a newer version of Alluxio, just check their docs (Hadoop compute integration section) to see what version is safest to use.</p>

<p>On the ONTAP side, we could use iSCSI or NVMe/TCP or other block storage protocol, but NFS is easier to manage and we can create NFS shares that can be shared among multiple compute nodes (e.g. <code class="language-plaintext highlighter-rouge">ipv4:/share/node[1,2,3,4]-cache</code>) which is certainly easier than creating one or more dedicated block device(s) for each worker node.</p>

<h3 id="underlay-storage-s3">Underlay Storage (S3)</h3>

<p>We create a bucket (<code class="language-plaintext highlighter-rouge">hahdupe</code>) on a StorageGRID system that’s 10,000 km away and mount it from Alluxio overlay at the root:</p>

<p><code class="language-plaintext highlighter-rouge">/ =&gt; s3://hahdupe</code></p>

<p>Next, we set up our caching devices.</p>

<h3 id="multi-tiered-s3-cache">Multi-tiered S3 Cache</h3>

<p>By default, and I used that approach in the StorageGRID post, you have an underlay storage such as S3, in-worker RAM is used for caching. But you can also set it up so that Alluxio uses other devices - ONTAP NFS shares, EF600 IB block devices, etc. Note that we could have a multi-device SSD pool as well (example: <code class="language-plaintext highlighter-rouge">/share1/worker1cache</code>, <code class="language-plaintext highlighter-rouge">/share2/worker1cache</code> configured for <code class="language-plaintext highlighter-rouge">worker1</code> at the same time).</p>

<p>We can guess what <code class="language-plaintext highlighter-rouge">MEM</code> (below)  means. Alluxio calls the next fast tier <code class="language-plaintext highlighter-rouge">SSD</code>, and the one below that would be <code class="language-plaintext highlighter-rouge">HDD</code>. Faster cache is preferred so Used Capacity on MEM tier is more, everything else (such as cache tiers’ capacity, in this case) being the same.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Alluxio cluster summary: 
    Master Address: localhost:19998
    Web Port: 19999
    Rpc Port: 19998
    Started: 12-15-2021 08:59:33:596
    Uptime: 0 day(s), 18 hour(s), 18 minute(s), and 50 second(s)
    Version: 2.7.1
    Safe Mode: false
    Zookeeper Enabled: false
    Live Workers: 1
    Lost Workers: 0
    Total Capacity: 2048.00MB
        Tier: MEM  Size: 1024.00MB
        Tier: SSD  Size: 1024.00MB
    Used Capacity: 1119.98MB
        Tier: MEM  Size: 799.98MB
        Tier: SSD  Size: 320.00MB
    Free Capacity: 928.02MB
</code></pre></div></div>

<p>What we see above is one Alluxio worker with two-tiered Alluxio cache (RAM and SSD-based NFS).</p>

<p>By default Alluxio fills up faster cache first and it has a gazillion knobs and switches that influence how it behaves, in the case you need to override default behavior.</p>

<h3 id="hadoop-mapreduce-compute-layer-configuration">Hadoop (MapReduce) Compute Layer Configuration</h3>

<p>Alluxio (RTFM) makes it very easy to integrate Hadoop. You’d just tell Hadoop to use Alluxio. In site properties:</p>

<div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;configuration&gt;</span>
  <span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>fs.alluxio.impl<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>alluxio.hadoop.FileSystem<span class="nt">&lt;/value&gt;</span>
    <span class="nt">&lt;description&gt;</span>The Alluxio FileSystem<span class="nt">&lt;/description&gt;</span>
  <span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;/configuration&gt;</span>
</code></pre></div></div>

<h2 id="results">Results</h2>

<h3 id="hot-read-cache-including-pinned-and-pre-loaded-content">Hot Read Cache (including pinned and pre-loaded content)</h3>

<p>Now we can run MapReduce jobs that have Alluxio overlay as both their source and target:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>./bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.0.jar wordcount <span class="se">\</span>
  <span class="nt">-libjars</span> <span class="nv">$ALLUXIO_HOME</span>/client/alluxio-2.7.1-client.jar <span class="se">\</span>
  alluxio://localhost:19998/people_data01.csv <span class="se">\</span>
  alluxio://localhost:19998/wordcount/output/pd1
</code></pre></div></div>

<p>That’s it!</p>

<pre><code class="language-raw">...
2021-12-15 09:21:04,516 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1274528484_0001_r_000000_0' to alluxio://localhost:19998/wordcount/output/pd1
2021-12-15 09:21:04,516 INFO mapred.LocalJobRunner: reduce &gt; reduce
2021-12-15 09:21:04,517 INFO mapred.Task: Task 'attempt_local1274528484_0001_r_000000_0' done.
2021-12-15 09:21:04,517 INFO mapred.Task: Final Counters for attempt_local1274528484_0001_r_000000_0: Counters: 29
        File System Counters
                ALLUXIO: Number of bytes read=22904321
                ALLUXIO: Number of bytes written=20252426
                ALLUXIO: Number of read operations=10
                ALLUXIO: Number of large read operations=0
                ALLUXIO: Number of write operations=3
                FILE: Number of bytes read=44673716
                FILE: Number of bytes written=45291627
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
        Map-Reduce Framework
                Combine input records=0
                Combine output records=0
                Reduce input groups=487339
                Reduce shuffle bytes=22196121
                Reduce input records=487339
                Reduce output records=487339
                Spilled Records=487339
                Shuffled Maps =1
                Failed Shuffles=0
                Merged Map outputs=1
                GC time elapsed (ms)=3
                Total committed heap usage (bytes)=892338176
        Shuffle Errors
                BAD_ID=0
                CONNECTION=0
                IO_ERROR=0
                WRONG_LENGTH=0
                WRONG_MAP=0
                WRONG_REDUCE=0
        File Output Format Counters 
                Bytes Written=20252426
2021-12-15 09:21:04,522 INFO mapred.LocalJobRunner: Finishing task: attempt_local1274528484_0001_r_000000_0
2021-12-15 09:21:04,522 INFO mapred.LocalJobRunner: reduce task executor complete.
2021-12-15 09:21:04,801 INFO mapreduce.Job:  map 100% reduce 100%
2021-12-15 09:21:04,802 INFO mapreduce.Job: Job job_local1274528484_0001 completed successfully
2021-12-15 09:21:04,807 INFO mapreduce.Job: Counters: 35
        File System Counters
                ALLUXIO: Number of bytes read=45808642
                ALLUXIO: Number of bytes written=20252426
                ALLUXIO: Number of read operations=15
                ALLUXIO: Number of large read operations=0
                ALLUXIO: Number of write operations=4
                FILE: Number of bytes read=44955158
                FILE: Number of bytes written=68387133
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
        Map-Reduce Framework
                Map input records=200001
                Map output records=1066379
                Map output bytes=27069836
                Map output materialized bytes=22196121
                Input split bytes=108
                Combine input records=1066379
                Combine output records=487339
                Reduce input groups=487339
                Reduce shuffle bytes=22196121
                Reduce input records=487339
                Reduce output records=487339
                Spilled Records=974678
                Shuffled Maps =1
                Failed Shuffles=0
                Merged Map outputs=1
                GC time elapsed (ms)=110
                Total committed heap usage (bytes)=1680343040
        Shuffle Errors
                BAD_ID=0
                CONNECTION=0
                IO_ERROR=0
                WRONG_LENGTH=0
                WRONG_MAP=0
                WRONG_REDUCE=0
        File Input Format Counters 
                Bytes Read=22904321
        File Output Format Counters 
                Bytes Written=20252426
</code></pre>

<h3 id="cold-cache-including-freed-and-evicted">Cold Cache (including free’d and evicted)</h3>

<p>Let’s see what happens when Alluxio starts with an empty cache. Doing a wordcount on a non-cached input file (22 MB) which takes 45 or so seconds to download from StorageGRID in my case. These details vary and change the equation for every situation, so while this doesn’t apply to any other use case, I need to describe my situation.</p>

<p>First (un-cached) MapReduce run took 55s. For a comparison, regular S3 client takes 45-47s to download this file.</p>

<ul>
  <li>04:05:46: wordcount job started</li>
  <li>04:05:48: map 0%, reduce 0%</li>
  <li>04:06:30: map 55%, reduce 0%</li>
  <li>04:06:38: starting flush of map output</li>
  <li>04:06:39: map task complete</li>
  <li>04:06:41: task done</li>
</ul>

<p>Log:</p>

<pre><code class="language-raw">...
2021-12-17 04:05:46,166 INFO hadoop.AbstractFileSystem: Creating Alluxio configuration from Hadoop configuration {}, uri configuration {alluxio.zookeeper.address=null, alluxio.zookeeper.enabled=false, alluxio.master.hostname=localhost, alluxio.master.rpc.addresses=null, alluxio.master.embedded.journal.addresses=null, alluxio.master.rpc.port=19998}
2021-12-17 04:05:46,246 INFO hadoop.AbstractFileSystem: Initializing filesystem with connect details localhost:19998
2021-12-17 04:05:46,320 INFO metrics.MetricsSystem: Starting sinks with config: {}.
...
2021-12-17 04:05:48,046 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2021-12-17 04:05:48,938 INFO mapreduce.Job: Job job_local1773778307_0001 running in uber mode : false
2021-12-17 04:05:48,938 INFO mapreduce.Job:  map 0% reduce 0%
...
2021-12-17 04:06:36,025 INFO mapred.LocalJobRunner: map &gt; map
2021-12-17 04:06:36,953 INFO mapreduce.Job:  map 58% reduce 0%
2021-12-17 04:06:38,315 INFO mapred.LocalJobRunner: map &gt; map
2021-12-17 04:06:38,315 INFO mapred.MapTask: Starting flush of map output
2021-12-17 04:06:38,315 INFO mapred.MapTask: Spilling map output
...
2021-12-17 04:06:39,834 INFO mapred.LocalJobRunner: map
2021-12-17 04:06:39,836 INFO mapred.Task: Task 'attempt_local1773778307_0001_m_000000_0' done.
...
2021-12-17 04:06:39,917 INFO mapred.Merger: Merging 1 sorted segments
2021-12-17 04:06:39,917 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 22189014 bytes
2021-12-17 04:06:39,954 INFO mapreduce.Job:  map 100% reduce 0%
...
2021-12-17 04:06:41,027 INFO mapred.LocalJobRunner: reduce task executor complete.
2021-12-17 04:06:41,955 INFO mapreduce.Job:  map 100% reduce 100%
2021-12-17 04:06:41,955 INFO mapreduce.Job: Job job_local1773778307_0001 completed successfully
2021-12-17 04:06:41,961 INFO mapreduce.Job: Counters: 35
</code></pre>

<p>Observations related to this run with a cold read cache:</p>

<ul>
  <li>The Map phase makes progress as data is being downloaded - there’s no need to wait for download to complete. But because this phase normally (local data or hot Alluxio cache) takes just 2-3 seconds, it doesn’t seem to help us. In some other situations, though, that would make a big difference (imagine if it took 3 hours to download and 3 hours to Map, and if you could have both done in 3 hours and 5 minutes)</li>
  <li>If the result is output to S3 it is by default transparently cached (this is adjustable) and uploaded in the background, so there’s no need to upload it separately or rely on other tools and workflows</li>
  <li>There’s nothing that prevents you from doing one-off adjustments that match different requirements; for example you can pre-load the file(s) before you run jobs, or let Alluxio load on demand (as in this example). You can also write-through, and then evict results from the cache if you no longer have use for them - you managed to avoid writing them 3x to HDFS - good on you!</li>
</ul>

<p>If you want to see how this works in real life, see the third video at the bottom of this post (the first two videos don’t have that part).</p>

<p>The next run (with hot cache) using the same input file was in line with what we saw earlier with other cached files (5s for a 22MB file on this 2 vCPU worker):</p>

<ul>
  <li>04:06:52: Alluxio-proxied Hadoop starts wordcount job</li>
  <li>04:06:54: map 0%, reduce 0%</li>
  <li>04:06:56: map 100%, reduce 0%</li>
  <li>04:06:57: map 100%, reduce 100%</li>
  <li>04:06:57: task done</li>
</ul>

<h3 id="observations-related-to-ssd-cache">Observations Related to SSD Cache</h3>

<p>One caching feature that is a bit harder to demonstrate is how cached blocks go up and down the tiers (MEM and SSD in this post).</p>

<p>When you evict a file from Alluxio, it doesn’t seem to go to SSD or HDD tier it’s simply gone, so we can’t demonstrate the use of SSD and HDD tiers that way.</p>

<p>I didn’t want to try harder because there’s not much point in doing that: we saw that SSD tier does get utilized, and we know the idea is to maximize the use of faster tiers, so as long as that tier isn’t full Alluxio will prefer to populate it.</p>

<p>We can see in one of the screenshots and various CLI output samples above that SSD tier does get used when necessary, that thin provisioning makes it save empty space in less-than-full 64MB (by default) Alluxio blocks, and that slower than RAM but much faster than a remote non-cached S3.</p>

<p>It is very likely that multiple NFS shares would be better for the performance of SSD tier used with Alluxio, but because I didn’t have any issues with performance in the simple tests I performed, I can’t say at what point and with what workload that would become helpful.</p>

<h2 id="benefits-and-conclusion">Benefits and Conclusion</h2>

<p>In the video you will see that just reading a very remote 20 MB S3 object takes tens of seconds, and writes aren’t any faster.</p>

<p>Alluxio writes to remote (underlay) layer transparently and quickly - the result of job above was written from in-Alluxio cache to StorageGRID.</p>

<p>Alluxio must download objects (we can’t avoid that if objects are uploaded from other locations, bypassing Alluxio cache), but it downloads them faster and it’s transparent to the user and application. We don’t have to download data until we need it and we also don’t need to know what we need. And - given cache that’s big enough for working set - we can avoid waiting for local storage both when we write and read.</p>

<p>In summary:</p>

<ul>
  <li>Alluxio needs one read per object, multiple in-RAM or on-disk copies may be created if necessary; we can flexibly decide how much copies. While shared filesystems like Hadoop also need one read, there may be less flexibility in terms of controlling its behavior and writes usually land on NL-SAS HDDs, thrice</li>
  <li>Data can be pinned in advance (e.g. for scheduled analytics jobs this afternoon, pin IoT files generated by 10am this morning (<code class="language-plaintext highlighter-rouge">s3://hahdupe/iot*2021-12-16-0*.csv</code>)) or otherwise fetched before jobs run</li>
  <li>You don’t have to manually manage local cache, neither read nor write. That includes deciding where and when to save data, and remembering to delete it</li>
  <li>You can cache writes as well, if you want/need to do that. If you don’t, you can write-through and bypass Alluxio</li>
  <li>You can take advantage of not just RAM, but also fast nearby storage tiers that don’t consume 3x the capacity and anything they do consume can be thin-provisioned</li>
  <li>Data is persisted (or not - you can fine tune this behavior, too) to S3 while your compute and storage resources (as well as any software licenses or subscriptions) are better utilized</li>
  <li>Alluxio supports various underlays with various compute integrations at the same time. This allows you to use the same underlay (StorageGRID S3 storage) with different compute integrations at the same time. S3 overlay from this post can be used outside of Hadoop - from the shell, from Python scripts and in various other ways (see previous Alluxio-StorageGRID post) - it’s not an island!</li>
</ul>

<p>There’s more, but you get the idea.</p>

<p>As a result, Hadoop and other analytics applications can make full use of S3 in ways that optimize both compute and data management.</p>

<p>As far StorageGRID is concerned, this approach also lets customers run on-demand analytics jobs in the public cloud: as an example, we could deploy Alluxio in hyperscaler compute nodes and use in-worker RAM for the <code class="language-plaintext highlighter-rouge">MEM</code> tier, and NetApp CVO or CVS as <code class="language-plaintext highlighter-rouge">SSD</code> tier for cloud-based StorageGRID cache. That would give us the same architecture and same benefits when using StorageGRID from the public cloud.</p>

<h2 id="demo">Demo</h2>

<ul>
  <li>Shorter version: <a href="https://youtu.be/EiU0cLzD-2U">Multi-tiered read-write cache for Hadoop with Alluxio and StorageGRID</a> - 2m06s
    <ul>
      <li>Longer version: <a href="https://youtu.be/riRuotCOb9c">Multi-tiered read-write cache for Hadoop with Alluxio and StorageGRID</a> - 4m32s</li>
    </ul>
  </li>
  <li>Additional segment about loading data to Alluxio cache by preloading or on-demand: <a href="https://youtu.be/w9wW416s1Po">Starting with Cold Cache</a> - 2m47s</li>
</ul>

      </article>
    </div>
    
      <div class="categories">
    <span><p>Categories:</p>
    
    
      <a href="
      /categories/#analytics">analytics</a>
      &nbsp; 
    
      <a href="
      /categories/#storage">storage</a>
       
    
  </span>
</div>
    

  
    <div>
      <h3>Related Posts</h3>
      <ul>
      
        <li><a href="/2021/11/21/alluxio-ontap.html">Alluxio and ONTAP NFS</a></li>
      
        <li><a href="/2021/11/12/alluxio-storagegrid-s3.html">Alluxio and StorageGRID</a></li>
      
        <li><a href="/2023/11/30/elasticsearch-ilm-netapp-eseries.html">Snapshots and ILM with Elasticsearch 8</a></li>
      
        <li><a href="/2024/07/13/velero-aws-plugin-s3-signature-does-not-match-nonsense.html">Velero AWS Plugin and SignatureDoesNotMatch nonsense</a></li>
      
        <li><a href="/2023/07/28/thanos-with-minio-eseries-or-ontap-s3.html">Thanos with different S3 backends - StorageGRID, ONTAP S3 and MinIO on E-Series</a></li>
      
      </ul>
    </div>
  

    
  </div><footer class= "footer">
    <p>2025-05-18 05:26 </p>
    <p>Copyright © 2025 scaleoutSean. Content is released under the CC BY license. Design: Alessio Franceschi</p>
</footer>

</body>
</html>
