<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.27.3 by Michael Rose
  Copyright 2013-2025 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Kubernetes failover and failback with Trident CSI and SolidFire | Acting Technologist</title>
<meta name="description" content="Use Trident CSI and SolidFire to protect data and provide failover and failback to Kubernetes clusters">


  <meta name="author" content="scaleoutSean">
  
  <meta property="article:author" content="scaleoutSean">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Acting Technologist">
<meta property="og:title" content="Kubernetes failover and failback with Trident CSI and SolidFire">
<meta property="og:url" content="https://scaleoutsean.github.io/2021/03/20/kubernetes-solidfire-failover-failback.html">


  <meta property="og:description" content="Use Trident CSI and SolidFire to protect data and provide failover and failback to Kubernetes clusters">





  <meta name="twitter:site" content="@scaleoutSean">
  <meta name="twitter:title" content="Kubernetes failover and failback with Trident CSI and SolidFire">
  <meta name="twitter:description" content="Use Trident CSI and SolidFire to protect data and provide failover and failback to Kubernetes clusters">
  <meta name="twitter:url" content="https://scaleoutsean.github.io/2021/03/20/kubernetes-solidfire-failover-failback.html">

  
    <meta name="twitter:card" content="summary">
    
  

  



  <meta property="article:published_time" content="2021-03-20T00:00:00+08:00">






<link rel="canonical" href="https://scaleoutsean.github.io/2021/03/20/kubernetes-solidfire-failover-failback.html">







  <meta name="google-site-verification" content="F6q7vIwQ2G0j8tk-KL9rAXOMLXMDMMUkEz4fRs1Nnnc" />


  <meta name="msvalidate.01" content="7cf5b7d96a77410a8ad035f764dc81b3">





<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Acting Technologist Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  window.enable_copy_code_button = true;
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <link href="https://fonts.googleapis.com/css?family=PT+Sans+Narrow:400,700|PT+Serif:400,700,400italic" rel="stylesheet" type="text/css">


  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
          <a class="site-logo" href="/"><img src="/assets/images/scaleoutsean-acting-technologist.png" alt="Acting Technologist"></a>
        
        <a class="site-title" href="/">
          Acting Technologist
          <span class="site-subtitle">human action through technology</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/archive/"
                
                
              >Archive</a>
            </li><li class="masthead__menu-item">
              <a
                href="/categories/"
                
                
              >Categories</a>
            </li><li class="masthead__menu-item">
              <a
                href="/tags/"
                
                
              >Tags</a>
            </li><li class="masthead__menu-item">
              <a
                href="/projects/"
                
                
              >Projects</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">

  

  <div class="author__content">
    <h3 class="author__name p-name" itemprop="name">
      <a class="u-url" rel="me" href="https://scaleoutsean.github.io/" itemprop="url">scaleoutSean</a>
    </h3>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Kubernetes failover and failback with Trident CSI and SolidFire">
    <meta itemprop="description" content="  UPDATE          Summary      Introduction                  Comparison between PV Patching and Trident Volume Import approach          Why only Scenario 1                    Failover (PROD=&gt;DR)                  Steps to failover a SolidFire cluster                          Configure Kubernetes and Trident CSI for the SolidFire cluster PROD              Notes about restoring workloads on the remote SolidFire cluster                                Steps to configure SolidFire volume replication in the opposite direction                    Failback (DR=&gt;PROD)                  Steps for a SolidFire cluster failback                          Notes on failback                                          Conclusion      Appendix                  SolidFire replication and switch-over under the hood          SolidFire Volume Names and IDs          Fail PROD cluster          Configure replication          Restore read-write access to target replica volumes          Why use Retain for replicated volumes          SolidFire replication and Kubernetes Snapshot Volume Class          Dealing with different types of storage cluster failures          Conduct failover and failback testing          Automate storage failover steps          Video demo                    ">
    <meta itemprop="datePublished" content="2021-03-20T00:00:00+08:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://scaleoutsean.github.io/2021/03/20/kubernetes-solidfire-failover-failback.html" itemprop="url">Kubernetes failover and failback with Trident CSI and SolidFire
</a>
          </h1>
          

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2021-03-20T00:00:00+08:00">2021-03-20 00:00</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          36 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
        <ul>
  <li><a href="#update">UPDATE</a>
    <ul>
      <li><a href="#summary">Summary</a></li>
      <li><a href="#introduction">Introduction</a>
        <ul>
          <li><a href="#comparison-between-pv-patching-and-trident-volume-import-approach">Comparison between PV Patching and Trident Volume Import approach</a></li>
          <li><a href="#why-only-scenario-1">Why only Scenario 1</a></li>
        </ul>
      </li>
      <li><a href="#failover-proddr">Failover (PROD=&gt;DR)</a>
        <ul>
          <li><a href="#steps-to-failover-a-solidfire-cluster">Steps to failover a SolidFire cluster</a>
            <ul>
              <li><a href="#configure-kubernetes-and-trident-csi-for-the-solidfire-cluster-prod">Configure Kubernetes and Trident CSI for the SolidFire cluster PROD</a></li>
              <li><a href="#notes-about-restoring-workloads-on-the-remote-solidfire-cluster">Notes about restoring workloads on the remote SolidFire cluster</a></li>
            </ul>
          </li>
          <li><a href="#steps-to-configure-solidfire-volume-replication-in-the-opposite-direction">Steps to configure SolidFire volume replication in the opposite direction</a></li>
        </ul>
      </li>
      <li><a href="#failback-drprod">Failback (DR=&gt;PROD)</a>
        <ul>
          <li><a href="#steps-for-a-solidfire-cluster-failback">Steps for a SolidFire cluster failback</a>
            <ul>
              <li><a href="#notes-on-failback">Notes on failback</a></li>
            </ul>
          </li>
        </ul>
      </li>
      <li><a href="#conclusion">Conclusion</a></li>
      <li><a href="#appendix">Appendix</a>
        <ul>
          <li><a href="#solidfire-replication-and-switch-over-under-the-hood">SolidFire replication and switch-over under the hood</a></li>
          <li><a href="#solidfire-volume-names-and-ids">SolidFire Volume Names and IDs</a></li>
          <li><a href="#fail-prod-cluster">Fail PROD cluster</a></li>
          <li><a href="#configure-replication">Configure replication</a></li>
          <li><a href="#restore-read-write-access-to-target-replica-volumes">Restore read-write access to target replica volumes</a></li>
          <li><a href="#why-use-retain-for-replicated-volumes">Why use Retain for replicated volumes</a></li>
          <li><a href="#solidfire-replication-and-kubernetes-snapshot-volume-class">SolidFire replication and Kubernetes Snapshot Volume Class</a></li>
          <li><a href="#dealing-with-different-types-of-storage-cluster-failures">Dealing with different types of storage cluster failures</a></li>
          <li><a href="#conduct-failover-and-failback-testing">Conduct failover and failback testing</a></li>
          <li><a href="#automate-storage-failover-steps">Automate storage failover steps</a></li>
          <li><a href="#video-demo">Video demo</a></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h1 id="update">UPDATE</h1>

<p>I notice people visit this post often. Please note:</p>

<ul>
  <li>This scenario is for a single Kubernetes/Trident CSI instance with two SolidFire clusters which is <strong>NOT</strong> recommended (this posts explains why)</li>
  <li><a href="/2024/07/05/kubefire-for-failover-failback-of-kubernetes-with-solidfire-backend.html">Kubefire - tools and recipes for Kubernetes failover/failback with SolidFire</a> introduces the recommended approach and links to a new Github repository which contains an additional scenario (two sites with Kubernetes/Trident/SolidFire at each) and will feature a complete workflow with scripts</li>
  <li>Additional background can be found in the post <a href="/2024/06/01/pvc-volume-relationships-in-solidfire-trident-part-1.html">Kubernetes, Trident and SolidFire configuration visibility</a> and other posts in that series. There's a Kubernetes-to-Trident-to-SolidFire mapping script in that post, there's a <a href="/2024/06/11/introducing-project-longhorny.html">SolidFire replication management CLI</a> and SolidFire replication monitoring is now available in <a href="/2024/06/15/sfc-adds-volume-replication-monitoring.html">SFC v2</a>. It's all permissively licensed and you can reuse and improve all of them, so make sure to check the recent posts for the latest and greatest in this area.</li>
</ul>

<h2 id="summary">Summary</h2>

<p>This post describes my preferred personal workflow for setting up SolidFire storage cluster replication, failover, and failback in a single-cluster Kubernetes environment using NetApp Trident v21.01.</p>

<p>The process is simple and works similar to SolidFire failover for physical servers: we use SolidFire storage replication to replicate data (sync, or async, or async with snapshots), and the Trident Volume Import feature to access volumes on the remote storage array.</p>

<p>Due to the relative scarcity of related information (including the official Trident documentation) this post is long and touches upon most relevant details architects and solution designers need to consider when planning and executing storage replication, failover and failback in a single-cluster Kubernetes environment.</p>

<p>In an attempt to provide additional details while keeping them separate from core content, that information can be found in Appendix.</p>

<p>If you'd just like to skim through a video, I recorded two versions:</p>

<ul>
  <li><a href="https://youtu.be/aSFxlGoHgdA">fast &amp; simple version</a> (2m56s) - uses PostgreSQL and all CLI commands are executed from scripts to make the video shorter)</li>
  <li>long version (10m55s) - focus on the details of Trident v21.01 behavior, whys and hows (I don't recommend it, but the link is provided at the bottom)</li>
</ul>

<h2 id="introduction">Introduction</h2>

<p>Previously I created two videos related to the topic of SolidFire replication: one about <a href="https://youtu.be/LdKBYJhvwrU">configuring SolidFire array &amp; volume replication using SolidFire PowerShell Tools</a> in which I demonstrated how SolidFire replication can be configured in mere seconds, and another about manual <a href="https://www.youtube.com/watch?v=f-PJGCtEojQ">failover and failback of Kubernetes with SolidFire</a> which used an old approach with patching of Kubernetes PV objects.</p>

<p>The Trident documentation v21.01.01 doesn't have much on the topic of Kubernetes failover with SolidFire storage, so I've been wanting revisit this topic.</p>

<p>There are several scenarios one could use to protect Kubernetes data on SolidFire across sites. I'd expect 90% of SolidFire users use these three:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Scenario</th>
      <th style="text-align: center">K8s clusters</th>
      <th style="text-align: center">SF clusters</th>
      <th style="text-align: center">Protects from failure of</th>
      <th style="text-align: center">Switch-over required</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">1</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">2</td>
      <td style="text-align: center">SF cluster</td>
      <td style="text-align: center">SF</td>
    </tr>
    <tr>
      <td style="text-align: center">2</td>
      <td style="text-align: center">2</td>
      <td style="text-align: center">2</td>
      <td style="text-align: center">K8s or SF cluster</td>
      <td style="text-align: center">Both K8s &amp; SF</td>
    </tr>
    <tr>
      <td style="text-align: center">3</td>
      <td style="text-align: center">1,3</td>
      <td style="text-align: center">1,3</td>
      <td style="text-align: center">AZ</td>
      <td style="text-align: center">None</td>
    </tr>
  </tbody>
</table>

<p>In <strong>Scenario 1</strong> we assume SolidFire replication is in place. There is only one Kubernetes cluster, so we just need to make Kubernetes to switch to the backup storage backend.</p>

<p>In <strong>Scenario 2</strong> there's one Kubernetes and one SolidFire cluster <em>at each site</em>. If either Kubernetes or SolidFire cluster fails, we switch both Kubernetes and SolidFire to the surviving site.</p>

<p>The difference compared to Scenario 1 is the remote site is likely to have a Kubernetes cluster that is unaware of the existence of the PROD site. As soon as we promote the remote SolidFire cluster to Read/Write, replica volumes at the remote SolidFire can come online for read-write access by the remote Kubernetes cluster. Note that the remote Kubernetes cluster could also have a configuration fully or partially imported from the cluster at the production site which would make that situation similar to Scenario 1: technically it'd be a separate cluster, but logically it could be almost like the same cluster, depending on the extent of similarities in configuration (in terms of storage we obviously think about PVCs).</p>

<p><strong>Scenario 3</strong> is about more complex combinations. One of them (single Kubernetes cluster, single SolidFire cluster, spread across three sites connected by a low-latency L2 network) involves SolidFire Protection Domains (PDs). What do PDs do?</p>

<p>They're similar to Availability Zones in Public Coud. A SolidFire cluster with six or more nodes can be logically segregated into three Protection Domains, so that redundant copies of each data block that SolidFire makes to protect data from disk or node failures never land in the same Protection Domain (in this example that'd mean two copies would never be stored in the same Protection Domain).</p>

<p>If your Kuberetes cluster is spread spread over multiple buildings in a campus environment, you could deploy six or more SolidFire nodes to provide redundancy within each PD, but also protect data and services in the case one building loses power or network connectivity. This approach has a zero RTO and RPO because there is no SolidFire cluster failover involved.</p>

<p>PD failover is just a failover of a node or domain. Unlike Public Cloud AZs, SolidFire PDs don't and can't limit access to or even prefer local iSCSI clients: whether it's one or three Kubernetes clusters connected to a SolidFire v12 cluster with PDs, all workers need to access all PDs as long as SolidFire PDs are up and running. Should one PD fail, Kubernetes workers would find their iSCSI volumes available in one of the the remaining two PDs.</p>

<h3 id="comparison-between-pv-patching-and-trident-volume-import-approach">Comparison between PV Patching and Trident Volume Import approach</h3>

<p>To me these would be most significant differences (it's been a while since I looked at the PV patching/replacement approach so I'm not entirely sure about that column):</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Feature</th>
      <th style="text-align: center">PV Patching</th>
      <th style="text-align: center">Trident Volume Import</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Works with CSI-compatible Trident</td>
      <td style="text-align: center">x (?)</td>
      <td style="text-align: center">o</td>
    </tr>
    <tr>
      <td style="text-align: left">Static Kubernetes PV names</td>
      <td style="text-align: center">o</td>
      <td style="text-align: center">x</td>
    </tr>
    <tr>
      <td style="text-align: left">Same Kubernetes PV name across sites</td>
      <td style="text-align: center">o</td>
      <td style="text-align: center">x</td>
    </tr>
    <tr>
      <td style="text-align: left">Same SolidFire Volume name across sites</td>
      <td style="text-align: center">o</td>
      <td style="text-align: center">o</td>
    </tr>
    <tr>
      <td style="text-align: left">Relies on Trident CSI for volume import</td>
      <td style="text-align: center">x</td>
      <td style="text-align: center">o</td>
    </tr>
    <tr>
      <td style="text-align: left">Requires Trident backend changes</td>
      <td style="text-align: center">x</td>
      <td style="text-align: center">o</td>
    </tr>
    <tr>
      <td style="text-align: left">Easy integration with external tooling</td>
      <td style="text-align: center">o</td>
      <td style="text-align: center">o</td>
    </tr>
  </tbody>
</table>

<p>I suppose that those who like consistent and static PVC names in Kubernetes and SolidFire strongly prefer PV patching but I really don't know what's more or less important to whom, and I probably can't think of some other reasons why one or the other may be more suitable for you.</p>

<h3 id="why-only-scenario-1">Why only Scenario 1</h3>

<p><strong>This post considers only Scenario 1</strong> because Scenario 2 is either similar (when some or most of Kubernetes configuration from Source is applied on Destination) or easier (when Kubernetes clusters are completely independent). Scenario 3 requires no special considerations because there's no SolidFire site or cluster failover (and we can imagine that anyone with the main site running Scenario 3 would have another site which would make it Scenario 2).</p>

<p>High level steps for each phase of Scenario 1:</p>

<ul>
  <li>Failover:
    <ul>
      <li>SolidFire DR cluster: delete volume replication pairs to stop replication. Make SolidFire replica volumes Read/Write</li>
      <li>Kubernetes: delete the backend PROD to prevent its use by Trident that side comes up, then create a new Trident backend for the SolidFire DR cluster, use Trident import volume to import replicated volume and finally recreate PVC</li>
      <li>SolidFire PROD &amp; DR cluster:
        <ul>
          <li>SolidFire PROD cluster (once it becomes accessible): delete volume pairs, make replicated volumes replication targets</li>
          <li>SolidFire DR cluster: configure and start DR=&gt;PROD replication to prepare for failback</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Failback:
    <ul>
      <li>Kubernetes: stop workloads using the remote SolidFire cluster</li>
      <li>SolidFire DR cluster: stop and delete replication pairings</li>
      <li>SolidFire PROD cluster: delete replication pairings</li>
      <li>Kuberntes: reinstall Trident and recreate backend PROD, delete stale PVs, then import volumes from PROD and recreate PVCs</li>
    </ul>
  </li>
</ul>

<p>SolidFire users in Scenario 2 may be able to apply most, if not all, SolidFire-related steps and ignore Trident-related steps that involve backend changes or reinstallation (just <code class="language-plaintext highlighter-rouge">import volume</code>, managed or unmanaged, should be enough). I may write a post about Scenario 2 in the future.</p>

<h2 id="failover-proddr">Failover (PROD=&gt;DR)</h2>

<p>My setup:</p>

<ul>
  <li>One K8s cluster with Trident CSI v21.01.1 (just in the case you have a newer version, check its change log)</li>
  <li>Two SolidFire storage clusters paired for volume replication named PROD and DR (SolidFire version 12.2 and 11.7, respectively)</li>
  <li>One (default) storage class configured to use <code class="language-plaintext highlighter-rouge">solidfire-san</code> configured in Kubernetes</li>
</ul>

<p>In order to use different storage clusters you must use two backends (why, see Appendix for details).</p>

<p>We start with a backend that uses PROD and to failover switch to a backend that uses the cluster "DR". To fail back we again set up replication, this time in the opposite direction.</p>

<p>SolidFire Volume Names and IDs in Failover and Failback belong to the same (successful) workflow, while names from Appendix are from different situations so don't try to "map" the stuff from Appendix to other steps or processes.</p>

<p>High level steps before site failover is attempted:</p>

<ul>
  <li>Prepare Kubernetes and two SolidFire clusters:
    <ul>
      <li>pair PROD &amp; DR clusters</li>
      <li>install Trident on your only Kubernetes cluster</li>
      <li>create Storage Class(es) for the SolidFire, NetApp HCI or eSDS back-end (<code class="language-plaintext highlighter-rouge">solidfire-san</code>) and with <code class="language-plaintext highlighter-rouge">reclaimPolicy: Retain</code></li>
      <li>create a storage account on each SolidFire cluster and ensure both backends work with Trident</li>
    </ul>
  </li>
  <li>Production site:
    <ul>
      <li>Trident: create a CHAP-based backend configuration file for "PROD"</li>
      <li>Kubernetes: create a PVC, identify resulting PV on SolidFire by its Volume ID</li>
    </ul>
  </li>
  <li>DR site:
    <ul>
      <li>SolidFire: Create a target volume with same properties (size, 512e emulation, storage account owner), set access mode to <code class="language-plaintext highlighter-rouge">replicationTarget</code></li>
      <li>SolidFire: Create a CHAP-based backend configuratino file for "DR"</li>
      <li>Kubernetes: make sure you have a copy of PVC or other YAML files with PVCs from the production site</li>
    </ul>
  </li>
  <li>Production site:
    <ul>
      <li>SolidFire: configure SolidFire volume pairs and replication (PROD=&gt;DR) for that PV (complete it by approving the step on the remote SolidFire cluster)</li>
    </ul>
  </li>
</ul>

<h3 id="steps-to-failover-a-solidfire-cluster">Steps to failover a SolidFire cluster</h3>

<ul>
  <li>Production site:
    <ul>
      <li>SolidFire: power off or reboot SolidFire array at the PROD site (to simulate unplanned site failure)</li>
    </ul>
  </li>
  <li>Production site:
    <ul>
      <li>SolidFire: delete volume replication (pairs) to stop replication, change replica volumes' mode from <code class="language-plaintext highlighter-rouge">replicationTarget</code> to <code class="language-plaintext highlighter-rouge">readWrite</code></li>
      <li>Trident: delete backend "PROD" (we'll create it again after we fail back), create the DR backend</li>
      <li>Kubernetes: delete stale PVC from the PROD site (we need to remove them in order to recreate them with <code class="language-plaintext highlighter-rouge">import volume</code>)</li>
      <li>Kubernetes: delete stale PV from the PROD site (this will delete it only from Kubernetes, not from the backend PROD because we removed it earlier)</li>
      <li>Trident: use <code class="language-plaintext highlighter-rouge">tridentctl import volume</code> to import replica PV and recreate PVC configuration</li>
    </ul>
  </li>
</ul>

<h4 id="configure-kubernetes-and-trident-csi-for-the-solidfire-cluster-prod">Configure Kubernetes and Trident CSI for the SolidFire cluster PROD</h4>

<p>Create backend for the production site.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>./trident-installer/tridentctl create backend <span class="nt">-n</span> trident <span class="nt">-f</span> be-prod.json 
+----------------------+----------------+--------------------------------------+--------+---------+
|         NAME         | STORAGE DRIVER |                 UUID                 | STATE  | VOLUMES |
+----------------------+----------------+--------------------------------------+--------+---------+
| SF-PROD-192.168.1.30 | solidfire-san  | 57883f01-94ba-48aa-8acd-e7330c128bce | online |       0 |
+----------------------+----------------+--------------------------------------+--------+---------+
</code></pre></div></div>

<p>We need a Storage Class which we'll use on both sites. Use <code class="language-plaintext highlighter-rouge">ReclaimPolicy: Retain</code> to prevent automatic deletion of PVs after PVC release.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl get sc
NAME              PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
basic <span class="o">(</span>default<span class="o">)</span>   csi.trident.netapp.io   Retain          Immediate           <span class="nb">true                   </span>4h20m

<span class="nv">$ </span>kubectl describe sc 
Name:                  basic
IsDefaultClass:        Yes
Annotations:           storageclass.kubernetes.io/is-default-class<span class="o">=</span><span class="nb">true
</span>Provisioner:           csi.trident.netapp.io
Parameters:            <span class="nv">IOPS</span><span class="o">=</span>300,backendType<span class="o">=</span>solidfire-san,clones<span class="o">=</span><span class="nb">true</span>,fsType<span class="o">=</span>ext4,snapshots<span class="o">=</span><span class="nb">true
</span>AllowVolumeExpansion:  True
MountOptions:          &lt;none&gt;
ReclaimPolicy:         Retain
VolumeBindingMode:     Immediate
Events:                &lt;none&gt;
</code></pre></div></div>

<p>This storage class is not site-specific so we can leave it in place and just manipulate backends. There may be other approaches (more complex, too).</p>

<p>Request a PV in the <code class="language-plaintext highlighter-rouge">pg</code> namespace:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">kind</span><span class="pi">:</span> <span class="s">PersistentVolumeClaim</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">pg</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">accessModes</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">ReadWriteOnce</span>
  <span class="na">resources</span><span class="pi">:</span>
    <span class="na">requests</span><span class="pi">:</span>
      <span class="na">storage</span><span class="pi">:</span> <span class="s">1Gi</span>
  <span class="na">storageClassName</span><span class="pi">:</span> <span class="s">basic</span>
</code></pre></div></div>

<p>Now we have a PVC, and a PV to protect. Maintain an off-site copy of PVC-related configuration files because you'll need to recreate PVCs on the remote site to failover.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl get pvc <span class="nt">-n</span> pg
NAME   STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
pg     Bound    pvc-fdd499f6-14f3-4280-bdef-35d84c5794bb   1Gi        RWO            basic          106m

<span class="nv">$ </span>kubectl get pv <span class="nt">-n</span> pg
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM   STORAGECLASS   REASON   AGE
pvc-fdd499f6-14f3-4280-bdef-35d84c5794bb   1Gi        RWO            Retain           Bound    pg/pg   basic                   106m
</code></pre></div></div>

<p>New PVC:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl describe pv pvc-fdd499f6-14f3-4280-bdef-35d84c5794bb <span class="nt">-n</span> pg
Name:            pvc-fdd499f6-14f3-4280-bdef-35d84c5794bb
...
Source:
    Type:              CSI <span class="o">(</span>a Container Storage Interface <span class="o">(</span>CSI<span class="o">)</span> volume <span class="nb">source</span><span class="o">)</span>
    Driver:            csi.trident.netapp.io
    FSType:            ext4
    VolumeHandle:      pvc-fdd499f6-14f3-4280-bdef-35d84c5794bb
    ReadOnly:          <span class="nb">false
    </span>VolumeAttributes:      <span class="nv">backendUUID</span><span class="o">=</span>57883f01-94ba-48aa-8acd-e7330c128bce
                           <span class="nv">internalName</span><span class="o">=</span>pvc-b4ad4998-05ce-46ab-9f67-a6e972805fa3
                           <span class="nv">name</span><span class="o">=</span>pvc-fdd499f6-14f3-4280-bdef-35d84c5794bb
...
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">internalName=pvc-b4ad4998-05ce-46ab-9f67-a6e972805fa3</code>: this is the SolidFire volume name. It is stored by Trident in SolidFire volume attributes at creation or (managed) import.</p>

<p><code class="language-plaintext highlighter-rouge">backendUUID</code> is Trident's internal, non-deterministic unique identifier for storage backends. Here it's <code class="language-plaintext highlighter-rouge">57883f01-94ba-48aa-8acd-e7330c128bce</code>, but if we recreated the same backend its backend UUID would be different.</p>

<p>Replicate this volume to SolidFire on the remote site (more on that in Appendix, if you wish to use the CLI or automate), wait until the pair is in sync, and then fail the PROD cluster to simulate a site failure. Repeat the procedure for multiple volumes if you have them.</p>

<h4 id="notes-about-restoring-workloads-on-the-remote-solidfire-cluster">Notes about restoring workloads on the remote SolidFire cluster</h4>

<p>Once you decide it's time to fail over, the first thing you do is delete volume pairing configuration used PROD=&gt;DR replication, while noting Volume IDs of volume pairs - we will have to replicate reverse once PROD comes back online!</p>

<p>You probably don't want to make such notes during failover. It's better to create a CSV, JSON or YAML file and keep it in an object storage bucket or private Github repository so that you can always access it from both sites. That table can be simple (example below) or sophisticated, but as long as it is correct and up to date, it will work well.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">ns/pod</th>
      <th style="text-align: center">src</th>
      <th style="text-align: center">dst</th>
      <th style="text-align: center">replicate</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">pg/pg</td>
      <td style="text-align: center">476</td>
      <td style="text-align: center">126</td>
      <td style="text-align: center">true</td>
    </tr>
    <tr>
      <td style="text-align: center">pg/logs</td>
      <td style="text-align: center">477</td>
      <td style="text-align: center">128</td>
      <td style="text-align: center">true</td>
    </tr>
    <tr>
      <td style="text-align: center">pg/bkp</td>
      <td style="text-align: center">478</td>
      <td style="text-align: center">130</td>
      <td style="text-align: center">true</td>
    </tr>
    <tr>
      <td style="text-align: center">pg/dmp</td>
      <td style="text-align: center">481</td>
      <td style="text-align: center">-</td>
      <td style="text-align: center">false</td>
    </tr>
  </tbody>
</table>

<p>As you create new PVCs - whether they require replication or not - update that table in the same go. When we pair volumes in either direction or import volumes, we pull/download the table and use its fields as inputs to Ansible, PowerShell, or some other tool. Have a similar procedure for when you delete PVCs.</p>

<p>Having deleted stale volume pairs on the remote SolidFire cluster we can promote all its replica volumes from <code class="language-plaintext highlighter-rouge">replicationTarget</code> to <code class="language-plaintext highlighter-rouge">readWrite</code> in order to make them writeable and turn them into Sources of replication. This can be easily automated but for a handful of volumes you could also use the SolidFire Web UI.</p>

<p>Then we delete PROD from Trident backends. This is necessary in order to make Trident forget about the failed SolidFire cluster. Because if PROD comes back online, a <code class="language-plaintext highlighter-rouge">kubectl delete pv</code> could accidentally purge a Source volume. And <code class="language-plaintext highlighter-rouge">kubectl create pvc</code> could create new volumes off-site - also not desirable. Virtual Storage Pools, topology-aware CSI and multiple Storage Classes may be able to help us do that smarter, but at the cost of more complex configuration and I haven't yet found a way to apply such features to this use case.</p>

<p>Let's go ahead and delete "PROD" using <code class="language-plaintext highlighter-rouge">tridentctl delete backend</code>. It's offline (failed) as we do this, so a delete will likely leave it in the <code class="language-plaintext highlighter-rouge">deleting</code> state like this:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>./trident-installer/tridentctl get backend <span class="nt">-n</span> trident
+----------------------+----------------+--------------------------------------+----------+---------+
|         NAME         | STORAGE DRIVER |                 UUID                 |  STATE   | VOLUMES |
+----------------------+----------------+--------------------------------------+----------+---------+
| SF-PROD-192.168.1.30 | solidfire-san  | 57883f01-94ba-48aa-8acd-e7330c128bce | deleting |       1 |
+----------------------+----------------+--------------------------------------+----------+---------+
</code></pre></div></div>

<p>We could also reinstall Trident to be sure. But in my testing I noticed that the <code class="language-plaintext highlighter-rouge">deleting</code> state is sufficient to ensure that new volumes don't get created on such a backend even if it is online.</p>

<p>While we're here, we can create a new Trident backend that uses the remote cluster (we <em>must</em> do this before we can import volumes, but not too early - such as before storage failover - because you could end up creating and using DR volumes from more remote Kubernetes workers (close to PROD storage) which may be undesirable for several reasons):</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>./trident-installer/tridentctl create backend <span class="nt">-n</span> trident <span class="nt">-f</span> be-dr.json 
+--------------------+----------------+--------------------------------------+--------+---------+
|        NAME        | STORAGE DRIVER |                 UUID                 | STATE  | VOLUMES |
+--------------------+----------------+--------------------------------------+--------+---------+
| SF-DR-192.168.1.34 | solidfire-san  | 38455724-54ad-4d6d-a1dc-7437ab56803b | online |       0 |
+--------------------+----------------+--------------------------------------+--------+---------+
</code></pre></div></div>

<p>After that we can delete the stale PVC and PV without actually deleting the PROD PV on the SolidFire PROD cluster (and that is because its backend is already in the <code class="language-plaintext highlighter-rouge">deleting</code> state). If you have more than one PVC, rinse &amp; repeat.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl delete pvc pg <span class="nt">-n</span> pg
persistentvolumeclaim <span class="s2">"pg"</span> deleted

<span class="nv">$ </span>kubectl delete pv pvc-fdd499f6-14f3-4280-bdef-35d84c5794bb <span class="nt">-n</span> pg
warning: deleting cluster-scoped resources, not scoped to the provided namespace
persistentvolume <span class="s2">"pvc-fdd499f6-14f3-4280-bdef-35d84c5794bb"</span> deleted
</code></pre></div></div>

<p>Because our Storage Class uses the <code class="language-plaintext highlighter-rouge">Retain</code> reclaim policy, removing a PVC will still leave us with the PROD PV in place. If you've deleted the production site backend (PROD) by now, you may delete that PV from Kubernetes as well (<code class="language-plaintext highlighter-rouge">kubectl delete pv</code>) - it won't be deleted from PROD.</p>

<p>To recap our current situation:</p>

<ul>
  <li>Removed from SolidFire at the DR site: replication pairs</li>
  <li>Removed from Kubernetes: backend "PROD", old PVC and old PV</li>
  <li>Added to Kubernetes: backend "DR"</li>
</ul>

<p>We're ready to import the replica with <code class="language-plaintext highlighter-rouge">tridentctl import volume</code> (using managed import). To that end we provide the names of the remote back-end (<code class="language-plaintext highlighter-rouge">SF-DR-192.168.1.34</code>), remote volume (<code class="language-plaintext highlighter-rouge">dr-$VOLNAME</code>), and the original PVC YAML created while using the PROD cluster (this tells Kubernetes what the volume is used for):</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>./trident-installer/tridentctl import volume <span class="nt">-n</span> trident SF-DR-192.168.1.34 dr-pvc-b4ad4998-05ce-46ab-9f67-a6e972805fa3 <span class="nt">-n</span> trident <span class="nt">-f</span> pvc-simple.yaml
+------------------------------------------+---------+---------------+----------+--------------------------------------+--------+---------+
|                   NAME                   |  SIZE   | STORAGE CLASS | PROTOCOL |             BACKEND UUID             | STATE  | MANAGED |
+------------------------------------------+---------+---------------+----------+--------------------------------------+--------+---------+
| pvc-d24cf316-496a-4807-aa54-08999f712f9d | 1.0 GiB | basic         | block    | 38455724-54ad-4d6d-a1dc-7437ab56803b | online | <span class="nb">true</span>    |
+------------------------------------------+---------+---------------+----------+--------------------------------------+--------+---------+
</code></pre></div></div>

<p>As you can see we created the remote SolidFire target volume we prefixed the source name with <code class="language-plaintext highlighter-rouge">dr-</code> (<code class="language-plaintext highlighter-rouge">dr-</code> + <code class="language-plaintext highlighter-rouge">pvc-b4ad4998-05ce-46ab-9f67-a6e972805fa3</code>). Use any SolidFire Volume Name you like when creating replica volumes, but don't lose track of which Volume ID you need to use - Volume Names are easy to look up (see Appendix).</p>

<p>Let's examine the recreated PVC and imported PV:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl get pvc <span class="nt">-n</span> pg
NAME   STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
pg     Bound    pvc-d24cf316-496a-4807-aa54-08999f712f9d   1Gi        RWO            basic          13m

<span class="nv">$ </span>kubectl get pv <span class="nt">-n</span> pg
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM   STORAGECLASS   REASON   AGE
pvc-d24cf316-496a-4807-aa54-08999f712f9d   1Gi        RWO            Retain           Bound    pg/pg   basic                   13m

<span class="nv">$ </span>kubectl describe pvc pg <span class="nt">-n</span> pg
Name:          pg
Namespace:     pg
StorageClass:  basic
Status:        Bound
Volume:        pvc-d24cf316-496a-4807-aa54-08999f712f9d
Labels:        &lt;none&gt;
Annotations:   pv.kubernetes.io/bind-completed: <span class="nb">yes
               </span>pv.kubernetes.io/bound-by-controller: <span class="nb">yes
               </span>trident.netapp.io/importBackendUUID: 38455724-54ad-4d6d-a1dc-7437ab56803b
               trident.netapp.io/importOriginalName: dr-pvc-b4ad4998-05ce-46ab-9f67-a6e972805fa3
               trident.netapp.io/notManaged: <span class="nb">false
               </span>volume.beta.kubernetes.io/storage-provisioner: csi.trident.netapp.io
...
</code></pre></div></div>

<p>Imported PV:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl describe pv pvc-d24cf316-496a-4807-aa54-08999f712f9d <span class="nt">-n</span> pg
Name:            pvc-d24cf316-496a-4807-aa54-08999f712f9d
Labels:          &lt;none&gt;
Annotations:     pv.kubernetes.io/provisioned-by: csi.trident.netapp.io
Finalizers:      <span class="o">[</span>kubernetes.io/pv-protection]
...
Source:
    Type:              CSI <span class="o">(</span>a Container Storage Interface <span class="o">(</span>CSI<span class="o">)</span> volume <span class="nb">source</span><span class="o">)</span>
    Driver:            csi.trident.netapp.io
    FSType:            ext4
    VolumeHandle:      pvc-d24cf316-496a-4807-aa54-08999f712f9d
    ReadOnly:          <span class="nb">false
    </span>VolumeAttributes:      <span class="nv">backendUUID</span><span class="o">=</span>38455724-54ad-4d6d-a1dc-7437ab56803b
                           <span class="nv">internalName</span><span class="o">=</span>dr-pvc-b4ad4998-05ce-46ab-9f67-a6e972805fa3
                           <span class="nv">name</span><span class="o">=</span>pvc-d24cf316-496a-4807-aa54-08999f712f9d
...
</code></pre></div></div>

<p>CSI gave the imported volume its own name (<code class="language-plaintext highlighter-rouge">pvc-d24cf316-496a-4807-aa54-08999f712f9d</code>) although the SolidFire Volume Name is <code class="language-plaintext highlighter-rouge">dr-pvc-b4ad4998-05ce-46ab-9f67-a6e972805fa3</code>.</p>

<p>Well, at least we can still find the SolidFire Volume Name in <code class="language-plaintext highlighter-rouge">internalName</code> value, and make use of it in storage management outside of Kubernetes. For example, when we need to create a new destination volume and setup volume pairs, we can name it <code class="language-plaintext highlighter-rouge">$NS-$POD-$PVC</code> and later use that to setup backup or simply know what volume is what without having to look it up in Kubernetes. But we can't decide volume name for Kubernetes-created volumes, and shouldn't change those names either so we'd probably be better off if we just focused on that chart with Volume IDs and used that as a basis for storage and Kubernetes management.</p>

<p><code class="language-plaintext highlighter-rouge">backendUUID</code> from the SolidFire DR cluster is expectedly different from that of the PROD cluster.</p>

<p>Our current situation:</p>

<ul>
  <li>Removed from SolidFire at the DR site: replication pairs</li>
  <li>Removed from Kubernetes: backend PROD, old PVC and old PV</li>
  <li>Added to Kubernetes: backend DR, and (using <code class="language-plaintext highlighter-rouge">tridentctl import volume</code>) PV and PVC</li>
</ul>

<p>We're back in business, but we must not forget to protect our data and get ready for failback, as soon as the PROD cluster is accessible:</p>

<h3 id="steps-to-configure-solidfire-volume-replication-in-the-opposite-direction">Steps to configure SolidFire volume replication in the opposite direction</h3>

<ul>
  <li>Production site:
    <ul>
      <li>SolidFire: go to Data Protection &gt; Volume Pairs, delete volume pairs for outbound (PROD=&gt;DR) replication while taking note of Volume IDs in replication pairs (we need to recreate pairs for inbound replication)</li>
      <li>SolidFire: go to Manage &gt; Volumes, find each Volume that was replicated and change it from <code class="language-plaintext highlighter-rouge">Read/Write</code> (<code class="language-plaintext highlighter-rouge">readWrite</code> in the API and PowerShell) to <code class="language-plaintext highlighter-rouge">replicationTarget</code></li>
      <li>SolidFire: if new PVCs have been created while operating at the remote site (DR), create matching volumes on PROD (set them to access mode: <code class="language-plaintext highlighter-rouge">replicationTarget</code>) and update your volume pairs document</li>
    </ul>
  </li>
  <li>DR site:
    <ul>
      <li>SolidFire: using notes from failover plus any new volumes (created at on DR cluster), use Src &amp; Dst Volume ID pairs (DR=&gt;PROD) to configure and initiate reverse replication for DR=&gt;PROD failback</li>
    </ul>
  </li>
</ul>

<p>When completing volume pairing for reverse replication the last step is done at the Destination as usual: in this screenshot we can see in this last step PROD refers to DR as Source of replication:</p>

<p><img src="/assets/images/solidfire-kubernetes-replication-failover-failback-07-complete-replication-pairing-at-destination.png" alt="Trident-inserted SolidFire volume attributes in SolidFire UI at Active site" /></p>

<p>As we pair volumes from DR to PROD, if volumes paired on the PROD cluster are set to <code class="language-plaintext highlighter-rouge">replicationTarget</code> replication should kick off in two-three minutes.</p>

<p>SolidFire replication is efficient in both directions. Only the changed blocks will need to be copied between the sites (and even those will be compressed and deduplicated).</p>

<h2 id="failback-drprod">Failback (DR=&gt;PROD)</h2>

<p>We reversed storage replication in the Failover section (see the previous paragraphs), so DR=&gt;PROD replication is already taking place.</p>

<p>There is a symmetry in SolidlFire failover and failback, as far as amount of data and workflow are concerned. The main difference between SolidFire failover and failback is that failback is (usually) planned and both sites are online.</p>

<p>Therefore, in this part we skip repetitive details and focus on what's different (Trident failback).</p>

<h3 id="steps-for-a-solidfire-cluster-failback">Steps for a SolidFire cluster failback</h3>

<ul>
  <li>DR site:
    <ul>
      <li>Kubernetes: scale in to 0, to make workloads stop</li>
      <li>SolidFire: observe replication status of replicated volumes, they need to be in sync (with async, delay should become <code class="language-plaintext highlighter-rouge">00:00:00.000000</code>) and any snapshots that are supposed to be replicated should be visible on the Destination cluster</li>
      <li>SolidFire: assuming all workloads were stopped earlier, there's nothing new in replication queues - delete all volume replication pairs</li>
      <li>SolidFire: change all volumes that have been Source, to Replication Target, to make them ready for PROD=&gt;DR replication (once we successfully failback)</li>
    </ul>
  </li>
  <li>Production site:
    <ul>
      <li>SolidFire: delete volume pairs (DR=&gt;PROD), make volumes readWrite</li>
      <li>SolidFire: configure and initiate PROD=&gt;DR replication (last step is performed at the destination when the SolidFire Web UI is used)</li>
      <li>SolidFire: delete volume replication (pairs) to stop replication, change replica volumes' mode from <code class="language-plaintext highlighter-rouge">replicationTarget</code> to <code class="language-plaintext highlighter-rouge">readWrite</code></li>
      <li>Kubernetes: uninstall and then install Trident to clean backends and volume configuration (this also deletes the backend DR)</li>
      <li>Kubernetes: create a new backend PROD using the same configuration file from before. Its backendUUID will be unique</li>
      <li>Kubernetes: delete "stale" PVC from the site DR and any stale PVs (this will delete them from Kubernetes but not from SolidFire, because all backends which provisioned them were wiped when we reinstalled Trident)</li>
      <li>Trident: use <code class="language-plaintext highlighter-rouge">tridentctl import volume</code> to import replica PV and recreate PVC configuration</li>
      <li>SolidFire: configure storage replication (PROD=&gt;DR)</li>
    </ul>
  </li>
</ul>

<h4 id="notes-on-failback">Notes on failback</h4>

<p>How to tell all replicas are up to date, and it's safe to fail back? We can use the Web UI for visual inspection and PowerShell, Ansible, Python CLI, JSON-RPC for automated checks to make sure all data changes at Source ("DR") have been replicated to Destination ("PROD").</p>

<p><img src="/assets/images/solidfire-kubernetes-replication-failover-failback-05-async-replication-status.png" alt="Monitor replication delay in SolidFire" /></p>

<p>The Trident reinstall step may be controversial - it's certainly not officially recommended, but then again there's no recommendation of any kind and I think it makes the process easy. Where things can get dangerous is if multiple backends are used - then re-installation may become complicated or at the very least require additional planning.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>./trident-installer/tridentctl uninstall <span class="nt">-n</span> trident
./trident-installer/tridentctl <span class="nb">install</span> <span class="nt">-n</span> trident
</code></pre></div></div>

<p>In Scenario 2 with one Kubernetes cluster per site we wouldn't have two backends on either of those Kubernetes clusters and wouldn't benefit from reinstalling Trident. But I have one cluster that's switching sites and backends multiple times.</p>

<p>What should those users who also have <em>other</em> Trident volumes which weren't replicated do? Those volumes could be imported too, and it'd be seamless if we maintained PVC-Volume ID pairings as mentioned earlier. Otherwise - in Scenario 1 at least - we also couldn't delete backend PROD upon failover, and couldn't prevent Trident from creating volumes on the remote backend. So in order to avoid having to reinstall Trident and maintain PVC-VolID pairs we'd need to make substantial changes for Scenario 1.</p>

<p>As we create "PROD" for the second time, its randomly generated UUID is different (now: <code class="language-plaintext highlighter-rouge">1a474ae1-...</code>, before: <code class="language-plaintext highlighter-rouge">57883f01-...</code>).</p>

<p>We import the old-new volume (SolidFire Volume Name <code class="language-plaintext highlighter-rouge">pvc-b4ad4998-05ce-46ab-9f67-a6e972805fa3</code>) and it gets a new name, assigned to the new backend:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="nv">$ </span>./trident-installer/tridentctl import volume <span class="nt">-n</span> trident SF-PROD-192.168.1.30 pvc-b4ad4998-05ce-46ab-9f67-a6e972805fa3 <span class="nt">-n</span> trident <span class="nt">-f</span> pvc-simple.yaml
+------------------------------------------+---------+---------------+----------+--------------------------------------+--------+---------+
|                   NAME                   |  SIZE   | STORAGE CLASS | PROTOCOL |             BACKEND UUID             | STATE  | MANAGED |
+------------------------------------------+---------+---------------+----------+--------------------------------------+--------+---------+
| pvc-f5cda73c-98d2-441a-9296-93f841a5cde8 | 1.0 GiB | basic         | block    | 1a474ae1-6559-472a-a5ae-74d8c220a8b2 | online | <span class="nb">true</span>    |
+------------------------------------------+---------+---------------+----------+--------------------------------------+--------+---------+
</code></pre></div></div>

<p>Like with failover, we must delete the PVC in order to use <code class="language-plaintext highlighter-rouge">import volume</code>:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl delete pvc pg <span class="nt">-n</span> pg
persistentvolumeclaim <span class="s2">"pg"</span> deleted
</code></pre></div></div>

<p>At this point we haven't yet imported any volumes, so any <code class="language-plaintext highlighter-rouge">kubectl get pv</code> output is likely to contain (at most) stale PVs from the site DR.</p>

<p>You can use <code class="language-plaintext highlighter-rouge">./trident-installer/tridentctl get volumes -n trident</code> (assuming Trident is installed in that namespace) to narrow down the list to only Trident-provisioned volumes, but <code class="language-plaintext highlighter-rouge">kubectl get pv</code> will likely know of stale Trident volumes that <code class="language-plaintext highlighter-rouge">tridentctl</code> doesn't know (because we reinstalled Trident).</p>

<p>So use <code class="language-plaintext highlighter-rouge">kubectl</code> to check any PVs for their backend information and, if their backend UUIDs are from "DR" or the old "PROD", delete them (<code class="language-plaintext highlighter-rouge">kubectl delete pv</code>). They won't be removed from SolidFire, either PROD or DR, because Trident has no way to map these stale volumes to backendUUIDs it knows nothing about: that is why we reinstalled Trident!</p>

<p>All the newly imported volumes should come back and have the latest PROD's backendUUID.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl describe pv pvc-f5cda73c-98d2-441a-9296-93f841a5cde8 <span class="nt">-n</span> pg
Name:            pvc-f5cda73c-98d2-441a-9296-93f841a5cde8
...         
Source:
    Type:              CSI <span class="o">(</span>a Container Storage Interface <span class="o">(</span>CSI<span class="o">)</span> volume <span class="nb">source</span><span class="o">)</span>
    Driver:            csi.trident.netapp.io
    FSType:            ext4
    VolumeHandle:      pvc-f5cda73c-98d2-441a-9296-93f841a5cde8
    ReadOnly:          <span class="nb">false
    </span>VolumeAttributes:      <span class="nv">backendUUID</span><span class="o">=</span>1a474ae1-6559-472a-a5ae-74d8c220a8b2
                           <span class="nv">internalName</span><span class="o">=</span>pvc-b4ad4998-05ce-46ab-9f67-a6e972805fa3
                           <span class="nv">name</span><span class="o">=</span>pvc-f5cda73c-98d2-441a-9296-93f841a5cde8
...
</code></pre></div></div>

<p>We have already configured and initiated PROD=&gt;DR replication, so failback is now complete.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Kubernetes failover and failback with Trident and SolidFire are cleaner than the "old school" approach that used patching. Trident's volume import feature makes the proces much more convenient.</p>

<p>While the reinstallation of Trident prior to failback is not required in the official documentation, there's no failover and failback recipe in the official documentation and my approach works nicely and takes just 30 seconds. I didn't notice any downsides to it except that PVCs from the PROD cluster get new PV names in Kubernetes, but if you use Volume IDs to manage replication and import, that shouldn't be a problem. If, on the other hand, you have other data protection software in place, or have many volumes that aren't replicated, you should evaluate a more suitable approach.</p>

<p>Kubernetes' PV names are unsuitable for human consumption so SolidFire's primary focus on Volume IDs makes it possible to forget Volume Names and stick to managing Volume IDs. If we store Kubernetes configuration YAMLs and SolidFire volume IDs in a repository, we can use that to automate everything (the creation of remote volumes, the pairing of Src-Dst and Dst-Src volumes, Trident-related failover and failback operations including volume import).</p>

<p>Because Trident deletes SolidFire volumes by <em>purging</em> them, it is important to ensure that storage classes for replicated volumes have reclaim policy for volumes (and possibly for volume snapshots; see in Appendix for additional notes on that) set to Retain.</p>

<p>Topology-aware CSI configurations may be able to simplify the management of multiple back-ends in the scenario where one Kubernetes cluster can access two SolidFire back-ends, but I have not found a way to effectively use it with SolidFire. NetApp Trident is constantly evolving so check the latest features and newer blog posts out there to find about latest improvements in SolidFire-related features.</p>

<h2 id="appendix">Appendix</h2>

<h3 id="solidfire-replication-and-switch-over-under-the-hood">SolidFire replication and switch-over under the hood</h3>

<p>Assuming your network configuration is symmetric (that is, your volume X isn't exposed on VLAN 100 and VLAN 200, respectively), you only need to worry about the following differences between sites:</p>

<ul>
  <li>Storage VIP (one per SolidFire cluster) (example: <code class="language-plaintext highlighter-rouge">192.168.1.30</code>)</li>
  <li>Unique cluster ID (example: <code class="language-plaintext highlighter-rouge">mn4y</code>)</li>
  <li>Volume ID (example: <code class="language-plaintext highlighter-rouge">270</code>)</li>
</ul>

<p>Of course, there's also a Volume Name, which can be consistent across sites, prefixed with a unique site-specific or Kubernetes cluster-specific string, or entirely different.</p>

<p>Example:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Cluster</th>
      <th style="text-align: center">Svip</th>
      <th style="text-align: center">uniqueId</th>
      <th style="text-align: center">volumeName</th>
      <th style="text-align: center">volId</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">PROD</td>
      <td style="text-align: center">192.168.1.30</td>
      <td style="text-align: center">mn4y</td>
      <td style="text-align: center">pvc-ba229a61-6026-44bd-8b39-29863261469a</td>
      <td style="text-align: center">270</td>
    </tr>
  </tbody>
</table>

<p>When an iSCSI client connects to a volume like that, this is what we see in Reporting &gt; iSCSI Sessions in the SolidFire Web UI:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">iqn.2010-01.com.solidfire:mn4y.pvc-ba229a61-6026-44bd-8b39-29863261469a.270</code></li>
</ul>

<p>The same can be observed with <code class="language-plaintext highlighter-rouge">Get-SFIscsiSession</code> (PowerShell):</p>

<div class="language-powershell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">&gt;</span><span class="w"> </span><span class="n">Get-SFIscsiSession</span><span class="w">

</span><span class="n">DriveIDs</span><span class="w">               </span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="mi">1</span><span class="p">}</span><span class="w">
</span><span class="n">AccountID</span><span class="w">              </span><span class="p">:</span><span class="w"> </span><span class="nx">14</span><span class="w">
</span><span class="o">...</span><span class="w">
</span><span class="n">InitiatorPortName</span><span class="w">      </span><span class="p">:</span><span class="w"> </span><span class="nx">iqn.1993-08.org.debian:01:9dbcf2ddfb75</span><span class="p">,</span><span class="nx">i</span><span class="p">,</span><span class="nx">0x23d000001</span><span class="w">
</span><span class="n">TargetPortName</span><span class="w">         </span><span class="p">:</span><span class="w"> </span><span class="nx">iqn.2010-01.com.solidfire:mn4y.pvc-ba229a61-6026-44bd-8b39-29863261469a.270</span><span class="p">,</span><span class="nx">t</span><span class="p">,</span><span class="nx">0x1</span><span class="w">
</span><span class="n">InitiatorName</span><span class="w">          </span><span class="p">:</span><span class="w"> </span><span class="nx">iqn.1993-08.org.debian:01:9dbcf2ddfb75</span><span class="w">
</span><span class="o">...</span><span class="w">
</span><span class="n">TargetName</span><span class="w">             </span><span class="p">:</span><span class="w"> </span><span class="nx">iqn.2010-01.com.solidfire:mn4y.pvc-ba229a61-6026-44bd-8b39-29863261469a.270</span><span class="w">
</span><span class="n">TargetIP</span><span class="w">               </span><span class="p">:</span><span class="w"> </span><span class="nx">192.168.103.29:3260</span><span class="w">
</span><span class="n">VirtualNetworkID</span><span class="w">       </span><span class="p">:</span><span class="w"> </span><span class="nx">0</span><span class="w">
</span><span class="n">VolumeID</span><span class="w">               </span><span class="p">:</span><span class="w"> </span><span class="nx">270</span><span class="w">
</span></code></pre></div></div>

<p>Now we know how to put together our SF iSCSI target:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">iqn.2010-01.com.solidfire:</code> + <code class="language-plaintext highlighter-rouge">${CLUSTER-UNIQUE-ID}</code> + <code class="language-plaintext highlighter-rouge">.</code> + <code class="language-plaintext highlighter-rouge">${VOLUME-NAME}</code> + <code class="language-plaintext highlighter-rouge">.</code> + <code class="language-plaintext highlighter-rouge">${VOLUME-ID}</code></li>
</ul>

<p>We can get Svip (Storage Virtual IP) and UniqueID using <code class="language-plaintext highlighter-rouge">Get-SFClusterInfo</code>, which means we have everything required to transform iSCSI device paths from PROD and swap them with Target device paths (which is what storage site failover would require), and back.</p>

<p>And now also understand why Kubernetes cannot transparently switch bewteen two SolidFire clusters, even with identical SVIPs (there would be a router in betwween): UniqueId makes device names different and PV patching is required if you don't choose to use the volume import feature. If you are interested in the patching approach you can consider using the script linked at the bottom of this post.</p>

<h3 id="solidfire-volume-names-and-ids">SolidFire Volume Names and IDs</h3>

<p>On SolidFire, a Volume Name is just a tag for the volume object uniquely identified by Volume ID. Volume ID is unique within a cluster, while Volume Names can be duplicate (which can create confusion among people and scripts, so when we create clones or new volumes in the same cluster we try to avoid doing that).</p>

<p>We can't decide Kubernetes volume names when they are created, and we shouldn't change them once they are created. Across clusters, each volume in a pairs can have the same Name (as in the example above) and some people may find that easier to work with.</p>

<p>Because names of volumes at the remote site are decided by us when we create replica volumes we can pick any allowed, but I assume most folks would prefer one of these approaches:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Approach</th>
      <th style="text-align: left">SF Vol Name at PROD site</th>
      <th style="text-align: left">SF Vol Name at DR site</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Symmetric</td>
      <td style="text-align: left">pvc-ba229a61-6026-44bd-8b39-29863261469a</td>
      <td style="text-align: left">pvc-ba229a61-6026-44bd-8b39-29863261469a</td>
    </tr>
    <tr>
      <td style="text-align: left">Asymmetric</td>
      <td style="text-align: left">prod-pvc-ba229a61-6026-44bd-8b39-29863261469a</td>
      <td style="text-align: left">dr-pvc-ba229a61-6026-44bd-8b39-29863261469a</td>
    </tr>
    <tr>
      <td style="text-align: left">Asymmetric for Humans</td>
      <td style="text-align: left">pvc-ba229a61-6026-44bd-8b39-29863261469a</td>
      <td style="text-align: left">pg-data</td>
    </tr>
  </tbody>
</table>

<p>Kubernetes picks its own volume names, but we pick the names of remote volumes that we create before we set up replication.</p>

<p>I'm not convinced that naming the remote volume <code class="language-plaintext highlighter-rouge">pg-data</code> would be helpful but sure, you could use that or have your own schema (<code class="language-plaintext highlighter-rouge">$SITE-$NS-$POD-$DISK</code> or whatever works for you) for replica volumes. But I suggest to try to manage Volume IDs in Git or S3 and automate.</p>

<p>What's partially replicated with Volume is its other tags (called Attributes). Among other things we note the volume is formatted with XFS, origin cluster was Kubernetes v1.20.2 on ARM64 - all important details for CSI are in there.</p>

<p><img src="/assets/images/solidfire-kubernetes-replication-failover-failback-01-volume-attributes.png" alt="Trident-inserted SolidFire volume attributes in SolidFire UI at Active site" /></p>

<p>Form the Trident documentation: "When a volume is imported, an annotation is added to the PVC and PV that serves a dual purpose of indicating that the volume was imported and if the PVC and PV are managed. This annotation should not be modified or removed."</p>

<h3 id="fail-prod-cluster">Fail PROD cluster</h3>

<p>To simulate SolidFire cluster failure I simply shut down PROD (SolidFire cluster at the primary site). The command can be used to shut down all SolidFire nodes at once (use <code class="language-plaintext highlighter-rouge">-Node</code> to pass all node IDs in a comma-separated list).</p>

<div class="language-powershell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">&gt;</span><span class="w"> </span><span class="n">Invoke-SFShutdown</span><span class="w"> </span><span class="nt">-Option</span><span class="w"> </span><span class="nx">halt</span><span class="w">
</span></code></pre></div></div>

<p>If you don't have a way to power it up again (IPMI or physical access), don't use <code class="language-plaintext highlighter-rouge">halt</code>. I used <code class="language-plaintext highlighter-rouge">halt</code> because I wanted to ensure the failed cluster doesn't come up again, and <code class="language-plaintext highlighter-rouge">reboot</code> when I didn't mind that.</p>

<p>If you test failover in a production environment, there's no particular reason to power off anything - see the last part at the very bottom of this post.</p>

<h3 id="configure-replication">Configure replication</h3>

<p>As stated at the beginning, we assume no one screwed with the volume names or attributes so we can query the API for that name to get one and only one VolumeID that matches it.</p>

<p>If we had this information out-of-band information (externally stored YAML or CSV files) we could work with SolidFire Volume IDs and pick random volume names at the DR site.</p>

<div class="language-powershell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">&gt;</span><span class="w"> </span><span class="n">Get-SFVolume</span><span class="w"> </span><span class="nt">-Name</span><span class="w"> </span><span class="s2">"pvc-a410d978-c346-4322-912d-5323571020b3"</span><span class="w">

</span><span class="n">VolumeID</span><span class="w">                    </span><span class="p">:</span><span class="w"> </span><span class="nx">273</span><span class="w">
</span><span class="n">Name</span><span class="w">                        </span><span class="p">:</span><span class="w"> </span><span class="nx">pvc-a410d978-c346-4322-912d-5323571020b3</span><span class="w">
</span><span class="n">AccountID</span><span class="w">                   </span><span class="p">:</span><span class="w"> </span><span class="nx">21</span><span class="w">
</span><span class="n">CreateTime</span><span class="w">                  </span><span class="p">:</span><span class="w"> </span><span class="nx">2021-03-15T08:47:59Z</span><span class="w">
</span><span class="n">VolumeConsistencyGroupUUID</span><span class="w">  </span><span class="p">:</span><span class="w"> </span><span class="nx">18bd234c-e333-499e-87b7-92de29a8b52f</span><span class="w">
</span><span class="n">VolumeUUID</span><span class="w">                  </span><span class="p">:</span><span class="w"> </span><span class="nx">ab7c5eb2-0138-4f40-9b7d-d32570bc1dee</span><span class="w">
</span><span class="n">EnableSnapMirrorReplication</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="nx">False</span><span class="w">
</span><span class="n">Status</span><span class="w">                      </span><span class="p">:</span><span class="w"> </span><span class="nx">active</span><span class="w">
</span><span class="n">Access</span><span class="w">                      </span><span class="p">:</span><span class="w"> </span><span class="nx">readWrite</span><span class="w">
</span><span class="n">Enable512e</span><span class="w">                  </span><span class="p">:</span><span class="w"> </span><span class="nx">True</span><span class="w">
</span><span class="n">Iqn</span><span class="w">                         </span><span class="p">:</span><span class="w"> </span><span class="nx">iqn.2010-01.com.solidfire:mn4y.pvc-a410d978-c346-4322-912d-5323571020b3.273</span><span class="w">
</span><span class="n">ScsiEUIDeviceID</span><span class="w">             </span><span class="p">:</span><span class="w"> </span><span class="nx">6d6e347900000111f47acc0100000000</span><span class="w">
</span><span class="n">ScsiNAADeviceID</span><span class="w">             </span><span class="p">:</span><span class="w"> </span><span class="nx">6f47acc1000000006d6e347900000111</span><span class="w">
</span><span class="n">Qos</span><span class="w">                         </span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="s2">"MinIOPS"</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">200</span><span class="p">,</span><span class="w"> </span><span class="s2">"MaxIOPS"</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">390</span><span class="p">,</span><span class="w"> </span><span class="s2">"BurstIOPS"</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">600</span><span class="p">,</span><span class="w"> </span><span class="s2">"BurstTime"</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">60</span><span class="p">}</span><span class="w">
</span><span class="n">QosPolicyID</span><span class="w">                 </span><span class="p">:</span><span class="w"> 
</span><span class="n">VolumeAccessGroups</span><span class="w">          </span><span class="p">:</span><span class="w"> </span><span class="p">{}</span><span class="w">
</span><span class="n">VolumePairs</span><span class="w">                 </span><span class="p">:</span><span class="w"> </span><span class="p">{}</span><span class="w">
</span><span class="n">DeleteTime</span><span class="w">                  </span><span class="p">:</span><span class="w"> 
</span><span class="n">PurgeTime</span><span class="w">                   </span><span class="p">:</span><span class="w"> 
</span><span class="n">LastAccessTime</span><span class="w">              </span><span class="p">:</span><span class="w"> 
</span><span class="n">LastAccessTimeIO</span><span class="w">            </span><span class="p">:</span><span class="w"> 
</span><span class="n">SliceCount</span><span class="w">                  </span><span class="p">:</span><span class="w"> </span><span class="nx">1</span><span class="w">
</span><span class="n">TotalSize</span><span class="w">                   </span><span class="p">:</span><span class="w"> </span><span class="nx">1073741824</span><span class="w">
</span><span class="n">BlockSize</span><span class="w">                   </span><span class="p">:</span><span class="w"> </span><span class="nx">4096</span><span class="w">
</span><span class="n">VirtualVolumeID</span><span class="w">             </span><span class="p">:</span><span class="w"> 
</span><span class="n">Attributes</span><span class="w">                  </span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="n">fstype</span><span class="p">,</span><span class="w"> </span><span class="nx">provisioning</span><span class="p">,</span><span class="w"> </span><span class="nx">trident</span><span class="p">,</span><span class="w"> </span><span class="nx">docker-name</span><span class="p">}</span><span class="w">
</span><span class="n">CurrentProtectionScheme</span><span class="w">     </span><span class="p">:</span><span class="w"> </span><span class="nx">singleHelix</span><span class="w">
</span><span class="n">PreviousProtectionScheme</span><span class="w">    </span><span class="p">:</span><span class="w"> 
</span></code></pre></div></div>

<p>On the remote cluster if we wanted the same name and size, while storage AccountID (used by Kubernetes) would likely be different and QoSPolicy ID also. The SF API won't let us use <code class="language-plaintext highlighter-rouge">Access: replicationTarget</code> right away, so first we just need to create it and then change it. Both of these below are executed against the DR cluster.</p>

<div class="language-powershell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="w"> </span><span class="err">&gt;</span><span class="w"> </span><span class="n">New-SFVolume</span><span class="w"> </span><span class="nt">-Name</span><span class="w"> </span><span class="s2">"pvc-a410d978-c346-4322-912d-5323571020b3"</span><span class="w"> </span><span class="nt">-AccountID</span><span class="w"> </span><span class="nx">30</span><span class="w"> </span><span class="nt">-Enable512e</span><span class="p">:</span><span class="nv">$True</span><span class="w"> </span><span class="nt">-TotalSize</span><span class="w"> </span><span class="nx">1073741824</span><span class="w"> </span><span class="nt">-QoSPolicyID</span><span class="w"> </span><span class="nx">6</span><span class="w">                           

</span><span class="n">VolumeID</span><span class="w">                    </span><span class="p">:</span><span class="w"> </span><span class="nx">223</span><span class="w">
</span><span class="n">Name</span><span class="w">                        </span><span class="p">:</span><span class="w"> </span><span class="nx">pvc-a410d978-c346-4322-912d-5323571020b3</span><span class="w">
</span><span class="n">AccountID</span><span class="w">                   </span><span class="p">:</span><span class="w"> </span><span class="nx">30</span><span class="w">
</span><span class="n">CreateTime</span><span class="w">                  </span><span class="p">:</span><span class="w"> </span><span class="nx">2021-03-15T08:59:35Z</span><span class="w">
</span><span class="o">...</span><span class="w">
</span><span class="n">Status</span><span class="w">                      </span><span class="p">:</span><span class="w"> </span><span class="nx">active</span><span class="w">
</span><span class="n">Access</span><span class="w">                      </span><span class="p">:</span><span class="w"> </span><span class="nx">readWrite</span><span class="w">

</span><span class="err">&gt;</span><span class="w"> </span><span class="n">Set-SFVolume</span><span class="w"> </span><span class="nt">-VolumeID</span><span class="w"> </span><span class="nx">223</span><span class="w"> </span><span class="nt">-Access</span><span class="w"> </span><span class="nx">replicationTarget</span><span class="w"> </span><span class="nt">-Confirm</span><span class="p">:</span><span class="nv">$False</span><span class="w">

</span><span class="n">VolumeID</span><span class="w">                    </span><span class="p">:</span><span class="w"> </span><span class="nx">223</span><span class="w">
</span><span class="n">Name</span><span class="w">                        </span><span class="p">:</span><span class="w"> </span><span class="nx">pvc-a410d978-c346-4322-912d-5323571020b3</span><span class="w">
</span><span class="n">AccountID</span><span class="w">                   </span><span class="p">:</span><span class="w"> </span><span class="nx">30</span><span class="w">
</span><span class="o">...</span><span class="w">
</span><span class="n">Status</span><span class="w">                      </span><span class="p">:</span><span class="w"> </span><span class="nx">active</span><span class="w">
</span><span class="n">Access</span><span class="w">                      </span><span class="p">:</span><span class="w"> </span><span class="nx">replicationTarget</span><span class="w">
</span></code></pre></div></div>

<p>Use <code class="language-plaintext highlighter-rouge">Start-SFClusterPairing</code> (source) and <code class="language-plaintext highlighter-rouge">Complete-SFClusterPairing</code> (target) to pair clusters (do this just once per pair) and <code class="language-plaintext highlighter-rouge">Start-SFVolumePairing</code> (source) and <code class="language-plaintext highlighter-rouge">Complete-SFVolumePairing</code> (target) to pair volumes (once per each volume pair).</p>

<p><code class="language-plaintext highlighter-rouge">Set-SFVolumePair -Mode Async|Sync|snapshotOnly</code> can be used to configure volume replication mode.</p>

<p>Once done, you'll have a cluster and volume pair(s) like this:</p>

<div class="language-powershell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">&gt;</span><span class="w"> </span><span class="n">Get-SFClusterPair</span><span class="w">       

</span><span class="n">ClusterName</span><span class="w">     </span><span class="p">:</span><span class="w"> </span><span class="nx">DR</span><span class="w">
</span><span class="n">ClusterPairID</span><span class="w">   </span><span class="p">:</span><span class="w"> </span><span class="nx">10</span><span class="w">
</span><span class="n">ClusterPairUUID</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="nx">4aa9be97-c396-4de7-9bbb-2515c7d998f3</span><span class="w">
</span><span class="n">Latency</span><span class="w">         </span><span class="p">:</span><span class="w"> </span><span class="nx">1</span><span class="w">
</span><span class="n">Mvip</span><span class="w">            </span><span class="p">:</span><span class="w"> </span><span class="nx">192.168.1.34</span><span class="w">
</span><span class="n">Status</span><span class="w">          </span><span class="p">:</span><span class="w"> </span><span class="nx">Connected</span><span class="w">
</span><span class="n">Version</span><span class="w">         </span><span class="p">:</span><span class="w"> </span><span class="nx">11.7.0.76</span><span class="w">
</span><span class="n">ClusterUUID</span><span class="w">     </span><span class="p">:</span><span class="w"> </span><span class="nx">72k4</span><span class="w">

</span><span class="err">&gt;</span><span class="w"> </span><span class="n">Get-SFVolumePair</span><span class="w"> 

</span><span class="n">VolumeID</span><span class="w">                    </span><span class="p">:</span><span class="w"> </span><span class="nx">291</span><span class="w">
</span><span class="n">Name</span><span class="w">                        </span><span class="p">:</span><span class="w"> </span><span class="nx">pvc-6de28b98-343f-45f5-9810-da795a81306f</span><span class="w">
</span><span class="n">AccountID</span><span class="w">                   </span><span class="p">:</span><span class="w"> </span><span class="nx">21</span><span class="w">
</span><span class="n">CreateTime</span><span class="w">                  </span><span class="p">:</span><span class="w"> </span><span class="nx">2021-03-21T05:15:07Z</span><span class="w">
</span><span class="n">VolumeConsistencyGroupUUID</span><span class="w">  </span><span class="p">:</span><span class="w"> </span><span class="nx">2caf356f-81ac-41f5-b4f5-3923d4dc4a70</span><span class="w">
</span><span class="n">VolumeUUID</span><span class="w">                  </span><span class="p">:</span><span class="w"> </span><span class="nx">d40cce76-aea1-4cad-b4fd-e1baa0c037c1</span><span class="w">
</span><span class="n">EnableSnapMirrorReplication</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="nx">False</span><span class="w">
</span><span class="n">Status</span><span class="w">                      </span><span class="p">:</span><span class="w"> </span><span class="nx">active</span><span class="w">
</span><span class="n">Access</span><span class="w">                      </span><span class="p">:</span><span class="w"> </span><span class="nx">readWrite</span><span class="w">
</span><span class="n">Enable512e</span><span class="w">                  </span><span class="p">:</span><span class="w"> </span><span class="nx">True</span><span class="w">
</span><span class="n">Iqn</span><span class="w">                         </span><span class="p">:</span><span class="w"> </span><span class="nx">iqn.2010-01.com.solidfire:mn4y.pvc-6de28b98-343f-45f5-9810-da795a81306f.291</span><span class="w">
</span><span class="n">ScsiEUIDeviceID</span><span class="w">             </span><span class="p">:</span><span class="w"> </span><span class="nx">6d6e347900000123f47acc0100000000</span><span class="w">
</span><span class="n">ScsiNAADeviceID</span><span class="w">             </span><span class="p">:</span><span class="w"> </span><span class="nx">6f47acc1000000006d6e347900000123</span><span class="w">
</span><span class="n">Qos</span><span class="w">                         </span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="s2">"MinIOPS"</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">200</span><span class="p">,</span><span class="w"> </span><span class="s2">"MaxIOPS"</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">390</span><span class="p">,</span><span class="w"> </span><span class="s2">"BurstIOPS"</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">600</span><span class="p">,</span><span class="w"> </span><span class="s2">"BurstTime"</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">60</span><span class="p">}</span><span class="w">
</span><span class="n">QosPolicyID</span><span class="w">                 </span><span class="p">:</span><span class="w"> 
</span><span class="n">VolumeAccessGroups</span><span class="w">          </span><span class="p">:</span><span class="w"> </span><span class="p">{}</span><span class="w">
</span><span class="n">VolumePairs</span><span class="w">                 </span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="n">pg</span><span class="p">}</span><span class="w">
</span><span class="n">DeleteTime</span><span class="w">                  </span><span class="p">:</span><span class="w"> 
</span><span class="n">PurgeTime</span><span class="w">                   </span><span class="p">:</span><span class="w"> 
</span><span class="n">LastAccessTime</span><span class="w">              </span><span class="p">:</span><span class="w"> 
</span><span class="n">LastAccessTimeIO</span><span class="w">            </span><span class="p">:</span><span class="w"> 
</span><span class="n">SliceCount</span><span class="w">                  </span><span class="p">:</span><span class="w"> </span><span class="nx">1</span><span class="w">
</span><span class="n">TotalSize</span><span class="w">                   </span><span class="p">:</span><span class="w"> </span><span class="nx">1073741824</span><span class="w">
</span><span class="n">BlockSize</span><span class="w">                   </span><span class="p">:</span><span class="w"> </span><span class="nx">4096</span><span class="w">
</span><span class="n">VirtualVolumeID</span><span class="w">             </span><span class="p">:</span><span class="w"> 
</span><span class="n">Attributes</span><span class="w">                  </span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="n">docker-name</span><span class="p">,</span><span class="w"> </span><span class="nx">provisioning</span><span class="p">,</span><span class="w"> </span><span class="nx">fstype</span><span class="p">,</span><span class="w"> </span><span class="nx">trident</span><span class="p">}</span><span class="w">
</span><span class="n">CurrentProtectionScheme</span><span class="w">     </span><span class="p">:</span><span class="w"> </span><span class="nx">singleHelix</span><span class="w">
</span><span class="n">PreviousProtectionScheme</span><span class="w">    </span><span class="p">:</span><span class="w"> 
</span></code></pre></div></div>

<p>What about Volume Resizing? Yes both SolidFire and Trident CSI support it, but consider the benefit of Thin Provisioning before that. Why?</p>

<p>What happens when you resize a replication Source, forget to resize its paired Target and need to failover? I'd rather not have to know. If you don't want either, better plan and test resizing of paired volumes or (better yet) provision larger volumes from the beginning to minimize or eliminate that activity.</p>

<p>If you want to compare Volume Names and do other time consuming and error prone tasks, automate. An example that checks if two SolidFire Volume IDs have consistent Volume Names (not that I necessarily recommend using identical names for replicated volume pairs - but if wanted to check here's how easy that'd be):</p>

<div class="language-powershell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">&gt;</span><span class="w"> </span><span class="kr">if</span><span class="w"> </span><span class="p">(</span><span class="w"> </span><span class="p">((</span><span class="n">Get-SFVolume</span><span class="w"> </span><span class="nt">-VolumeID</span><span class="w"> </span><span class="nx">273</span><span class="w"> </span><span class="nt">-SFConnection</span><span class="w"> </span><span class="nv">$connp</span><span class="p">)</span><span class="o">.</span><span class="nf">Name</span><span class="p">)</span><span class="w"> </span><span class="o">-eq</span><span class="w"> </span><span class="p">((</span><span class="n">Get-SFVolume</span><span class="w"> </span><span class="nt">-VolumeID</span><span class="w"> </span><span class="nx">223</span><span class="w"> </span><span class="nt">-SFConnection</span><span class="w"> </span><span class="nv">$connd</span><span class="p">)</span><span class="o">.</span><span class="nf">Name</span><span class="p">)</span><span class="w"> </span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">Write-Host</span><span class="w"> </span><span class="s2">"Good!"</span><span class="p">}</span><span class="w">
</span><span class="n">Good</span><span class="o">!</span><span class="w">
</span></code></pre></div></div>

<p>It takes just one second to compare 50 volume names with PowerShell, and you'll make zero mistakes (if your Volume IDs come from a correct, up-to-date config file; in this simple example Volume IDs are hardcoded).</p>

<p>Once volumes are paired, we can connect to Source cluster to obtain pairing details (<code class="language-plaintext highlighter-rouge">Get-SFVolumePair</code>). Or you can see replication status in the Web interface - see the second screenshot in the Failback-related content above.</p>

<p>Async replication delay can be obtained with <code class="language-plaintext highlighter-rouge">Get-SFVolumeStats</code> in SolidFire PowerShell Tools.</p>

<div class="language-powershell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">&gt;</span><span class="w"> </span><span class="p">(((</span><span class="n">Get-SFVolumeStats</span><span class="w"> </span><span class="nt">-VolumeID</span><span class="w"> </span><span class="nx">291</span><span class="p">)</span><span class="o">.</span><span class="nf">AsyncDelay</span><span class="p">)</span><span class="w"> </span><span class="o">-eq</span><span class="w"> </span><span class="s2">"00:00:00.000000"</span><span class="p">)</span><span class="w">
</span><span class="n">True</span><span class="w">
</span></code></pre></div></div>

<h3 id="restore-read-write-access-to-target-replica-volumes">Restore read-write access to target replica volumes</h3>

<p>Whether it's failover or failback, we delete volume pairing at the site we failing over <em>to</em> and make replica volumes writeable:</p>

<p><img src="/assets/images/solidfire-kubernetes-replication-failover-failback-04-make-replica-volume-writeable.png" alt="Make SolidFire Volume writeable" /></p>

<p>You must have noticed I say "delete volume pairings" rather than "pause replication". The reason is in Scenario 1 we don't expect to ever need to resume replication after failover. A contrived counter-example might be "We failed over too early before all changed data made it to the other site". Well, don't do that! As a reminder, replication is paused on a per-volume basis (for a pair), not for the entire cluster.</p>

<p>Example of a paused volume replication on the destination cluster (which we can tell by the direction of the arrow, <code class="language-plaintext highlighter-rouge">&lt;=</code>):</p>

<p><img src="/assets/images/solidfire-kubernetes-replication-failover-failback-06-paused-replication.png" alt="Pause SolidFire replication for a volume" /></p>

<h3 id="why-use-retain-for-replicated-volumes">Why use Retain for replicated volumes</h3>

<p>We could use Delete, too. If your protected volumes use Delete, you don't have to deal with Retain'ed PVs. Very convenient! Buuuut, when you release a PVC its PV gets both deleted <em>and</em> purged. Annnnd  it's gone!</p>

<p>If replication is active, you still have a good copy at the remote site (at least you had when you last checked three weeks ago). You may need to replicate that data back and within hours you'll recover from that mistake. What you don't want is to to make two mistakes at the same time (lose a volume <em>after</em> you've already lost a site).</p>

<p>The second thing to consider is snapshots. If you use CSI snapshots, presumably you want to be able to recover from data loss. In order to protect snapshots, PV should be Retain'ed so that released PVC don't purge PVs and effectively invalidate snapshot protection.</p>

<p>So there are multiple reasons why Retain is a safer approach to critical data, especially when there's a lot of it and you can't instantly re-replicate mistakenly reclaimed volumes.</p>

<h3 id="solidfire-replication-and-kubernetes-snapshot-volume-class">SolidFire replication and Kubernetes Snapshot Volume Class</h3>

<p>For important data, Volume Snapshot Class could also be set to Retain. But you'd be unable to expire such Snapshots (they'd remain Retain'ed, just like PVs with reclaimPolicy set to Retain).</p>

<p>Also note that Trident doesn't include snapshots into replication configuration of underlying volume, so what you could do is:</p>

<ul>
  <li>schedule snapshots in SolidFire independently of Kubernetes (which would make them crash-consistent snapshots) and include them in replication if the underlying volume is replicated (which know it is). This is easy and convenient. You don't get application-consistent snapshots (unless you suspend the app that uses that volume), but you don't have to deal with Kubernetes and it's extremely easy to automate (both schedules and retention) with tools like PowerShell or Ansible.</li>
  <li>make snapshot replication part of your snapshot workflow: take a Kubernetes snapshot, find its snapshot ID on SolidFire, and enable its replication with the underlying volume</li>
  <li>use external application and data protection solution that understands Kubernetes and leverages CSI snapshots to do this for you</li>
</ul>

<p>Mind the maximum number of snapshots per volume (30-ish).</p>

<p>Example of a Snapshot ID 1830 of a Volume ID 221:</p>

<p><img src="/assets/images/solidfire-kubernetes-replication-failover-failback-02-volume-snapshot-replication.png" alt="SolidFire snapshot at Standby site" /></p>

<h3 id="dealing-with-different-types-of-storage-cluster-failures">Dealing with different types of storage cluster failures</h3>

<p>This is another can of worms, but we assume that for asynchronous replication all Source volumes failed (stopped being updated) at the same time and all paired volumes at the Destination received updates to the same time point.</p>

<p>If that assumption isn't accurate and that matters, use synchronous replication or use async replication with a separate schedule for group snapshots included in replication.</p>

<p>In the latter case you'd have a common point-in-time state for a bunch of related replica volumes, and with such snapshots replicated to the remote site you could first revert all related volumes to the same snapshot, and then continue failover. Of course, you'd miss some latest changes as snapshots take minutes to get replicated and can be scheduled to 5 minute or higher intervals. There's no group snapshots in Kubernetes, but as far as I know (I'm lazy to check now), so configure and schedule - including for replication - group snapshots in SoldiFire if you need them.</p>

<h3 id="conduct-failover-and-failback-testing">Conduct failover and failback testing</h3>

<p>With SolidFire that is very easy. With only a few hours of work you could create an end-to-end (storage, pods, applications) automated test plan that runs on its own every day.</p>

<p>Before modern storage people used to pause replication, promote replicas to read-write mode for testing, and revert to resume replication once they're done. You can pause replication with SolidFire, but you don't have to.</p>

<p><img src="/assets/images/solidfire-kubernetes-replication-failover-failback-03-pause-volume-replication.png" alt="Pause SolidFire volume replication" /></p>

<p>SolidFire is space-efficient and easy to automate, so personally I'd prefer this:</p>

<ul>
  <li>clone replicated volume (give it a unique name, such as the current name prefixed with <code class="language-plaintext highlighter-rouge">test</code> so that <code class="language-plaintext highlighter-rouge">$NAME</code> becomes <code class="language-plaintext highlighter-rouge">test-$NAME</code>)</li>
  <li>assign that volume to another SolidFire storage account (<code class="language-plaintext highlighter-rouge">trident-test</code>, for example). Now Kubernetes can't even see it. You can serve this iSCSI clone volume over a different iSCSI VLAN, if you want to ensure additional segregation for security or performance</li>
  <li>stand up a Kubernetes test cluster and add a new Trident back-end to it (to use the <code class="language-plaintext highlighter-rouge">trident-test</code> account), so that it can see the cloned volume to be used for running various tests</li>
  <li>run <code class="language-plaintext highlighter-rouge">tridentctl import volume -n $TRIDENT-NAMESPACE test-$NAME</code> to import cloned volume</li>
</ul>

<p>Optionally:</p>

<ul>
  <li>adjust QoS on this (or all <code class="language-plaintext highlighter-rouge">test-*</code> volumes) to give it less performance (you could create a set of low performance policies for testing, so that you can simply apply them in one second with <code class="language-plaintext highlighter-rouge">Get-SFVolume -Name test* | Set-SFVolume -QoSPolicyId 7)</code>, for example)</li>
  <li>use a SC with the retention policy Delete, if you don't want to hang onto deleted test volumes and their snapshots</li>
  <li>when you're done, you can delete all PVCs (and PVs) of your <em>test</em> clones, but you can also leave them - they cost you (essentially) nothing! If you want to save IOPS, set all <code class="language-plaintext highlighter-rouge">test-*</code> volumes to Min/Max/Burst 50/50/100 IOPS when not using them, and <code class="language-plaintext highlighter-rouge">Get-SFVolume -Name test* | Set-SFVolume -QoSPolicyId 7)</code> when you need them</li>
  <li>when you want to test with latest replica data, use the SolidFire Copy Volume (not Clone Volume) method to eliminate the need to clone again (<code class="language-plaintext highlighter-rouge">Copy-SFVolume</code>). This command copies the difference between volumes so in less than a minute you'll have latest data and won't have to clone and import again</li>
  <li>evaluate whether it makes sense to use non-managed volume import (see the Trident documentation) and set reclaimPolicy to Delete (for test/dev)</li>
</ul>

<p>Finally, SolidFire Demo VM (free download from the NetApp support Web site) is excellent for this type testing in a VMware environment.</p>

<p>You can develop automation and run replicated test clusters without dedicated physical hardware - all you need is 4 VMs (2 Kubernetes and 2 SolidFire). That is what I used to write this post (apart from the ARM64 screenshot which is just a screenshot taken while working on the <a href="https://scaleoutsean.github.io/2021/02/24/netapp-trident-on-arm64.html">Trident-on-ARM64</a> post).</p>

<h3 id="automate-storage-failover-steps">Automate storage failover steps</h3>

<ul>
  <li>Using <code class="language-plaintext highlighter-rouge">tridentctl import volume</code> (CSI-compatible approach): TODO</li>
  <li>Using PV patching approach with pre-CSI K8s: evaluate and adjust <a href="https://github.com/scaleoutsean/sf-failover-k8s">this script</a> if possible</li>
</ul>

<h3 id="video-demo">Video demo</h3>

<ul>
  <li>SolidFire storage cluster failover using PostgreSQL: <a href="https://youtu.be/aSFxlGoHgdA">quick version</a> (2m56s) - PostgreSQL failover and failback, with scripted CLI commands</li>
  <li>SolidFire storage cluster failover with the focus on Trident CSI behavior, using a PVC/PV pair: <a href="https://youtu.be/kvy9I7rwfn8">detailed version</a> (10m55s) - features just a PV failover/failback, with detailed CLI commands</li>
</ul>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/bc" class="page__taxonomy-item p-category" rel="tag">bc</a><span class="sep">, </span>
    
      <a href="/tags/dr" class="page__taxonomy-item p-category" rel="tag">dr</a><span class="sep">, </span>
    
      <a href="/tags/netapp" class="page__taxonomy-item p-category" rel="tag">netapp</a><span class="sep">, </span>
    
      <a href="/tags/solidfire" class="page__taxonomy-item p-category" rel="tag">solidfire</a><span class="sep">, </span>
    
      <a href="/tags/trident" class="page__taxonomy-item p-category" rel="tag">trident</a><span class="sep">, </span>
    
      <a href="/tags/trident" class="page__taxonomy-item p-category" rel="tag">trident</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/automation" class="page__taxonomy-item p-category" rel="tag">automation</a><span class="sep">, </span>
    
      <a href="/categories/kubernetes" class="page__taxonomy-item p-category" rel="tag">kubernetes</a><span class="sep">, </span>
    
      <a href="/categories/storage" class="page__taxonomy-item p-category" rel="tag">storage</a>
    
    </span>
  </p>


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time class="dt-published" datetime="2021-03-20T00:00:00+08:00">2021-03-20 00:00</time></p>

      </footer>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://x.com/intent/tweet?via=scaleoutSean&text=Kubernetes+failover+and+failback+with+Trident+CSI+and+SolidFire%20https%3A%2F%2Fscaleoutsean.github.io%2F2021%2F03%2F20%2Fkubernetes-solidfire-failover-failback.html" class="btn btn--x" aria-label="Share on X" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on X">
    <i class="fab fa-fw fa-x-twitter" aria-hidden="true"></i><span> X</span>
  </a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://scaleoutsean.github.io/2021/03/20/kubernetes-solidfire-failover-failback.html" class="btn btn--linkedin" aria-label="Share on LinkedIn" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn">
    <i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span>
  </a>

</section>


      
  <nav class="pagination">
    
      <a href="/2021/03/09/get-started-with-solidfire-exporter.html" class="pagination--pager" title="Get started with solidfire-exporter">Previous</a>
    
    
      <a href="/2021/03/24/netapp-hci-compute-node-local-hardware-monitoring.html" class="pagination--pager" title="Hardware monitoring of NetApp HCI compute nodes for dark sites">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
    
<div class="page__related">
  
  <h2 class="page__related-title">You may also enjoy</h2>
  <div class="grid__wrapper">
    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/2025/08/11/epa-4-beta.html" rel="permalink">E-Series Performance Analyzer (EPA) v4 beta
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2025-08-11T00:00:00+08:00">2025-08-11 00:00</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          9 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">
  Introduction
  EPA 4
  EPA Collector-specific
  Details related to storage requirements
  What's next

</p>
  </article>
</div>

    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/2025/08/11/bing-index-issue.html" rel="permalink">Got binged in July 2025
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2025-08-11T00:00:00+08:00">2025-08-11 00:00</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          3 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">I don't track users (see Privacy page), don't have ads and don't care about "likes" or "followers", but generally I want my content to be in search engine in...</p>
  </article>
</div>

    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/2025/07/30/solidfire-windows-admin-center-extension.html" rel="permalink">SolidFire Extension for Windows Admin Center 2025
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2025-07-30T00:00:00+08:00">2025-07-30 00:00</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          19 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">
  Introduction
  Walk-through
  And now
  Conclusion
  Appendix A: demo
    
      Animated GIF (no playback control)
      Video demo with voice-over
    ...</p>
  </article>
</div>

    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/2025/07/26/solidfire-windows-admin-center-gateway.html" rel="permalink">SolidFire Gateway for Windows Admin Center 2025
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2025-07-26T00:00:00+08:00">2025-07-26 00:00</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          5 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">
  Introduction
  What it is
  What it does
  IIS Setup
  Why this matters
  What about it?
  Conclusion

</p>
  </article>
</div>

    
  </div>
</div>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        

<div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
          <li><a href="https://twitter.com/scaleoutsean" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/scaleoutsean" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>


<div class="page__footer-copyright">&copy; 2020-2025 scaleoutSean.github.io | <a href="https://scaleoutsean.github.io">Acting Technologist</a> | Terms: <a href="https://creativecommons.org/licenses/by/4.0/deed.en" target="_blank">CC BY 4.0</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>






  </body>
</html>
