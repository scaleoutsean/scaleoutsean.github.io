<!doctype html>
<html lang="en">
  <head>
        <meta charset="UTF-8">
    <meta http-equiv="content-language" content="en">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="googlebot" content="noarchive">
    <meta name="googlebot" content="max-snippet:60">
    <meta name="googlebot" content="max-image-preview:small">
    <meta name="bingbot" content="noarchive">
    <meta name="bingbot" content="max-snippet:60">
    <meta name="bingbot" content="max-image-preview:small">
    <meta name="robots" content="noarchive">
    <meta name="robots" content="max-snippet:60">
    <meta name="google-site-verification" content="F6q7vIwQ2G0j8tk-KL9rAXOMLXMDMMUkEz4fRs1Nnnc" />
    <title>
        
            Replicate volume data from Digital Ocean to SolidFire and back | Acting Technologist
      
    </title>
    <meta name="description" content="
     Steps for moving or copying data from Digital Ocean to SolidFire and back
     ">

    <!-- LINK TO ATOM FEED FOR SEO -->
    <link rel="alternate" type="application/atom+xml" href="https://scaleoutsean.github.io/feed.xml" />

    <!-- FAVICON -->
    <link rel="icon" href="/assets/favicon.ico" type="image/x-icon">
    <link rel="shortcut icon" href="/assets/favicon.ico" type="image/x-icon">

    <!-- CSS -->
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="stylesheet" href="/assets/css/syntax.css">

    <!-- Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css">

    <link href="/_pagefind/pagefind-ui.css" rel="stylesheet">
    <script src="/_pagefind/pagefind-ui.js" type="text/javascript"></script>
    <div id="search"></div>
    <script src="https://cdn.counter.dev/script.js" data-id="83dc700a-b821-4e57-9425-02b8336a9456" data-utcoffset="0"></script>
    <script>
        window.addEventListener('DOMContentLoaded', (event) => {
            new PagefindUI({ element: "#search" });
        });
    </script>

    <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Replicate volume data from Digital Ocean to SolidFire and back | Acting Technologist</title>
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="Replicate volume data from Digital Ocean to SolidFire and back" />
<meta name="author" content="scaleoutSean" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Steps for moving or copying data from Digital Ocean to SolidFire and back" />
<meta property="og:description" content="Steps for moving or copying data from Digital Ocean to SolidFire and back" />
<link rel="canonical" href="https://scaleoutsean.github.io/2021/11/30/digital-ocean-volume-to-solidfire-volume-and-back.html" />
<meta property="og:url" content="https://scaleoutsean.github.io/2021/11/30/digital-ocean-volume-to-solidfire-volume-and-back.html" />
<meta property="og:site_name" content="Acting Technologist" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-11-30T00:00:00+08:00" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://scaleoutsean.github.io/2021/11/30/digital-ocean-volume-to-solidfire-volume-and-back.html"},"author":{"@type":"Person","name":"scaleoutSean"},"description":"Steps for moving or copying data from Digital Ocean to SolidFire and back","@type":"BlogPosting","url":"https://scaleoutsean.github.io/2021/11/30/digital-ocean-volume-to-solidfire-volume-and-back.html","headline":"Replicate volume data from Digital Ocean to SolidFire and back","dateModified":"2021-11-30T00:00:00+08:00","datePublished":"2021-11-30T00:00:00+08:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  </head>

<body>

  <div class="container">
    <header id="header">
	<div id = site_title>
		<a href="https://scaleoutsean.github.io/">
			<h2 style="font-size:1.8em;">Acting Technologist</h2>
		</a>
		
		<h3>
			civilizations are created by individuals
		</h3>
		
	</div>

	<div id="subheader">
		
		<nav class="pages">
<a href="/about.html">About</a>

<a href="/archive.html">Archive</a>

<a href="/categories/">Categories</a>

<a href="/projects.html">Projects</a>
</nav>
		
		
		<nav class="social">
			
  
    <a href="https://www.github.com/scaleoutsean" target="_blank" id="github"><i class="fab fa-github" aria-hidden="true"></i></a>
  
   

  
    <a href="https://twitter.com/scaleoutsean" target="_blank" id="twitter"><i class="fab fa-twitter" aria-hidden="true"></i></a>
  
   

  
  
    <a href="/feed.xml" target="_blank" id="rss"><i class="fas fa-rss" aria-hidden="true"></i></a>
   

		</nav>
		
	</div>
</header>


    <div class="post-container">
      <article id = "post">
        <h1 id = "post-title">Replicate volume data from Digital Ocean to SolidFire and back</h1>

  
  <div class = "post-info">
    <span>
        <i class="far fa-calendar" aria-hidden="true"></i> <span>30 Nov 2021</span> - <i class="far fa-clock"></i> 


  
  
    11 minute read
  

    </span>
  </div>
  
        <p>Digital Ocean is a no-nonsense cloud provider that you’ve probably heard of.</p>

<p>SolidFire and Digital Ocean users may sometime want to migrate volume data without using application-level replication. That may involve activities such as Backup &amp; Restore (each action on one side) or offline data synchronization or replication.</p>

<p>In this post I want to describe how to migrate or protect volume images from Digital Ocean to SolidFire. I already described the approach and demonstrated it <a href="/2021/05/08/revisiting-solidbackup/">SolidBackup</a> <a href="/2021/06/18/solidbackup-with-alternative-backup-clients">posts</a> and demos, so this post will be short and just focus on Digital Ocean-specific details as they apply to native Linux and Windows volumes (NTFS, ext4, etc.). If you have no idea what I’m talking about, it’s raw volume backups - please check the second link (SolidBackup with alternative backup clients) to see.</p>

<p>If you’re interested in file-level replication or backup-restore, that’s not Digital Ocean-specific so you can just search the Internet or check the first of those SolidBackup posts I mentioned above.</p>

<h2 id="volume-size-units-on-digital-ocean-and-solidfire">Volume size units on Digital Ocean and SolidFire</h2>

<p>This is one of the first things to check because when copying raw data from one volume to another, it’s best when they’re exactly the same.</p>

<p>Currently Digital Ocean volumes use binary units, so when you create a 50 “gig” volume, that volume is going to be 50*1024^3 bytes (50 GiB) large. <code class="language-plaintext highlighter-rouge">fdisk</code> output for a 50 GiB volume on Digital Ocean:</p>

<pre><code class="language-raw">Disk /dev/sda: 50 GiB, 53687091200 bytes, 104857600 sectors
Disk model: Volume          
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
</code></pre>

<p>Using their CLI, you could see that volume size GiB:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>doctl compute volume list | <span class="nb">grep</span> <span class="nt">-E</span> <span class="s1">'uploader|Droplet IDs'</span>
ID                                      Name                      Size      Region    Filesystem Type    Filesystem Label    Droplet IDs    Tags
e1fec965-42b2-11ec-ac33-0a58ac14a1d1    uploader                  50 GiB    sgp1      xfs                                    <span class="o">[</span>273172181]  
</code></pre></div></div>

<p>The SolidFire API in versions 12 and 11 uses the same units, so we just use the same size in bytes in <code class="language-plaintext highlighter-rouge">CreateVolume</code> (adjust account ID and QoS values as necessary; it appears 512 byte emulation is in place in Digital Ocean so we use it here as well):</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
    </span><span class="nl">"method"</span><span class="p">:</span><span class="w"> </span><span class="s2">"CreateVolume"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"params"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="nl">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"uploader"</span><span class="p">,</span><span class="w">
        </span><span class="nl">"accountID"</span><span class="p">:</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span><span class="w">
        </span><span class="nl">"totalSize"</span><span class="p">:</span><span class="w"> </span><span class="mi">53687091200</span><span class="p">,</span><span class="w">
        </span><span class="nl">"enable512e"</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span><span class="w">
        </span><span class="nl">"attributes"</span><span class="p">:</span><span class="w"> </span><span class="p">{},</span><span class="w">
        </span><span class="nl">"qos"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
            </span><span class="nl">"minIOPS"</span><span class="p">:</span><span class="w"> </span><span class="mi">100</span><span class="p">,</span><span class="w">
            </span><span class="nl">"maxIOPS"</span><span class="p">:</span><span class="w"> </span><span class="mi">300</span><span class="p">,</span><span class="w">
            </span><span class="nl">"burstIOPS"</span><span class="p">:</span><span class="w"> </span><span class="mi">500</span><span class="p">,</span><span class="w">
            </span><span class="nl">"burstTime"</span><span class="p">:</span><span class="w"> </span><span class="mi">60</span><span class="w">
        </span><span class="p">}</span><span class="w">
    </span><span class="p">},</span><span class="w">
    </span><span class="nl">"id"</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<h2 id="volume-sizing-and-resizing">Volume sizing and resizing</h2>

<p>In the example above we aim to have volumes with identical sizes.</p>

<p>Of course, if we were to copy just the content on file or application level, Destination would have to be large enough to fit all the data, but perhaps not as large as Source. But if you copy volume data byte for byte (image backup or image replication), then you’d prefer to have identical capacity because it’s easier and better and you can copy back and forth without issues (it’s possible to restore a volume image to a larger volume, but I don’t like the thought of doing that).</p>

<p>If you want to go <em>back and forth</em> over a medium term (say, you have a DR/BC setup for hybrid cloud) and volume sizes grow over time, you may use doctl <code class="language-plaintext highlighter-rouge">compute volume-action resize</code> and <code class="language-plaintext highlighter-rouge">ModifyVolume</code> method on SolidFire (you can also grow volume size from the Web UIs or CLIs) to handle that.</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
    </span><span class="nl">"method"</span><span class="p">:</span><span class="w"> </span><span class="s2">"ModifyVolume"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"params"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="nl">"volumeID"</span><span class="p">:</span><span class="w"> </span><span class="err">&lt;Volume</span><span class="w"> </span><span class="err">ID&gt;</span><span class="p">,</span><span class="w">
        </span><span class="nl">"totalSize"</span><span class="p">:</span><span class="w"> </span><span class="err">&lt;Optional</span><span class="w"> </span><span class="err">New</span><span class="w"> </span><span class="err">Volume</span><span class="w"> </span><span class="err">Size</span><span class="w"> </span><span class="err">in</span><span class="w"> </span><span class="err">Bytes&gt;</span><span class="p">,</span><span class="w">
        </span><span class="err">...</span><span class="w">
    </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>If you need or already have the volume size that one side cannot support, then volume image backup isn’t the way to go; you should probably use another approach to spread data across several volumes (see the SolidBackup read-me file).</p>

<h2 id="use-dotctl-to-clone-a-snapshot-and-attach-a-volume-to-droplet">Use <code class="language-plaintext highlighter-rouge">dotctl</code> to clone a snapshot and attach a volume to droplet</h2>

<p>Copying data from SolidFire to Digital Ocean would work as explained in the older SolidBackup posts linked above. What about copying volume images from Digital Ocean to SolidFire? Well, you would use the same backup utility that you prefer to leverage from SolidBackup. For example, Restic.</p>

<p>To get started, you’d take a snapshot of your Digital Ocean volume. For a crash-consistent snapshot, just do it online. For an application-consistent you’d probably have to run a step on the application side or OS level, take a snapshot with <code class="language-plaintext highlighter-rouge">doctl</code> and then resume normal processing in your application or OS. That’s off topic here so we’ll move on to taking a Digital Ocean snapshot:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>doctl compute volume snapshot e1fec965-42b2-11ec-ac33-0a58ac14a1d1 <span class="nt">--snapshot-desc</span> mig-to-sf <span class="nt">--snapshot-name</span> uploader-snap
</code></pre></div></div>

<p>You’ll get a new Snapshot with unique ID. Use <code class="language-plaintext highlighter-rouge">compute snapshot list</code> and <code class="language-plaintext highlighter-rouge">compute snapshot get</code> to find about this snapshot and check if it’s ready (seeing how slow that works even on SSDs, reminded me how nice Element OS is!).</p>

<p>Stand up a new VM (“droplet”), deploy and configure your backup utility and attach the snapshot to the VM:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">doctl compute droplet list | grep &lt;droplet-name&gt;</code> - find your Droplet ID</li>
  <li><code class="language-plaintext highlighter-rouge">doctl compute volume-action attach &lt;volume-id&gt; &lt;droplet-id&gt;</code> - attach a Volume snapshot to a Droplet</li>
</ul>

<p>That should be similar to this:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>doctl compute volume-action attach e9fec965-42b2-11ec-ac33-0a58ac14a1d1 283172781
</code></pre></div></div>

<h2 id="backup--restore-volume-image">Backup &amp; restore volume image</h2>

<p>With a clone of a snapshot attached to the new droplet, now you need to manually mount the volume, take image backup and restore it to an identically sized SolidFire volume. A more sophisticated approach would start a ready-made container or use cloud-init (can be provided during droplet creation) to do this automatically and clean up after itself.</p>

<p>For regular use we could maintain a pair of VPN-connected VMs on each side (SolidBackup on prem, Restic in the cloud) and stream 50GiB-sized images over VPN in both directions as necessary, without raw image files ever landing on disk (except when extracted to be written to devices).</p>

<p>Another option - that leaves a low-cost copy (also known as a backup) behind it - would be to first make a backup to an object store (S3 or similar) located in a different cloud region, then unmount and detach that volume from the droplet as well as delete the volume and the original snapshot if you no longer need it (either on SolidFire or Digital Ocean). An S3 “bucket” is the equivalent of an individual Space in Digital Ocean, so because some marketing guys got carried away now we have to explain how Digital Ocean’s object storage works…</p>

<h3 id="digital-ocean-spaces-they-have-a-different-term-for-everything">Digital Ocean Spaces: they have a different term for everything</h3>

<p>I installed <code class="language-plaintext highlighter-rouge">s3cmd</code> and configured it like this (I mention only the parts that I changed from defaults <em>and</em> that were different from what you might use with AWS S3 or NetApp StorageGRID):</p>

<pre><code class="language-raw">access_key  =        # &lt;= get it from DO UI/API
secret_key  =        # &lt;= get it from DO UI/API
bucket_location = US # &lt;= this was weird, but it worked with sgp1!
host_base   = sgp1.digitaloceanspaces.com           # &lt;= SGP1 location
host_bucket = %(bucket).sgp1.digitaloceanspaces.com # &lt;= A Space is really a bucket
</code></pre>

<p>Replace <code class="language-plaintext highlighter-rouge">sgp1</code> (Singapore 1) with your <code class="language-plaintext highlighter-rouge">${region}{region-number}</code> in both <code class="language-plaintext highlighter-rouge">host_base</code> and <code class="language-plaintext highlighter-rouge">host_bucket</code>). Assuming your Space (bucket) is <code class="language-plaintext highlighter-rouge">https://backup-bucket.sgp1.digitaloceanspaces.com</code>, you could work with it like so:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>s3cmd <span class="nb">ls </span>s3://
2021-11-09 08:36  s3://backup-bucket
2021-11-09 08:36  s3://random-crap
<span class="nv">$ </span>s3cmd put db-vol.tar.gz s3://backup-bucket/do-backups/
</code></pre></div></div>

<p>Wow, wow, wow - what’s that random crap? Glad you’ve asked!</p>

<p>It turns out when it comes to Spaces, sharing is caring: one set of credentials works for all buckets (Spaces) that belong to your account! As a consequence you <strong>should</strong> encrypt your backups (Restic and many backup and compression utilities can do that for you) if you have users with different access requirements.</p>

<p>After the above <code class="language-plaintext highlighter-rouge">PUT</code> completes you should be able to see the backup (aka “snapshot”, in Restic speak) with on-prem Restic (or other tool) on-prem to restore that big image to the SolidFire volume of the same size. Please check the SolidBackup posts for examples and demos.</p>

<h2 id="detach-and-delete-unnecessary-clone-volume">Detach and delete unnecessary clone volume</h2>

<p>After backup workflow is done we no longer need the volume we can detach it with <code class="language-plaintext highlighter-rouge">compute volume-action detach ${VolumeID} ${DropletID}</code>:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>doctl compute volume-action detach e9fec965-42b2-11ec-ac33-0a58ac14a1d1 283172781 
</code></pre></div></div>

<p>If you used the above process to restore data from a SolidFire clone in order to use it in Digital Ocean, now you could attach this volume to another Droplet ID where you want to access it. But if the detached volume is a clone volume created from a Droplet volume snapshot, you may want to delete it (and also the snapshot, perhaps):</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>doctl compute volume delete e9fec965-42b2-11ec-ac33-0a58ac14a1d1
</code></pre></div></div>

<h2 id="automate-volume-to-volume-replication">Automate volume-to-volume replication</h2>

<p>How can we automate set up of Digital Ocean &lt;=&gt; SolidFire volume pairings?</p>

<p>I haven’t had a reason to try, but here’s how I’d go about it if I needed to:</p>

<ul>
  <li>Use Digital Ocean and SolidFire provider for HashiCorp Terraform to create volume pairs, or</li>
  <li>Use Ansible with Digital Ocean (community.digitalocean) and SolidFire (netapp.elementsw), or</li>
  <li>Use SolidFire Tools for PowerShell and doctl (Digital Ocean CLI) with solidbackup, or</li>
  <li>Combination of the above</li>
</ul>

<p><a href="https://github.com/scaleoutsean/solidbackup">SolidBackup</a> already leverages Ansible and it should be easy to modify the backup binary to use a binary rsync tool such as diskrsync. Additional role for verification could be added if desired.</p>

<h2 id="demo">Demo</h2>

<ul>
  <li>Create 1GiB volume on SolidFire and Digital Ocean (whole GibiByte units are identical, so if you use those, you don’t need to convert them)</li>
  <li>Create a small VM on each side (512MiB or 1GiB RAM is enough)</li>
  <li>Install a binary sync utility</li>
  <li>Create filesystem and a 100MiB file at Source (VM attached to SolidFire volume), then unmount the volume (normally we’d make a clone and sync data from a clone as we do in solidbackup)</li>
  <li>Sync Source volume with Destination volume attached to (but not mounted on) a VM in Digital Ocean</li>
  <li>At both Source and Destination, the entire volume will be read and chunks of it checksumed and compared. Any difference will be compressed and transferred over SSH</li>
  <li>Activity on Source VM (constrained by read speed (20 MB/s) in SolidFire Demo VM) as we begin:</li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>----system---- -dsk/total- --total-cpu-usage--
     time     | read  writ|usr sys idl wai stl
20-02 15:22:18|  52k   32k|  1   0  99   0   0
20-02 15:22:19|  19M 8192B|  2   0  97   1   0
20-02 15:22:20|  18M   16k|  1   1  88  11   0
20-02 15:22:21|  19M  460k|  1   0  88  10   0
20-02 15:22:22|  18M   52k|  1   0  89  10   0
20-02 15:22:23|  19M    0 |  1   0  88  10   0
20-02 15:22:24|  20M 8192B|  1   1  89  10   0
</code></pre></div></div>

<ul>
  <li>Activity at Destination; the entire volume is read quickly (300 MB/s with 1 CPU only 50% busy)</li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>----system---- -net/total- -dsk/total- --total-cpu-usage--
     time     | recv  send| read  writ|usr sys idl wai stl
20-02 07:22:18|4193B 3501B|  16k   20k|  3   3  94   0   0
20-02 07:22:19|1550B 1996B|  92M    0 | 55  21  21   2   1
20-02 07:22:20| 132B  396B| 300M    0 | 46  23   0  32   0
20-02 07:22:21| 132B  396B| 300M    0 | 45  22   0  33   0
20-02 07:22:22| 132B  396B| 300M    0 | 50  20   0  29   1
20-02 07:22:23| 198B  562B|  31M    0 |  4   4  91   1   0
20-02 07:22:24| 132B  396B|   0    44k|  0   0  99   1   0
</code></pre></div></div>

<ul>
  <li>After both volumes are read, checksums are compared and only compressed difference between 1MiB chunks is transferred. In this case it was a resync with a small differential so network transfer to Digital Ocean was very small</li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>----system---- -dsk/total- --total-cpu-usage--
     time     | read  writ|usr sys idl wai stl
20-02 15:23:13| 188k    0 |  0   0 100   0   0
20-02 15:23:14|   0  8192B|  0   0  99   0   0
20-02 15:23:15|   0    24k|  0   0 100   0   0
20-02 15:23:16|   0    16k|  1   0  99   0   0
20-02 15:23:17|   0    28k|  0   0  99   0   0
20-02 15:23:18|   0     0 |  0   1  99   0   0
20-02 15:23:19|   0  8192B|  1   0  99   0   0
</code></pre></div></div>

<ul>
  <li>To verify correctness of replication, we mount and checksum the files at source and destination to ensure they match</li>
  <li>Digital Ocean VMs are billed even when powered off, so you may as well leave the “sync VM” on. If you need to failover to the cloud, detach the volumes from the “sync VM” and attach to your workload VM (<code class="language-plaintext highlighter-rouge">doctl compute volume-action detach|attach</code> takes less than a minute)</li>
</ul>

<h2 id="demo-1">Demo</h2>

<ul>
  <li><a href="https://rumble.com/vvhqjz-binary-replication-of-solidfire-volumes.html">Binary sync of SolidFire volume to Digital Ocean volume</a> - 3m35s</li>
</ul>

<h2 id="conclusion">Conclusion</h2>

<p>The approach taken by SolidBackup works with both Digital Ocean virtualization and object storage, which was one of the reasons it was made that way (general purpose, open approach).</p>

<p>However, both the Digital Ocean CLI (<code class="language-plaintext highlighter-rouge">doctl</code>) and Spaces take some getting used to - it took me too much time to find the exact commands and ways to configure Spaces - so I hope this post will save some time to those who read it.</p>

<p>The entire process is simple and repetitive enough to automate with mainstream automation tools and languages.</p>

      </article>
    </div>
    
      <div class="categories">
    <span><p>Categories:</p>
    
    
      <a href="
      /categories/#solidfire">solidfire</a>
      &nbsp; 
    
      <a href="
      /categories/#cloud">cloud</a>
      &nbsp; 
    
      <a href="
      /categories/#automation">automation</a>
       
    
  </span>
</div>
    

  
    <div>
      <h3>Related Posts</h3>
      <ul>
      
        <li><a href="/2023/08/30/monitoring-solidfire-clone-and-backup-jobs.html">Use SolidFire API to monitor clone and backup jobs for profit and pleasure</a></li>
      
        <li><a href="/2021/12/22/rsync-with-storagegrid.html">Using rclone to copy StorageGRID S3 data to local filesystem</a></li>
      
        <li><a href="/2021/04/29/storagegrid-cloudmirror-async-replication-to-remote-s3-bucket.html">Using StorageGRID CloudMirror to replicate objects to remote S3-compatible bucket</a></li>
      
        <li><a href="/2023/01/19/containerized-netapp-cloudsync.html">Containerized Cloud Sync Data Broker for Docker, Kubernetes and Nomad</a></li>
      
        <li><a href="/2024/07/05/kubefire-for-failover-failback-of-kubernetes-with-solidfire-backend.html">Kubefire - tools and recipes for Kubernetes failover/failback with SolidFire</a></li>
      
      </ul>
    </div>
  

    
  </div><footer class= "footer">
    <p>2025-06-30 17:37 </p>
    <p>Copyright © 2025 scaleoutSean. Content is released under the CC BY license. Design: Alessio Franceschi</p>
</footer>

</body>
</html>
