<!doctype html>
<html lang="en">
  <head>
        <meta charset="UTF-8">
    <meta http-equiv="content-language" content="en">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="googlebot" content="noarchive">
    <meta name="googlebot" content="max-snippet:60">
    <meta name="googlebot" content="max-image-preview:small">
    <meta name="bingbot" content="noarchive">
    <meta name="bingbot" content="max-snippet:60">
    <meta name="bingbot" content="max-image-preview:small">
    <meta name="robots" content="noarchive">
    <meta name="robots" content="max-snippet:60">
    <meta name="google-site-verification" content="F6q7vIwQ2G0j8tk-KL9rAXOMLXMDMMUkEz4fRs1Nnnc" />
    <title>
        
            ThinkParQ BeeGFS v8 with NetApp E-Series | Acting Technologist
      
    </title>
    <meta name="description" content="
     How to get most out of BeeGFS v8 with NetApp E-Series
     ">

    <!-- LINK TO ATOM FEED FOR SEO -->
    <link rel="alternate" type="application/atom+xml" href="https://scaleoutsean.github.io/feed.xml" />

    <!-- FAVICON -->
    <link rel="icon" href="/assets/favicon.ico" type="image/x-icon">
    <link rel="shortcut icon" href="/assets/favicon.ico" type="image/x-icon">

    <!-- CSS -->
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="stylesheet" href="/assets/css/syntax.css">

    <!-- Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css">

    <link href="/_pagefind/pagefind-ui.css" rel="stylesheet">
    <script src="/_pagefind/pagefind-ui.js" type="text/javascript"></script>
    <div id="search"></div>
    <script src="https://cdn.counter.dev/script.js" data-id="83dc700a-b821-4e57-9425-02b8336a9456" data-utcoffset="0"></script>
    <script>
        window.addEventListener('DOMContentLoaded', (event) => {
            new PagefindUI({ element: "#search" });
        });
    </script>

    <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>ThinkParQ BeeGFS v8 with NetApp E-Series | Acting Technologist</title>
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="ThinkParQ BeeGFS v8 with NetApp E-Series" />
<meta name="author" content="scaleoutSean" />
<meta property="og:locale" content="en" />
<meta name="description" content="How to get most out of BeeGFS v8 with NetApp E-Series" />
<meta property="og:description" content="How to get most out of BeeGFS v8 with NetApp E-Series" />
<link rel="canonical" href="https://scaleoutsean.github.io/2025/05/17/beegfs-v8-netapp-e-series-indexing-tiering-workflows.html" />
<meta property="og:url" content="https://scaleoutsean.github.io/2025/05/17/beegfs-v8-netapp-e-series-indexing-tiering-workflows.html" />
<meta property="og:site_name" content="Acting Technologist" />
<meta property="og:image" content="https://scaleoutsean.github.io/assets/images/beegfs-8-pools.svg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-05-17T00:00:00+08:00" />
<script type="application/ld+json">
{"image":"https://scaleoutsean.github.io/assets/images/beegfs-8-pools.svg","mainEntityOfPage":{"@type":"WebPage","@id":"https://scaleoutsean.github.io/2025/05/17/beegfs-v8-netapp-e-series-indexing-tiering-workflows.html"},"author":{"@type":"Person","name":"scaleoutSean"},"description":"How to get most out of BeeGFS v8 with NetApp E-Series","@type":"BlogPosting","url":"https://scaleoutsean.github.io/2025/05/17/beegfs-v8-netapp-e-series-indexing-tiering-workflows.html","headline":"ThinkParQ BeeGFS v8 with NetApp E-Series","dateModified":"2025-05-17T00:00:00+08:00","datePublished":"2025-05-17T00:00:00+08:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  </head>

<body>

  <div class="container">
    <header id="header">
	<div id = site_title>
		<a href="https://scaleoutsean.github.io/">
			<h2 style="font-size:1.8em;">Acting Technologist</h2>
		</a>
		
		<h3>
			civilizations are created by individuals
		</h3>
		
	</div>

	<div id="subheader">
		
		<nav class="pages">
<a href="/about.html">About</a>

<a href="/archive.html">Archive</a>

<a href="/categories/">Categories</a>

<a href="/projects.html">Projects</a>
</nav>
		
		
		<nav class="social">
			
  
    <a href="https://www.github.com/scaleoutsean" target="_blank" id="github"><i class="fab fa-github" aria-hidden="true"></i></a>
  
   

  
    <a href="https://twitter.com/scaleoutsean" target="_blank" id="twitter"><i class="fab fa-twitter" aria-hidden="true"></i></a>
  
   

  
  
    <a href="/feed.xml" target="_blank" id="rss"><i class="fas fa-rss" aria-hidden="true"></i></a>
   

		</nav>
		
	</div>
</header>


    <div class="post-container">
      <article id = "post">
        <h1 id = "post-title">ThinkParQ BeeGFS v8 with NetApp E-Series</h1>

  
  <div class = "post-info">
    <span>
        <i class="far fa-calendar" aria-hidden="true"></i> <span>17 May 2025</span> - <i class="far fa-clock"></i> 


  
  
    17 minute read
  

    </span>
  </div>
  
        <ul>
  <li><a href="#introducti">Introducti</a></li>
  <li><a href="#data-tiering-with-pools">Data tiering with pools</a></li>
  <li><a href="#zfs">ZFS</a>
    <ul>
      <li><a href="#beegfs-snapshots">BeeGFS snapshots</a>
        <ul>
          <li><a href="#cognitive-overhead-of-snapshots">Cognitive overhead of snapshots</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#remote-storage">Remote storage</a></li>
  <li><a href="#file-system-events">File-system events</a></li>
  <li><a href="#file-index">File index</a></li>
  <li><a href="#monitoring">Monitoring</a></li>
  <li><a href="#conclusion">Conclusion</a></li>
</ul>

<h2 id="introducti">Introducti</h2>

<h2 id="data-tiering-with-pools">Data tiering with pools</h2>

<p>This isn’t a new feature in version 8, but even if I’ve mentioned it before, it must have been briefly.</p>

<p>So, what’s the big deal?</p>

<ul>
  <li>A BeeGFS pool is one or more storage target (aka LUN) tagged for specific property (cost, performance, location, etc.)</li>
  <li>One can have a BeeGFS file-system with more than one pool</li>
  <li>Furthermore, one can easily move data (files or entire directory trees) between any two pools without stubs, shortcuts or other major inconveniences for users</li>
</ul>

<p>As you guess, that allows us to mix different disk types or even arrays in the same BeeGFS file-system.</p>

<p>As an example, we could use high performance arrays (or media) in with R10 (for metadata) or R6 (for data) volumes and lower-cost QLC-based DDP-based LUNs in the same BeeGFS file-system.</p>

<p>Tiering is performed with a simple command that moves files from source-tagged LUNs to target-tagged LUNs (copy data from source to destination, redirect metadata to destination, delete source data when done).</p>

<p><img src="/assets/images/beegfs-8-pools.svg" alt="BeeGFS pools with EF600 and EF600C" /></p>

<p>BeeGFS file-system is literally (and in the schematic) a black box; file (chunk) location is obfuscated from the user. Lateral movement (tiering) moves files from one pool to another, but their file-system path remains the same.</p>

<p>We can get as fancy as we want - we could use one hybrid array (e.g. EF300) for everything (R10 TLC NVMe for MD, R6 TLC NVMe for Hot Data, R6 HDD for Cold Data), for example.</p>

<p>Just remember to watch pool and target fullness in order to avoid blowing up a pool. You also don’t want to be too stingy and having to waste IO on moving data back and forth all day long.</p>

<p>This example below shows a BeeGFS file-system with 2 data disks: target_0-6826D866-1 (Tier 1) and target_1-6826D866-1 (Tier 2). When creating pools (<code class="language-plaintext highlighter-rouge">beegfs pool create</code>), target_1-6826D866-1 was assigned to <code class="language-plaintext highlighter-rouge">archive</code> tier. (I should have aliased the targets better…)</p>

<p><img src="/assets/images/beegfs-8-pools-01-storage.png" alt="BeeGFS and E-Series Pools" /></p>

<p>After giving the storage targets better aliases:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>beegfs target list
ID     TYPE     ALIAS           NODE  STORAGE_POOL  
s:101  storage  s101_default_0  s:1   s:1           
s:102  storage  s102_archive_0  s:1   s:2           
m:1    meta     target_meta_1   m:1   <span class="o">(</span>n/a<span class="o">)</span>         

<span class="nv">$ </span><span class="nb">sudo </span>beegfs pool list
ID   ALIAS                 TARGETS  MIRRORS  
s:1  storage_pool_default  s:101             
s:2  archive               s:102  
</code></pre></div></div>

<p>By default, new files land on <code class="language-plaintext highlighter-rouge">storage_pool_default</code> constituents (before target_0-6826D866-1 and now s101_default_0, as I have just one storage target in the default pool, but normally we’d have 4 or more).</p>

<p>If I examine this new file, I can see its location is as expected.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>beegfs entry info /mnt/beegfs/archive/flights-1m_v2.csv 
PATH                        ENTRY_ID      TYPE  META_NODE  META_MIRROR   STORAGE_POOL              STRIPE_PATTERN  STORAGE_TARGETS  STORAGE_MIRRORS  REMOTE_TARGETS  
/archive/flights-1m_v2.csv  4-68289A9B-1  file  m:1        <span class="o">(</span>unmirrored<span class="o">)</span>  storage_pool_default <span class="o">(</span>1<span class="o">)</span>  RAID0 <span class="o">(</span>4x512K<span class="o">)</span>  s:101            <span class="o">(</span>unmirrored<span class="o">)</span>     <span class="o">(</span>none<span class="o">)</span>          
</code></pre></div></div>

<p>When the file is no longer actively used (more on that later), I can move it to archive tier.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>beegfs entry migrate <span class="nt">--from-pools</span><span class="o">=</span>storage_pool_default <span class="nt">--pool</span><span class="o">=</span>archive /mnt/beegfs/archive/flights-1m_v3.csv 
Summary: <span class="o">{</span>MigrationStatusUnknown:0 MigrationErrors:0 MigrationNotSupported:0 MigrationSkippedDirs:0 MigrationNotNeeded:0 MigrationNeeded:0 MigratedFiles:1 MigrationUpdatedDirs:0<span class="o">}</span>
</code></pre></div></div>

<p>Don’t be confused by its path (/mnt/beegfs/archive/), which doesn’t change throughout. I could have had the file in /mnt/beegfs/, and later move it to /mnt/beegfs/archive/ - that <em>still</em> wouldn’t move it to the archive tier. What moves it to another tier (pool) is the <code class="language-plaintext highlighter-rouge">entry migrate</code> command.</p>

<p>If it’d be less confusing this way, imagine having a workflow like this (recursive options are possible as well):</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/usr/bin/bash</span>
<span class="c"># Gets relative path of a file, moves it to /mnt/beegfs/archive/ and tiers to archive pool</span>
<span class="nb">read </span>myfile
<span class="nb">mv</span> /mnt/beegfs/<span class="k">${</span><span class="nv">myfile</span><span class="k">}</span> /mnt/beegfs/archive/<span class="k">${</span><span class="nv">myfile</span><span class="k">}</span>
<span class="nb">sudo </span>beegfs entry migrate <span class="se">\</span>
    <span class="nt">--from-pools</span><span class="o">=</span>storage_pool_default <span class="se">\</span>
    <span class="nt">--pool</span><span class="o">=</span>archive <span class="se">\</span>
    /mnt/beegfs/archive/<span class="k">${</span><span class="nv">myfile</span><span class="k">}</span>
</code></pre></div></div>

<p>Of course, you wouldn’t <em>really</em> have scripts like these; you’d have this in your workflows, job scripts or some other place that runs them automatically.</p>

<p>One of the reasons for that is tiered files aren’t supposed to be open (modified) during migration, so the best time to run them is right after jobs or workflow steps so that tiering doesn’t become a burden. (Tiering up - to a higher tier - could be done <em>before</em> jobs or workflows start, of course.)</p>

<p>After entry migration, we can see that STORAGE_POOL is now <code class="language-plaintext highlighter-rouge">archive</code>.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>beegfs entry info /mnt/beegfs/archive/flights-1m_v3.csv 
PATH                        ENTRY_ID      TYPE  META_NODE  META_MIRROR   STORAGE_POOL  STRIPE_PATTERN  STORAGE_TARGETS  STORAGE_MIRRORS  REMOTE_TARGETS  
/archive/flights-1m_v3.csv  1-68289D0B-1  file  m:1        <span class="o">(</span>unmirrored<span class="o">)</span>  archive <span class="o">(</span>2<span class="o">)</span>   RAID0 <span class="o">(</span>4x512K<span class="o">)</span>  s:102            <span class="o">(</span>unmirrored<span class="o">)</span>     <span class="o">(</span>none<span class="o">)</span>    
</code></pre></div></div>

<p>Related to this, I’d also like to say a thing or two about ZFS.</p>

<h2 id="zfs">ZFS</h2>

<p>BeeGFS supports ZFS (as well as other file systems) on Data disks and I’ve mentioned this on several occasions. I’ve also blogged about ZFS on E-Series in the context of LXD and Incus, so if you’re interested in ZFS deduplication, compression, and such, check blog archives or use the search feature to find those posts.</p>

<p>Here I’ll just point out that the screenshot above uses BeeGFS with ZFS metadata and data disks.</p>

<p>It’s important to remember that you won’t get this from NetApp, as the BeeGFS solution from NetApp deploys vanilla file-systems (ext4, XFS), so you’d have to modify the Ansible deployment scripts to get this done.</p>

<p>ThinkParQ does support ZFS and the pools feature requires Enterprise Edition features, which means you’d have this covered anyway. Just make sure you RTFM (BeeGFS v8) related to ZFS.</p>

<p>Back to the screnshot:</p>

<ul>
  <li>I used Ubuntu 24.04 with self-built ZFS 2.3.0 which is the version with smart(er) deduplication (I blogged about it with E-Series before it was released, so I won’t repeat any of that)</li>
  <li><code class="language-plaintext highlighter-rouge">storage_pool_default</code> pool: as this is my Tier 1 I have no compression, no dedupe set</li>
  <li><code class="language-plaintext highlighter-rouge">archive</code> pool: this is Tier 2 and both compression and dedupe are enabled</li>
</ul>

<p>Is ZFS dedupe super-smart? Not really, but it’s good enough for Tier 3. Identical files will be deduplicated. Partial matches should be as well, but “it depends”. The compression was and is legitimate, and you can pick any of several methods. I used ZSTD and LZ4 with BeeGFS but didn’t compare (savings shouldn’t be BeeGFS-specific).</p>

<p>We could have several archives with different (combinations of) settings, but it’s better to not fragment your storage pools unless your data is very predictable in terms of data types and capacity.</p>

<h3 id="beegfs-snapshots">BeeGFS snapshots</h3>

<p>Snapshots with ZFS are possible, but likely impractical. There’s no way to “quiesce” a live BeeGFS file-system, so the only sure way would be to stop services (mostly <code class="language-plaintext highlighter-rouge">beegfs-client</code>, but preferrably all others as well) and then take a snapshot. This is how E-Series (SANtricity) snapshots could work as well, of course.</p>

<p>It’s usually not possible to stop services, which is why I say snapshots without BeeGFS cooperation aren’t practical, but you may be able to do that on some file-systems (e.g. file-system used for home directories at 6am). Once snapshots are taken, they can be restored when BeeGFS is stopped.</p>

<p><img src="/assets/images/beegfs-8-zfs-01-snapshot.png" alt="BeeGFS with ZFS snapshots" /></p>

<ul>
  <li>(1) we delete files to simulate damage</li>
  <li>(2) stop BeeGFS services (at last beegfs-client, beegfs-meta, beegfs-storage, beegfs-sync)</li>
  <li>(3) we rollback all constituent volumes to last ZFS snapshot</li>
  <li>(4) BeeGFS services are started and all files from the third snapshot are back</li>
</ul>

<p>Following these steps I also executed BeeGFS <code class="language-plaintext highlighter-rouge">fsck</code> which found 0 errors on the restored filesystem.</p>

<p>The snapshot “third” is the one that was restored. (In the case you’re wondering where’s “second”; it was disappeared earlier when, having taken snapshot “second”, I rolled back to “first”.)</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>zfs list <span class="nt">-t</span> snapshot
NAME        USED  AVAIL  REFER  MOUNTPOINT
md1@first   230K      -  5.10M  -
md1@third   210K      -  5.10M  -
zd1@first    20K      -  38.7M  -
zd1@third  20.5K      -  38.7M  -
zd2@first    19K      -  22.6M  -
zd2@third    19K      -  22.6M  -

</code></pre></div></div>

<p>Note that stopping services in order to snapshot and/or rollback snapshots may have unpredictable consequences, e.g. on Hive indexes and file-system notifications. Some of these potential issues aren’t even necessarily related to snapshots (e.g. some notifications may fail to be delivered).</p>

<p>If these compromises and risks are acceptable, ZFS replication can also be used once we have snapshots to work with. Note that - unlike in regular stand-alone ZFS pools, here each metadata and storage target are stand-alone practically “R0-style” zpools and at the same time mutually dependent because they need to be “re-assembled” and imported into a BeeGFS cluster on destination BeeGFS cluster. Because that’s not trivial and because I don’t think many people would want to do this, I won’t attempt to do that in this post.</p>

<h4 id="cognitive-overhead-of-snapshots">Cognitive overhead of snapshots</h4>

<p>As an aside and something that I may revisit in a future post: recently, I’ve been thinking about the applicability of snapshots to modern data services. Assuming one can rollback a snapshot of metadata and data targets as well as any index, vector and other databases, that would seem useful only in limited and isolated cases such as small scale prototyping. Any data that has been fed or “propagated” to other systems is “spilled milk”.</p>

<p>It may be useful to restore a mistakenly deleted file or directory, but potentially at the cost of losing updates or new files created since the time of snapshot creation. In rare cases it may be possible to take a pre-recovery snapshot and use that to update a restored snapshot, but as I’ve mentioned earlier - is that really practical on large file-systems?</p>

<p>How many administrators are capable of understanding what rolling back a snapshot on a 10 PB file-system means and how that impacts various Hive indexes, external databases and downstream data consumers?</p>

<p>If I needed to design a snapshottable BeeGFS file-system attached to E-Series, I’d create a file-system out of single-array LUNs so that I can take (and restore) snapshots using SANtricty Consistency Groups. In that way, snapshots of small (few TB) to medium sized (hundred TB) file-systems could be created regardless of file-system type on data targets. They would be limited by the performance of single E-Series array, but given their size and nature (prototyping, etc.), that could suffice for most basic rollbacks.</p>

<p>Periodic BeeGFS sync to S3 with S3 versioning may sometimes be a better choice: there’s no need for file-system and storage administrator intervention, and all uploaded versions are accessible to the user. (BeeGFS sync in v8.0 doesn’t have the ability to use object versions, but if they bucket has them enabled, they should be available.)</p>

<h2 id="remote-storage">Remote storage</h2>

<p>This feature allows us to define Remote Storage Targets (RSTs) which are generally S3 buckets.</p>

<p>Then we can configure push and pull to/from those RSTs.</p>

<p>I’ve been having problems configuring these, so I don’t have much to say about them except that earlier I assumed pushing to an RST would evacuate data from BeeGFS. That doesn’t seem to be the case. It merely <em>copies</em> data to an RST. Think of it as a tool for push/pull data exchange.</p>

<p>Originally I had hoped one would be able to delete pushed data from the source and pull it back if and when needed, but that doesn’t seem to be a use case here. It’s more like pull/push in <a href="https://github.com/NetApp/netapp-dataops-toolkit/tree/main/netapp_dataops_traditional#cli-push-to-s3-directory">NetApp DataOps Toolkit</a> except that it should be much faster.</p>

<p>In terms of RST support, NetApp StorageGRID and other S3-like services (MinIO, Versity S3 Gateway) can use E-Series storage (I’ve blogged about them in several <a href="/2023/09/03/minio-erasure-coding-and-netapp-e-series.html">posts</a>). MinIO and Versity can run in Kubernetes with E-Series as well. I think <a href="/2022/07/06/apache-ozone-netapp-eseries.html">Apache Ozone</a> should work just fine. You could have any of these on a server (or three, especially with K8s and/or EC) attached to EF600 with NL-SAS. PBs of S3 at a low cost and headache-free management.</p>

<p>Note that for ZFS, <code class="language-plaintext highlighter-rouge">beegfs entry refresh</code> must be executed before <code class="language-plaintext highlighter-rouge">beegfs remote push</code>, so that ZFS updates sizes of new and modified files.</p>

<h2 id="file-system-events">File-system events</h2>

<p>This also isn’t a new feature, but it’s been reworked for v8.</p>

<p>Why should we care?</p>

<p>You may have heard of AI. Or ransomware. Or workflows. These are some situations where file-system events can be useful. For the ONTAP people, FPolicy may come to mind. Or inotify, in the world of Linux.</p>

<p>In essence, BeeGFS can send them to two places: a gRPC listener (new) and UNIX socket (legacy).</p>

<p>Using the legacy approach, run beegfs-event-listener on the socket “file” <code class="language-plaintext highlighter-rouge">beegfs-watch</code> service sends data to. Example:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/opt/beegfs/sbin/beegfs-event-listener /run/beegfs/eventlog
</code></pre></div></div>

<p>Then, as stuff happens on file-system, event listener spits out FS events it reads from the socket.</p>

<p><img src="/assets/images/beegfs-8-events-01-legacy.png" alt="Legacy file-system events " /></p>

<p>Now we just need to decide what we want to do with this.</p>

<p>For example, <code class="language-plaintext highlighter-rouge">LastWriterClosed</code> on <code class="language-plaintext highlighter-rouge">/mnt/beegfs/incoming/*.mp4</code> may be the thing we’re looking for. Then we kick off <code class="language-plaintext highlighter-rouge">ffmpeg</code> and let it do its thing on the file and use RST to push processed videos out to an S3 bucket.</p>

<p>The BeeGFS 8 documentation gives an example of piping those events to a user script:</p>

<p><img src="/assets/images/beegfs-8-events-02-legacy-pipe.png" alt="Legacy file-system events piped to a script" /></p>

<p>This is a basic example, though. Normally we’d filter by event type and do things with that:</p>

<ul>
  <li>Run anti-virus scanner (ClamAV, for example)</li>
  <li>Run various data processing</li>
</ul>

<p>For media, a data processing step could be format conversion (I <a href="/2022/04/05/nomad-beegfs-eseries.html">blogged about that</a> in 2022).</p>

<p>For AI, data processing steps could be frame-by-frame analysis and tagging. Or you could automatically create file embeddings in a (vector) database.</p>

<p>For SIEM, data processing steps could be parsing and ingress of <code class="language-plaintext highlighter-rouge">.log</code> files to Elasticsearch.</p>

<p>For anti-ransomware and anti-virus, we could run a scanner, but we could also build own detection of suspicious file-system activity. More on that in the next section. Here I’ll just say <a href="/2024/01/29/antivirus-scanning-for-on-premises-s3.html">I blogged</a> about AV scanning and getting an up-to-date list of objects required external services (Elasticsearch, Kafka, etc).</p>

<p>The watcher example from the documentation can be easily adjusted to initiate data pipeline processing along the lines of what I mentioned above: depending on document extension (and/or additional criteria such as path matching, owner and similar), new files can be processed as soon as last writer is done writing to the file.</p>

<p><img src="/assets/images/beegfs-8-events-03-decider.gif" alt="Legacy file-system events for data pipeline processing" /></p>

<p>While the modern (gRPC) BeeGFS file-system event listener service is robust, even this basic event processor would suffice for many use cases and can be created in minutes.</p>

<p>You may wonder where are these tasks and steps scheduled. Unlike on some highly-integrated storage systems (e.g. Vast Data), there’s no built-in generic message queue so we’d need to run our own service on BeeGFS or non-BeeGFS volumes - some extra work is required in exchange for flexibility and openness. Such services could run <a href="/2022/08/11/nomad-pack-influxdb-beegfs.html">on BeeGFS</a> or elsewhere (e.g. services in <a href="/2022/12/09/directpv-topolvm-csi-lvm-das-k8s-with-eseries.html">Kubernetes on non-BeeGFS volumes</a>).</p>

<p>Supported BeeGFS file-system events are: flush, close, trunc, setattr, link-op, open-read, open-write, open-readwrite.</p>

<h2 id="file-index">File index</h2>

<p>You can <a href="https://doc.beegfs.io/8.0/hive/hive_index.html#beegfs-hive-index">read about it in TFM</a>. My <code class="language-plaintext highlighter-rouge">tldr</code>:</p>

<ul>
  <li>Scans MD and stores file metadata into a database file</li>
  <li>BeeGFS Hive Index can be stored in-tree or out-of-tree (i.e. you can keep it on some NFS share or a non-BeeGFS FS on R10)</li>
  <li>SQL qeueries can be issued against this DB. They’re much faster than doing the same thing against file-system.</li>
</ul>

<p>BeeGFS Hive database(s) are SQLite3 databases and the index path and other details are configured in /etc/beegfs/index/config. For example, my BeeGFS file-system has just one directory (/mnt/beegfs/archive) and accordingly there are only 2 index databases (one for file-system “root” and another for the archive directory).</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">dir</span> <span class="nt">-laR</span> /work/index
/work/index:
total 60
drwxrwxrwx 3 root root    4096 May 17 14:43 <span class="nb">.</span>
drwxr-xr-x 3 root root    4096 May 17 07:29 ..
drwxrwxr-x 2 sean sean    4096 May 17 14:43 archive
<span class="nt">-rw-r--r--</span> 1 root root   16384 May 17 14:43 .bdm.db
<span class="nt">-rw-r--r--</span> 1 sean beegfs 32768 May 17 14:44 .bdm.db-shm
<span class="nt">-rw-r--r--</span> 1 sean beegfs     0 May 17 14:43 .bdm.db-wal

/work/index/archive:
total 20
drwxrwxr-x 2 sean sean  4096 May 17 14:43 <span class="nb">.</span>
drwxrwxrwx 3 root root  4096 May 17 14:43 ..
<span class="nt">-rw-r--r--</span> 1 sean sean 12288 May 17 14:43 .bdm.db
</code></pre></div></div>

<p>You may wonder if these are populated on-the-fly by BeeGFS Watch service. Not yet. It’s planned, though.</p>

<p>Because there may be billions of files and directories, that could represent a significant burden for the file-system. Currently you need to create (or refresh) an index. Refreshes can be done overnight, for example.</p>

<p>This screenshot shows index creation and a way to query entries.</p>

<p><img src="/assets/images/beegfs-8-events-01-index.png" alt="Query BeeGFS Hive index" /></p>

<p>Unfortunately there’s no JSON output (yet), so unlike with FS events (which we saw come as JSON documents) you’d have to create a wrapper or store output to CSV and import CSV in another step.</p>

<p>When watch and when Hive? I guess “it depends”. If I can, I’d always prefer to have as much as possible done in Hive because that’s “cheaper” in terms of resources.</p>

<p>For example, let’s say we want to index content of text files to be able to perform full text and vector similarity search:</p>
<ul>
  <li>For SIEM use cases, we’d want it done with Watch service because it’s important to spot problems quickly</li>
  <li>For RAG, maybe it’s not essential to have content indexed in near real-time and so overnight (or hourly) recursive refresh of a bunch of directories followed by a SQL query may be a better approach</li>
</ul>

<p>One popular use case for Hive index is deletion of files not accessed for more than X days from Tier 1 when Tier 1 is used exclusively for scratch space.</p>

<h2 id="monitoring">Monitoring</h2>

<p>This isn’t new, but it’s been improved. InfluxDB version 1.x is one of two supported InfluxDB editions and - as I said <a href="/2025/01/24/influxdb-3-core-alpha.html">here</a> - that means version 3 is supported as well.</p>

<p>My advice would be to use InfluxDB 3 OSS and configure BeeGFS v8 for InfluxDB version 1.x as per <a href="https://doc.beegfs.io/8.0/advanced_topics/mon.html#configuration">TFM</a>. I haven’t tried this myself (it’s on my TODO list), but there’s no reason this wouldn’t work.</p>

<h2 id="conclusion">Conclusion</h2>

<p>BeeGFS version 8 contains some great improvements that I haven’t even covered in this blog post. Some other features, added in recent years, have been integrated and improved, and yet others (monitoring) practically resurrected due to the appearance of InfluxDB 3 OSS.</p>

<p>BeeGFS Watch (file-system event monitoring) and Hive Index are very relevant in the world of AI and analytics, so I gave more space to these features. From the examples you can see there are multiple ways to process new data using these two features. Support for out-of-tree indexes, an open DB format and RST means BeeGFS name spaces are fully open to integrations <em>across file-systems, storage environments and locations</em>:</p>

<ul>
  <li>place indexes on non-BeeGFS file-systems or NFS shares</li>
  <li>send file-system modifications updates to external gRPC listeners or process updates in own scripts/services</li>
  <li>push BeeGFS files or directory trees to any standard S3-like RST</li>
  <li>process files on BeeGFS or on S3 (on-premises or in the cloud)</li>
</ul>

<p>Note that files <strong>do not</strong> necessarily need to be uploaded or copied to S3 for processing via S3. You may use <a href="/2023/09/20/versity-gw-s3-posix-gateway-beegfs-eseries.html">Versity S3 Gateway</a> to access BeeGFS files via S3 as soon as they’re written.</p>

<p>ZFS - after <a href="/2024/02/26/zfs-deduplication-netapp-eseries.html">improved deduplication</a> that I blogged about in early 2024 - deserves another look, especially since E-Series arrays have recently (finally) added QLC flash media support (if only in EF300C and EF600C), so ZFS and non-ZFS pools have become an attractive option for anyone with compressible, duplicate or other data suitable for special handling (e.g. placement based on file age or access time).</p>

<p>Keep in mind that the license for pools require Enterprise Edition. Support for ACLs was considered an enterprise feature in version 7, but thankfully it’s been moved to Community Edition in version 8.</p>

      </article>
    </div>
    
      <div class="categories">
    <span><p>Categories:</p>
    
    
      <a href="
      /categories/#storage">storage</a>
      &nbsp; 
    
      <a href="
      /categories/#analytics">analytics</a>
       
    
  </span>
</div>
    

  
    <div>
      <h3>Related Posts</h3>
      <ul>
      
        <li><a href="/2022/09/24/low-hanging-storage-efficiency-fruit-beegfs.html">Low-hanging BeeGFS efficiency fruit</a></li>
      
        <li><a href="/2023/11/28/postgres-pgvector-instacluster-eseries.html">Postgres, pgvector, E-Series and Instaclustr</a></li>
      
        <li><a href="/2024/02/26/zfs-deduplication-netapp-eseries.html">ZFS deduplication with NetApp E-Series</a></li>
      
        <li><a href="/2022/07/05/kafka-solidfire-efficiency.html">Storage efficiency with Kafka 3.2 and NetApp SolidFire 12</a></li>
      
        <li><a href="/2024/04/11/netapp-eseries-containerized-beegfs-nfs-s3-all-in-one.html">NetApp E-Series with containerized BeeGFS, NFS, S3</a></li>
      
      </ul>
    </div>
  

    
  </div><footer class= "footer">
    <p>2025-05-18 06:22 </p>
    <p>Copyright © 2025 scaleoutSean. Content is released under the CC BY license. Design: Alessio Franceschi</p>
</footer>

</body>
</html>
