<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.27.3 by Michael Rose
  Copyright 2013-2025 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>ThinkParQ BeeGFS v8 with NetApp E-Series | Acting Technologist</title>
<meta name="description" content="How to get most out of BeeGFS v8 with NetApp E-Series">


  <meta name="author" content="scaleoutSean">
  
  <meta property="article:author" content="scaleoutSean">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Acting Technologist">
<meta property="og:title" content="ThinkParQ BeeGFS v8 with NetApp E-Series">
<meta property="og:url" content="https://scaleoutsean.github.io/2025/05/17/beegfs-v8-netapp-e-series-indexing-tiering-workflows.html">


  <meta property="og:description" content="How to get most out of BeeGFS v8 with NetApp E-Series">





  <meta name="twitter:site" content="@scaleoutSean">
  <meta name="twitter:title" content="ThinkParQ BeeGFS v8 with NetApp E-Series">
  <meta name="twitter:description" content="How to get most out of BeeGFS v8 with NetApp E-Series">
  <meta name="twitter:url" content="https://scaleoutsean.github.io/2025/05/17/beegfs-v8-netapp-e-series-indexing-tiering-workflows.html">

  
    <meta name="twitter:card" content="summary">
    
  

  



  <meta property="article:published_time" content="2025-05-17T00:00:00+08:00">






<link rel="canonical" href="https://scaleoutsean.github.io/2025/05/17/beegfs-v8-netapp-e-series-indexing-tiering-workflows.html">







  <meta name="google-site-verification" content="F6q7vIwQ2G0j8tk-KL9rAXOMLXMDMMUkEz4fRs1Nnnc" />


  <meta name="msvalidate.01" content="7cf5b7d96a77410a8ad035f764dc81b3">





<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Acting Technologist Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  window.enable_copy_code_button = true;
</script>

<script src="https://cdn.counter.dev/script.js" data-id="83dc700a-b821-4e57-9425-02b8336a9456" data-utcoffset="0"></script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <link href="https://fonts.googleapis.com/css?family=PT+Sans+Narrow:400,700|PT+Serif:400,700,400italic" rel="stylesheet" type="text/css">


  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
          <a class="site-logo" href="/"><img src="/assets/images/scaleoutsean-acting-technologist.png" alt="Acting Technologist"></a>
        
        <a class="site-title" href="/">
          Acting Technologist
          <span class="site-subtitle">human action through technology</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/archive/"
                
                
              >Archive</a>
            </li><li class="masthead__menu-item">
              <a
                href="/categories/"
                
                
              >Categories</a>
            </li><li class="masthead__menu-item">
              <a
                href="/tags/"
                
                
              >Tags</a>
            </li><li class="masthead__menu-item">
              <a
                href="/projects.html"
                
                
              >Projects</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main">
  


  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="ThinkParQ BeeGFS v8 with NetApp E-Series">
    <meta itemprop="description" content="Put BeeGFS Enterprise Edition to best use with NetApp E-Series arrays: indexing, workflows, antivirus and more">
    <meta itemprop="datePublished" content="2025-05-17T00:00:00+08:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://scaleoutsean.github.io/2025/05/17/beegfs-v8-netapp-e-series-indexing-tiering-workflows.html" itemprop="url">ThinkParQ BeeGFS v8 with NetApp E-Series
</a>
          </h1>
          

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2025-05-17T00:00:00+08:00">2025-05-17 00:00</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          24 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
        <ul>
  <li><a href="#introduction">Introduction</a></li>
  <li><a href="#data-tiering-with-pools">Data tiering with pools</a></li>
  <li><a href="#zfs">ZFS</a>
    <ul>
      <li><a href="#beegfs-snapshots">BeeGFS snapshots</a></li>
      <li><a href="#role-of-snapshots">Role of snapshots</a></li>
      <li><a href="#cognitive-overhead-of-zfs">Cognitive overhead of ZFS</a></li>
    </ul>
  </li>
  <li><a href="#beegfs-copy-tool">BeeGFS copy tool</a></li>
  <li><a href="#remote-storage">Remote storage</a></li>
  <li><a href="#file-system-events">File-system events</a></li>
  <li><a href="#file-index">File index</a></li>
  <li><a href="#monitoring">Monitoring</a></li>
  <li><a href="#conclusion">Conclusion</a></li>
</ul>

<h2 id="introduction">Introduction</h2>

<p>BeeGFS version 8 recently came out and I wanted to celebrate that with a post that goes through new and underappreciated features of BeeGFS, especially in the context of NetApp E-Series.</p>

<p>Some of the "underappreciated" features came out in version 7, but were improved in version 8.</p>

<h2 id="data-tiering-with-pools">Data tiering with pools</h2>

<p>This isn't a new feature in version 8, but even if I've mentioned it before, it must have been briefly.</p>

<p>So, what's the big deal?</p>

<ul>
  <li>A BeeGFS pool is one or more storage target (aka LUN) tagged for specific property (cost, performance, location, etc.)</li>
  <li>One can have a BeeGFS file-system with more than one pool</li>
  <li>Furthermore, one can easily move data (files or entire directory trees) between any two pools without stubs, shortcuts or other major inconveniences for users</li>
</ul>

<p>As you guess, that allows us to mix different disk types or even arrays in the same BeeGFS file-system.</p>

<p>As an example, we could use high performance arrays (or media) in with R10 (for metadata) or R6 (for data) volumes and lower-cost QLC-based DDP-based LUNs in the same BeeGFS file-system.</p>

<p>Tiering is performed with a simple command that moves files from source-tagged LUNs to target-tagged LUNs (copy data from source to destination, redirect metadata to destination, delete source data when done).</p>

<p><img src="/assets/images/beegfs-8-pools.svg" alt="BeeGFS pools with EF600 and EF600C" /></p>

<p>BeeGFS file-system is literally (and in the schematic) a black box; file (chunk) location is obfuscated from the user. Lateral movement (tiering) moves files from one pool to another, but their file-system path remains the same.</p>

<p>We can get as fancy as we want - we could use one hybrid array (e.g. EF300) for everything (R10 TLC NVMe for MD, R6 TLC NVMe for Hot Data, R6 HDD for Cold Data), for example.</p>

<p>Just remember to watch pool and target fullness in order to avoid blowing up a pool. You also don't want to be too stingy and having to waste IO on moving data back and forth all day long.</p>

<p>This example below shows a BeeGFS file-system with 2 data disks: target_0-6826D866-1 (Tier 1) and target_1-6826D866-1 (Tier 2). When creating pools (<code class="language-plaintext highlighter-rouge">beegfs pool create</code>), target_1-6826D866-1 was assigned to <code class="language-plaintext highlighter-rouge">archive</code> tier. (I should have aliased the targets betterâ€¦)</p>

<p><img src="/assets/images/beegfs-8-pools-01-storage.png" alt="BeeGFS and E-Series Pools" /></p>

<p>After giving the storage targets better aliases:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>beegfs target list
ID     TYPE     ALIAS           NODE  STORAGE_POOL  
s:101  storage  s101_default_0  s:1   s:1           
s:102  storage  s102_archive_0  s:1   s:2           
m:1    meta     target_meta_1   m:1   <span class="o">(</span>n/a<span class="o">)</span>         

<span class="nv">$ </span><span class="nb">sudo </span>beegfs pool list
ID   ALIAS                 TARGETS  MIRRORS  
s:1  storage_pool_default  s:101             
s:2  archive               s:102  
</code></pre></div></div>

<p>By default, new files land on <code class="language-plaintext highlighter-rouge">storage_pool_default</code> constituents (before target_0-6826D866-1 and now s101_default_0, as I have just one storage target in the default pool, but normally we'd have 4 or more).</p>

<p>If I examine this new file, I can see its location is as expected.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>beegfs entry info /mnt/beegfs/archive/flights-1m_v2.csv 
PATH                        ENTRY_ID      TYPE  META_NODE  META_MIRROR   STORAGE_POOL              STRIPE_PATTERN  STORAGE_TARGETS  STORAGE_MIRRORS  REMOTE_TARGETS  
/archive/flights-1m_v2.csv  4-68289A9B-1  file  m:1        <span class="o">(</span>unmirrored<span class="o">)</span>  storage_pool_default <span class="o">(</span>1<span class="o">)</span>  RAID0 <span class="o">(</span>4x512K<span class="o">)</span>  s:101            <span class="o">(</span>unmirrored<span class="o">)</span>     <span class="o">(</span>none<span class="o">)</span>          
</code></pre></div></div>

<p>When the file is no longer actively used (more on that later), I can move it to archive tier.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>beegfs entry migrate <span class="nt">--from-pools</span><span class="o">=</span>storage_pool_default <span class="nt">--pool</span><span class="o">=</span>archive /mnt/beegfs/archive/flights-1m_v3.csv 
Summary: <span class="o">{</span>MigrationStatusUnknown:0 MigrationErrors:0 MigrationNotSupported:0 MigrationSkippedDirs:0 MigrationNotNeeded:0 MigrationNeeded:0 MigratedFiles:1 MigrationUpdatedDirs:0<span class="o">}</span>
</code></pre></div></div>

<p>Don't be confused by its path (/mnt/beegfs/archive/), which doesn't change throughout. I could have had the file in /mnt/beegfs/, and later move it to /mnt/beegfs/archive/ - that <em>still</em> wouldn't move it to the archive tier. What moves it to another tier (pool) is the <code class="language-plaintext highlighter-rouge">entry migrate</code> command.</p>

<p>If it'd be less confusing this way, imagine having a workflow like this (recursive options are possible as well):</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/usr/bin/bash</span>
<span class="c"># Gets relative path of a file, moves it to /mnt/beegfs/archive/ and tiers to archive pool</span>
<span class="nb">read </span>myfile
<span class="nb">mv</span> /mnt/beegfs/<span class="k">${</span><span class="nv">myfile</span><span class="k">}</span> /mnt/beegfs/archive/<span class="k">${</span><span class="nv">myfile</span><span class="k">}</span>
<span class="nb">sudo </span>beegfs entry migrate <span class="se">\</span>
    <span class="nt">--from-pools</span><span class="o">=</span>storage_pool_default <span class="se">\</span>
    <span class="nt">--pool</span><span class="o">=</span>archive <span class="se">\</span>
    /mnt/beegfs/archive/<span class="k">${</span><span class="nv">myfile</span><span class="k">}</span>
</code></pre></div></div>

<p>Of course, you wouldn't <em>really</em> have scripts like these; you'd have this in your workflows, job scripts or some other place that runs them automatically.</p>

<p>One of the reasons for that is tiered files aren't supposed to be open (modified) during migration, so the best time to run them is right after jobs or workflow steps so that tiering doesn't become a burden. (Tiering up - to a higher tier - could be done <em>before</em> jobs or workflows start, of course.)</p>

<p>After entry migration, we can see that STORAGE_POOL is now <code class="language-plaintext highlighter-rouge">archive</code>.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>beegfs entry info /mnt/beegfs/archive/flights-1m_v3.csv 
PATH                        ENTRY_ID      TYPE  META_NODE  META_MIRROR   STORAGE_POOL  STRIPE_PATTERN  STORAGE_TARGETS  STORAGE_MIRRORS  REMOTE_TARGETS  
/archive/flights-1m_v3.csv  1-68289D0B-1  file  m:1        <span class="o">(</span>unmirrored<span class="o">)</span>  archive <span class="o">(</span>2<span class="o">)</span>   RAID0 <span class="o">(</span>4x512K<span class="o">)</span>  s:102            <span class="o">(</span>unmirrored<span class="o">)</span>     <span class="o">(</span>none<span class="o">)</span>    
</code></pre></div></div>

<p>Related to this, I'd also like to say a thing or two about ZFS.</p>

<h2 id="zfs">ZFS</h2>

<p>BeeGFS supports ZFS (as well as other file systems) on Data disks and I've mentioned this on several occasions. I've also blogged about ZFS on E-Series in the context of LXD and Incus, so if you're interested in ZFS deduplication, compression, and such, check blog archives or use the search feature to find those posts.</p>

<p>Here I'll just point out that the screenshot above uses BeeGFS with ZFS metadata and data disks.</p>

<p>It's important to remember that you won't get this from NetApp, as the BeeGFS solution from NetApp deploys vanilla file-systems (ext4, XFS), so you'd have to modify the Ansible deployment scripts to get this done.</p>

<p>ThinkParQ does support ZFS and the pools feature requires Enterprise Edition features, which means you'd have this covered anyway. Just make sure you RTFM (BeeGFS v8) related to ZFS.</p>

<p>Back to the screnshot:</p>

<ul>
  <li>I used Ubuntu 24.04 with self-built OpenZFS 2.3.0 which is the version with smart(er) deduplication (I blogged about it with E-Series before it was released, so I won't repeat any of that)</li>
  <li><code class="language-plaintext highlighter-rouge">storage_pool_default</code> pool: as this is my Tier 1 I have no compression, no dedupe set</li>
  <li><code class="language-plaintext highlighter-rouge">archive</code> pool: this is Tier 2 and both compression and dedupe are enabled</li>
</ul>

<p>Is ZFS dedupe super-smart? Not really, but it's good enough for Tier 3. Identical files will be deduplicated. Partial matches should be as well, but "it depends". The compression was and is legitimate, and you can pick any of several methods. I used ZSTD and LZ4 with BeeGFS but didn't compare (savings shouldn't be BeeGFS-specific).</p>

<p>We could have several archives with different (combinations of) settings, but it's better to not fragment your storage pools unless your data is very predictable in terms of data types and capacity.</p>

<h3 id="beegfs-snapshots">BeeGFS snapshots</h3>

<p>Snapshots with ZFS are possible, but likely impractical. There's no way to "quiesce" a live BeeGFS file-system, so the only sure way would be to stop services (mostly <code class="language-plaintext highlighter-rouge">beegfs-client</code>, but preferrably all others as well) and then take a snapshot. This is how E-Series (SANtricity) snapshots could work as well, of course.</p>

<p>It's usually not possible to stop services, which is why I say snapshots without BeeGFS cooperation aren't practical, but you may be able to do that on some file-systems (e.g. file-system used for home directories at 6am). Once snapshots are taken, they can be restored when BeeGFS is stopped.</p>

<p><img src="/assets/images/beegfs-8-zfs-01-snapshot.png" alt="BeeGFS with ZFS snapshots" /></p>

<ul>
  <li>(1) we delete files to simulate damage</li>
  <li>(2) stop BeeGFS services (at least beegfs-client, beegfs-meta, beegfs-storage, beegfs-sync)</li>
  <li>(3) we rollback all constituent volumes to latest ZFS snapshot</li>
  <li>(4) BeeGFS services can now be started and all files from the third snapshot are back</li>
</ul>

<p>The entire process can take less than a minute.</p>

<p>Following these steps I executed BeeGFS <code class="language-plaintext highlighter-rouge">fsck</code> which found 0 errors on the restored BeeGFS file-system.</p>

<p>The snapshot "third" is the one that was restored. (In the case you're wondering where's "second"; it was disappeared earlier when, having taken the snapshot "second", I rolled the pools back to "first" thereby destroying the second set of snapshots.)</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>zfs list <span class="nt">-t</span> snapshot
NAME        USED  AVAIL  REFER  MOUNTPOINT
md1@first   230K      -  5.10M  -
md1@third   210K      -  5.10M  -
zd1@first    20K      -  38.7M  -
zd1@third  20.5K      -  38.7M  -
zd2@first    19K      -  22.6M  -
zd2@third    19K      -  22.6M  -

</code></pre></div></div>

<p>Note that stopping services in order to snapshot and/or rollback snapshots may have unpredictable consequences, e.g. on Hive indexes and file-system notifications. Some of these potential issues aren't even necessarily related to snapshots (e.g. some notifications may fail to be delivered).</p>

<p>If these compromises and risks are acceptable, ZFS replication can also be used once we have snapshots to work with. Note that - unlike in regular stand-alone ZFS pools, here each metadata and storage target are stand-alone, practically "single disk" zpools and at the same time mutually dependent because they need to be "re-assembled" and imported into a BeeGFS cluster on destination BeeGFS cluster. Because that's not trivial and because I don't think many people would want to do this, I won't attempt to do that in this post.</p>

<h3 id="role-of-snapshots">Role of snapshots</h3>

<p>How many administrators are capable of understanding what rolling back a snapshot on a 10 PB file-system means and how that impacts various Hive indexes, external databases and downstream data consumers?</p>

<p>My guess would be: not many. In very insular, "data island" style environments with one or two applications it may be possible to understand what's going on. In an environment with many applications or where events and data cross cluster boundaries, it may not. If you roll a snapshot here, what happens to data that left the cluster but has been rolled back on your own cluster?</p>

<p>If I needed to design a snapshottable BeeGFS file-system attached to E-Series, I'd create a file-system from single array LUNs, so that I can take (and restore) snapshots using SANtricty Consistency Groups. In that way, snapshots of small (few TB) to medium sized (up to a few hundred TB) file-systems could be created regardless of file-system type on data targets. They would be limited by the performance of single E-Series array, but given their size and nature (prototyping, etc.), that could suffice for most basic rollbacks.</p>

<p>BeeGFS snapshots can be useful, but like SANtricity snapshots, they're unlikely to be viable cluster-wide. If you have the need for snapshots, consider one of these workarounds:</p>

<ul>
  <li>Create smaller BeeGFS filesystems which can be taken offline and snapshot. As long as the restart cycles can be done faster than copying or rsync-ing data from a golden copy, this can be a good approach.</li>
  <li>Create temporary file-systems for experimentation and prototyping and copy data to those with BeeGFS copy (see <a href="https://doc.beegfs.io/latest/advanced_topics/beeond.html">BeeOND</a>). These can be made of R0 disks for lowest cost and highest performance. Although copying is inefficient, in a fast environment copying 5 or 10 TBs can take just several minutes. Even if done 10 times a day, it's only 30 minutes of copying.</li>
  <li>Periodic BeeGFS sync to S3 with S3 versioning may sometimes be a better choice: there's no need for file-system and storage administrator intervention, and all uploaded versions are accessible to the user. (BeeGFS sync in v8.0 doesn't have the ability to use object versions, but if they bucket has them enabled, they should be available.)</li>
  <li>If rapid prototyping can be done better on a non-BeeGFS ZFS (or non-ZFS) NAS system in the same environment, NFS and ZFS can be HA-clustered with Pacemaker and Corosync (note: the NetApp BeeGFS solution with E-Series also uses Pacemaker and Corosync for HA of BeeGFS server pairs) to give you ZFS features in a compact environment that's easier to manage.</li>
</ul>

<h3 id="cognitive-overhead-of-zfs">Cognitive overhead of ZFS</h3>

<p>I say this only half-jokingly because unlike the default storage and file-system configuration in the NetApp BeeGFS solution for E-Series, ZFS isn't trivial to get right, especially if deduplication is enabled.</p>

<p>I won't deep-dive into this since my first-hand experience isn't deep enough, but you can find about it on the Internet. Some notes:</p>

<ul>
  <li>Deduplication
    <ul>
      <li>Deduplication complicates things a lot, so don't even try unless you have plenty (2/3 or more) duplicate data.</li>
      <li>Deduplication needs a lot of RAM (anywhere between 0.5-2 % of usable "dedupe pool" (sum of storage targets) capacity), so a dedupe pool with 1PB usable may need ~10TB of RAM (which, spread over several servers may cost more in extra software, servers, and RAM than deduplication can save).</li>
      <li>Deduplication tables require lots of tiny reads and writes. That means that in the cluster depicted at the top we'd want to use special deduplication vdevs on the leftmost E-Series used for metadata. To physically segregate dedupe tables from regular MD disks used for BeeGFS, consider using dedicated R10 devices for that. That could still cause some interference on the controller level depending on situation (workload).</li>
      <li>Because deduplication overheads are so significant, tiering data back and forth should be minimized. If unsure, implement 3 pools (Tier 1 with no dedupe, Tier 2 with compression, Tier 3 with compression and dedupe, for example).</li>
      <li>Deduplication effectiveness with BeeGFS on ZFS will be lower than deduplication on individual ZFS systems because many duplicate blocks may land each on a different ZFS target, resulting in less (or no) deduplication.</li>
      <li>Deduplication cannot be disabled without destroying a pool, so don't enable it without testing and making sure it's worth the trouble and extra resources.</li>
    </ul>
  </li>
  <li>Compression
    <ul>
      <li>Compression is easy. Test some or all of the available algorithms and pick one for each filesystem (or device, if you are willing to break filesystem into several archive pools)</li>
      <li>CPU is the main concern here. Don't use ridiculous GZIP compression levels for data that doesn't compress well, for example. Data is either compressible or not. Use lowest levels of LZ4 or ZSDT for starters, or get some sample data to pick an optimal algo and setting.</li>
    </ul>
  </li>
  <li>E-Series-related notes
    <ul>
      <li>ZIL - intent log which must be protected - LUNs should be created on a R10 group based on NVMe SSDs</li>
      <li>L2ARC ("L2" read cache) - I suspect we mostly don't need it too large for truly archived data. I think we'd want to have one device per one NL-SAS LUN, so that a Tier 3 pool has N L2ARC (~5% of storage capacity) and N storage vdevs. For example, a 1PB Tier 3 pool could have 16 x 64 TB NL-SAS storage vdevs and 16 x 3.2 TB SSDs (R0 or R10 on E-Series TLC). If archive data is expected to be slow or access to data is very random, you don't need almost any L2ARC for it (we may want to allocate ~1% for metadata read cache, though).</li>
      <li>Special dedupe vdev - as mentioned earlier, R10 (2, 4, 8 NVMe SSDs for example) can be sliced into N LUNs for N stand-alone ZFS targets. Capacity requirement can be cut by using larger record sizes in ZFS</li>
      <li>BeeGFS metadata - several LUNs on R10 group of NVMe SSDs as a RAID group completely separate from ZFS-related special vdevs. Note that in an all-ZFS BeeGFS these would still be used for ZFS, but as single-device zpools (1 LUN = 1 metadata target). Or we could use ext4 for MD targets and avoid ZFS on these LUNs.</li>
      <li>EF600 (NVMe) can provide ~1 million 4kB IOPS. Not knowing the precise breakdown between each use case for R10 LUNs makes the option of using a single big R10 group for everything interesting, but I'd consider this only if I could populate EF600 with at least 20 disks, to max out IOPS</li>
      <li>For "all in one" R10 group consider underprivisioning NVMe LUN capacity by 30% (i.e. leave &gt;=30% of usable unused) to leave space for garbage collection and accommodate &gt;3 DWPD. I've blogged about DWPD before, so I won't repeat that. E-Series documentation refers to this as under-provisioning (of LUN capacity). I call it over-provisioning (of raw disk capacity, related to used LUN capacity).</li>
    </ul>
  </li>
</ul>

<p>To illustrate these comments about E-Series: after requirements analysis and storage design, the leftmost array (MD) might look like this: LUNs easier to size can be on smaller R1(0) disk groups while those with uncertain sizing or capacity requirements could be pooled on a larger R10 group.</p>

<p><img src="/assets/images/beegfs-8-e-series-ssd.png" alt="E-Series device and LUN layout" /></p>

<p>But if we weren't sure, we could just build a 24-wide R10. If lucky, we'd have enough IOPS and bandwidth for everything without squeezing any particular workload - it would be Kumbaya all day long. Another easy scenario is a simple low-cost, low-performance data dump area where we could use a single EF600 Hybrid (controller shelf full of NVMe flash in R10, expansion shelves with NL-SAS) and it'd all just work: 2.7 PB in NL-SAS (post RAID6) could be helped by 135 TB (5%) in R10 capacity for BeeGFS metadata and special ZFS disks. Need 10 PB? Add three more "pods".</p>

<p>Among the bad scenarios, one or two particularly busy SSD storage targets (e.g. dedupe vdev(s)) could slow down the other SSD-based LUNs and you'd be in trouble.</p>

<p>Using those flash storage-based LUNs on the first E-Series array and R6-based NL-SAS or SSD devices on other arrays, we'd assemble larger BeeGFS file-systems.</p>

<p>For the sake of simplicity, this image shows a single-tier BeeGFS-on-ZFS filesystem that has metadata on a R10 LUN and data spread across four ZFS pools - each single storage dev pool with its own ZIL, L2ARC, Dedupe devices.</p>

<p><img src="/assets/images/beegfs-8-e-series-on-zfs.png" alt="BeeGFS, ZFS and E-Series SSD layout" /></p>

<p>This isn't a recipe, but a starting point for further investigation. (In the easy "all in one" scenario this could be an entire BeeGFS cluster on a single EF600 Hybrid array.)</p>

<p>Because of large discrepancies in "real life" data and the many options throughout the entire stack (BeeGFS, ZFS, OS, storage), actual "optimal" configuration would probably differ from cases to case (but hopefully within an order of magnitude).</p>

<p>Unless you want to spend some time figuring all this out, it's best to enable compression on Tier 2 and Tier 3, and enable deduplication only if justified. If you want to prototype on a small scale BeeGFS cluster, consider using VMs on <a href="/2023/12/02/containerized-beegfs-with-netapp-eseries.html">ESXi</a> or KVM (it's much easier than containers!).</p>

<h2 id="beegfs-copy-tool">BeeGFS copy tool</h2>

<p>Earlier I've mentioned BeeOND and the ability to resync-on-demand with <code class="language-plaintext highlighter-rouge">beegfs copy</code>. The command works on regular BeeGFS as well.</p>

<p>Let's say we build a smaller, temporary system for job processing and our job needs data from <code class="language-plaintext highlighter-rouge">inbound</code> and <code class="language-plaintext highlighter-rouge">logs</code> to this new file-system. Our machine file has a list of BeeGFS cluster members we'll use. A total of 32 threads will copy files in 4 MB chunks. You may set up passwordless SSH among the machines involved in copying if you don't want to authenticate in console.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># start beeond on machinese in beeond-file.txt, use /zd3 on each, and mount filesystem at /mnt/beeond</span>
beeond start <span class="nt">-n</span> /mnt/beegfs/config/beeond-file.txt <span class="se">\</span>
    <span class="nt">-d</span> /zd3 <span class="se">\</span>
    <span class="nt">-c</span> /mnt/beeond
<span class="c"># copy two source directories to /mnt/beeond</span>
beegfs copy <span class="nt">-m</span> /mnt/beegfs/config/machine-file.txt <span class="se">\</span>
    /mnt/beegfs1/archive/inbound /mnt/beegfs2/archive/logs <span class="se">\</span>
    /mnt/beeond <span class="se">\</span>
    <span class="nt">-t</span> 32 <span class="nt">-c</span> 4
<span class="c"># run compute job(s)</span>
<span class="c"># stop and optionally (-d) delete beeond filesystem</span>
<span class="c"># beeond stop -n /mnt/beegfs/config/beeond-file.txt -L -d</span>
</code></pre></div></div>

<p>Source (and especially target) directories shouldn't be modified during copying, but as we said at 10 GB/s it'd take only 90 seconds to copy 1TB of data, so in many cases it will happen in a minutes at most - usually a fraction of the time required to run compute jobs.</p>

<p>If you plan to run multiple jobs on same source data, use beegfs copy to re-sync instead of starting over. One use case for this would be data cleansing or ETL prototyping. At the end of a successful run, we could use <code class="language-plaintext highlighter-rouge">beegfs copy</code> to copy data to the "main" BeeGFS file-system before deleting the temporary filesystem.</p>

<p>Since our sources above come from /mnt/beegfs1/archive/, one has to wonder: could we use <code class="language-plaintext highlighter-rouge">beegfs copy</code>to copy data from a deduplicated archive to BeeOND? Or should we use <code class="language-plaintext highlighter-rouge">beegfs entry migrate</code> and work on Tier 1 (without <code class="language-plaintext highlighter-rouge">beeond</code>)? Or work directly on Tier 2 (also without <code class="language-plaintext highlighter-rouge">beeond</code>)?</p>

<p>As almost always, "it depends". But <code class="language-plaintext highlighter-rouge">beegfs copy</code> (with or without <code class="language-plaintext highlighter-rouge">beeond</code>) should often be the right answer. If data sets are too large to copy (not enough destination capacity or time to copy) then copying is out of question, but it usually won't be. You could also copy from a deduplicated pool to Tier 1 on the same BeeGFS file-system (rather than to a new temporary file-system), of course.</p>

<p>This 90 second animation shows the entire process from this and previous section:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">beeond start|stop</code> (previous section) - create and destroy a temporary BeeGFS file-system</li>
  <li><code class="language-plaintext highlighter-rouge">beegfs copy</code> - copy data from "main" BeeGFS file-system to temporary file-system for processing</li>
</ul>

<p><img src="/assets/images/beegfs-beeond-setup-teardown.gif" alt="Beeond and beegfs copy animated demo" /></p>

<p>Beeond doesn't <em>have to</em> use shared storage. Compute nodes with fast internal media could use <code class="language-plaintext highlighter-rouge">beeond</code> to copy inputs to fast local storage for faster or lower-latency access. Whether <code class="language-plaintext highlighter-rouge">beeond</code> uses shared or local storage, <code class="language-plaintext highlighter-rouge">beegfs copy</code> is the recommended way to copy data from one BeeGFS file-system to another.</p>

<h2 id="remote-storage">Remote storage</h2>

<p>This feature allows us to define Remote Storage Targets (RSTs) which are generally S3 buckets.</p>

<p>Then we can configure push and pull to/from those RSTs.</p>

<p>I've been having problems configuring these, so I don't have much to say about them except that earlier I assumed pushing to an RST would evacuate data from BeeGFS. That doesn't seem to be the case. It merely <em>copies</em> data to an RST. Think of it as a tool for push/pull data exchange.</p>

<p>Originally I had hoped one would be able to delete pushed data from the source and pull it back if and when needed, but that doesn't seem to be a use case here. It's more like pull/push in <a href="https://github.com/NetApp/netapp-dataops-toolkit/tree/main/netapp_dataops_traditional#cli-push-to-s3-directory">NetApp DataOps Toolkit</a> except that it should be much faster.</p>

<p>In terms of RST support, NetApp StorageGRID and other S3-like services (MinIO, Versity S3 Gateway) can use E-Series storage (I've blogged about them in several <a href="/2023/09/03/minio-erasure-coding-and-netapp-e-series.html">posts</a>). MinIO and Versity can run in Kubernetes with E-Series as well. I think <a href="/2022/07/06/apache-ozone-netapp-eseries.html">Apache Ozone</a> should work just fine. You could have any of these on a server (or three, especially with K8s and/or EC) attached to EF600 with NL-SAS. PBs of S3 at a low cost and headache-free management.</p>

<p>Note that for ZFS, <code class="language-plaintext highlighter-rouge">beegfs entry refresh</code> must be executed before <code class="language-plaintext highlighter-rouge">beegfs remote push</code>, so that ZFS updates sizes of new and modified files.</p>

<h2 id="file-system-events">File-system events</h2>

<p>This also isn't a new feature, but it's been reworked for v8.</p>

<p>Why should we care?</p>

<p>You may have heard of AI. Or ransomware. Or workflows. These are some situations where file-system events can be useful. For the ONTAP people, FPolicy may come to mind. Or inotify, in the world of Linux.</p>

<p>In essence, BeeGFS can send them to two places: a gRPC listener (new) and UNIX socket (legacy).</p>

<p>Using the legacy approach, run beegfs-event-listener on the socket "file" <code class="language-plaintext highlighter-rouge">beegfs-watch</code> service sends data to. Example:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/opt/beegfs/sbin/beegfs-event-listener /run/beegfs/eventlog
</code></pre></div></div>

<p>Then, as stuff happens on file-system, event listener spits out FS events it reads from the socket.</p>

<p><img src="/assets/images/beegfs-8-events-01-legacy.png" alt="Legacy file-system events " /></p>

<p>Now we just need to decide what we want to do with this.</p>

<p>For example, <code class="language-plaintext highlighter-rouge">LastWriterClosed</code> on <code class="language-plaintext highlighter-rouge">/mnt/beegfs/incoming/*.mp4</code> may be the thing we're looking for. Then we kick off <code class="language-plaintext highlighter-rouge">ffmpeg</code> and let it do its thing on the file and use RST to push processed videos out to an S3 bucket.</p>

<p>The BeeGFS 8 documentation gives an example of piping those events to a user script:</p>

<p><img src="/assets/images/beegfs-8-events-02-legacy-pipe.png" alt="Legacy file-system events piped to a script" /></p>

<p>This is a basic example, though. Normally we'd filter by event type and do things with that:</p>

<ul>
  <li>Run anti-virus scanner (ClamAV, for example)</li>
  <li>Run various data processing</li>
</ul>

<p>For media, a data processing step could be format conversion (I <a href="/2022/04/05/nomad-beegfs-eseries.html">blogged about that</a> in 2022).</p>

<p>For AI, data processing steps could be frame-by-frame analysis and tagging. Or you could automatically create file embeddings in a (vector) database.</p>

<p>For SIEM, data processing steps could be parsing and ingress of <code class="language-plaintext highlighter-rouge">.log</code> files to Elasticsearch.</p>

<p>For anti-ransomware and anti-virus, we could run a scanner, but we could also build own detection of suspicious file-system activity. More on that in the next section. Here I'll just say <a href="/2024/01/29/antivirus-scanning-for-on-premises-s3.html">I blogged</a> about AV scanning and getting an up-to-date list of objects required external services (Elasticsearch, Kafka, etc).</p>

<p>The watcher example from the documentation can be easily adjusted to initiate data pipeline processing along the lines of what I mentioned above: depending on document extension (and/or additional criteria such as path matching, owner and similar), new files can be processed as soon as last writer is done writing to the file.</p>

<p><img src="/assets/images/beegfs-8-events-03-decider.gif" alt="Legacy file-system events for data pipeline processing" /></p>

<p>While the modern (gRPC) BeeGFS file-system event listener service is robust, even this basic event processor would suffice for many use cases and can be created in minutes.</p>

<p>You may wonder where are these tasks and steps scheduled. Unlike on some highly-integrated storage systems (e.g. Vast Data), there's no built-in generic message queue so we'd need to run our own service on BeeGFS or non-BeeGFS volumes - some extra work is required in exchange for flexibility and openness. Such services could run <a href="/2022/08/11/nomad-pack-influxdb-beegfs.html">on BeeGFS</a> or elsewhere (e.g. services in <a href="/2022/12/09/directpv-topolvm-csi-lvm-das-k8s-with-eseries.html">Kubernetes on non-BeeGFS volumes</a>).</p>

<p>Supported BeeGFS file-system events are: flush, close, trunc, setattr, link-op, open-read, open-write, open-readwrite.</p>

<h2 id="file-index">File index</h2>

<p>You can <a href="https://doc.beegfs.io/8.0/hive/hive_index.html#beegfs-hive-index">read about it in TFM</a>. My <code class="language-plaintext highlighter-rouge">tldr</code>:</p>

<ul>
  <li>Scans MD and stores file metadata into a database file</li>
  <li>BeeGFS Hive Index can be stored in-tree or out-of-tree (i.e. you can keep it on some NFS share or a non-BeeGFS FS on R10)</li>
  <li>SQL qeueries can be issued against this DB. They're much faster than doing the same thing against file-system.</li>
</ul>

<p>BeeGFS Hive database(s) are SQLite3 databases and the index path and other details are configured in /etc/beegfs/index/config. For example, my BeeGFS file-system has just one directory (/mnt/beegfs/archive) and accordingly there are only 2 index databases (one for file-system "root" and another for the archive directory).</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">dir</span> <span class="nt">-laR</span> /work/index
/work/index:
total 60
drwxrwxrwx 3 root root    4096 May 17 14:43 <span class="nb">.</span>
drwxr-xr-x 3 root root    4096 May 17 07:29 ..
drwxrwxr-x 2 sean sean    4096 May 17 14:43 archive
<span class="nt">-rw-r--r--</span> 1 root root   16384 May 17 14:43 .bdm.db
<span class="nt">-rw-r--r--</span> 1 sean beegfs 32768 May 17 14:44 .bdm.db-shm
<span class="nt">-rw-r--r--</span> 1 sean beegfs     0 May 17 14:43 .bdm.db-wal

/work/index/archive:
total 20
drwxrwxr-x 2 sean sean  4096 May 17 14:43 <span class="nb">.</span>
drwxrwxrwx 3 root root  4096 May 17 14:43 ..
<span class="nt">-rw-r--r--</span> 1 sean sean 12288 May 17 14:43 .bdm.db
</code></pre></div></div>

<p>You may wonder if these are populated on-the-fly by BeeGFS Watch service. Not yet. It's planned, though.</p>

<p>Because there may be billions of files and directories, that could represent a significant burden for the file-system. Currently you need to create (or refresh) an index. Refreshes can be done overnight, for example.</p>

<p>This screenshot shows index creation and a way to query entries.</p>

<p><img src="/assets/images/beegfs-8-events-01-index.png" alt="Query BeeGFS Hive index" /></p>

<ul>
  <li>(1) create index</li>
  <li>(2) query index (there's a lot of flexibility in how this can be done)</li>
  <li>(3) use query results for other steps in your data processing pipeline</li>
</ul>

<p>Unfortunately, <code class="language-plaintext highlighter-rouge">index query</code> does not yet support JSON output, so unlike with FS events (which we saw are native JSONLDs) we have to create a wrapper for CSV-to-JSON conversion, or store output to CSV and import that CSV in another step.</p>

<p>When watch and when Hive? I guess "it depends". If I can, I'd always prefer to have as much as possible done in Hive because that's "cheaper" in terms of resources.</p>

<p>For example, let's say we want to index content of text files to be able to perform full text and vector similarity search:</p>
<ul>
  <li>For SIEM use cases, we'd want it done with Watch service because it's important to spot problems quickly</li>
  <li>For RAG, maybe it's not essential to have content indexed in near real-time and so overnight (or hourly) recursive refresh of a bunch of directories followed by a SQL query may be a better approach</li>
</ul>

<p>One popular use case for Hive index is deletion of files not accessed for more than X days from Tier 1 when Tier 1 is used exclusively for scratch space.</p>

<h2 id="monitoring">Monitoring</h2>

<p>This isn't new, but it's been improved. InfluxDB version 1.x is one of two supported InfluxDB editions and - as I said <a href="/2025/01/24/influxdb-3-core-alpha.html">here</a> - that means version 3 is supported as well.</p>

<p>My advice would be to use InfluxDB 3 OSS and configure BeeGFS v8 for InfluxDB version 1.x as per <a href="https://doc.beegfs.io/8.0/advanced_topics/mon.html#configuration">TFM</a>. I haven't tried this myself (it's on my TODO list), but there's no reason this wouldn't work.</p>

<h2 id="conclusion">Conclusion</h2>

<p>BeeGFS version 8 contains some great improvements that I haven't even covered in this blog post. Some other features, added in recent years, have been integrated and improved, and yet others (monitoring) practically resurrected due to the appearance of InfluxDB 3 OSS.</p>

<p>BeeGFS Watch (file-system event monitoring) and Hive Index are very relevant in the world of AI and analytics, so I gave more space to these features. From the examples you can see there are multiple ways to process new data using these two features. Support for out-of-tree indexes, an open DB format and RST means BeeGFS name spaces are fully open to integrations <em>across file-systems, storage environments and locations</em>:</p>

<ul>
  <li>place indexes on non-BeeGFS file-systems or NFS shares</li>
  <li>send file-system modifications updates to external gRPC listeners or process updates in own scripts/services</li>
  <li>push BeeGFS files or directory trees to any standard S3-like RST</li>
  <li>process files on BeeGFS or on S3 (on-premises or in the cloud)</li>
</ul>

<p>Note that files <strong>do not</strong> necessarily need to be uploaded or copied to S3 for processing via S3. You may use <a href="/2023/09/20/versity-gw-s3-posix-gateway-beegfs-eseries.html">Versity S3 Gateway</a> to access BeeGFS files via S3 as soon as they're written.</p>

<p>ZFS - after <a href="/2024/02/26/zfs-deduplication-netapp-eseries.html">improved deduplication</a> that I blogged about in early 2024 - deserves another look, especially since E-Series arrays have recently (finally) added QLC flash media support (if only in EF300C and EF600C), so ZFS and non-ZFS pools have become an attractive option for anyone with compressible, duplicate or other data suitable for special handling (e.g. placement based on file age or access time).</p>

<p>Keep in mind that the license for pools require Enterprise Edition. Support for ACLs was considered an enterprise feature in version 7, but thankfully it's been moved to Community Edition in version 8.</p>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/ai" class="page__taxonomy-item p-category" rel="tag">ai</a><span class="sep">, </span>
    
      <a href="/tags/beegfs" class="page__taxonomy-item p-category" rel="tag">beegfs</a><span class="sep">, </span>
    
      <a href="/tags/compression" class="page__taxonomy-item p-category" rel="tag">compression</a><span class="sep">, </span>
    
      <a href="/tags/deduplication" class="page__taxonomy-item p-category" rel="tag">deduplication</a><span class="sep">, </span>
    
      <a href="/tags/e-series" class="page__taxonomy-item p-category" rel="tag">e-series</a><span class="sep">, </span>
    
      <a href="/tags/efficiency" class="page__taxonomy-item p-category" rel="tag">efficiency</a><span class="sep">, </span>
    
      <a href="/tags/embedding" class="page__taxonomy-item p-category" rel="tag">embedding</a><span class="sep">, </span>
    
      <a href="/tags/indexing" class="page__taxonomy-item p-category" rel="tag">indexing</a><span class="sep">, </span>
    
      <a href="/tags/netapp" class="page__taxonomy-item p-category" rel="tag">netapp</a><span class="sep">, </span>
    
      <a href="/tags/rag" class="page__taxonomy-item p-category" rel="tag">rag</a><span class="sep">, </span>
    
      <a href="/tags/tiering" class="page__taxonomy-item p-category" rel="tag">tiering</a><span class="sep">, </span>
    
      <a href="/tags/vector" class="page__taxonomy-item p-category" rel="tag">vector</a><span class="sep">, </span>
    
      <a href="/tags/zfs" class="page__taxonomy-item p-category" rel="tag">zfs</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/analytics" class="page__taxonomy-item p-category" rel="tag">analytics</a><span class="sep">, </span>
    
      <a href="/categories/storage" class="page__taxonomy-item p-category" rel="tag">storage</a>
    
    </span>
  </p>


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time class="dt-published" datetime="2025-05-17T00:00:00+08:00">2025-05-17 00:00</time></p>

      </footer>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://x.com/intent/tweet?via=scaleoutSean&text=ThinkParQ+BeeGFS+v8+with+NetApp+E-Series%20https%3A%2F%2Fscaleoutsean.github.io%2F2025%2F05%2F17%2Fbeegfs-v8-netapp-e-series-indexing-tiering-workflows.html" class="btn btn--x" aria-label="Share on X" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on X">
    <i class="fab fa-fw fa-x-twitter" aria-hidden="true"></i><span> X</span>
  </a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://scaleoutsean.github.io/2025/05/17/beegfs-v8-netapp-e-series-indexing-tiering-workflows.html" class="btn btn--linkedin" aria-label="Share on LinkedIn" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn">
    <i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span>
  </a>

</section>


      
  <nav class="pagination">
    
      <a href="/2025/04/16/cloudera-with-netapp-e-series.html" class="pagination--pager" title="Cloudera Base with NetApp E-Series">Previous</a>
    
    
      <a href="/2025/05/20/get-started-with-netapp-solidfire-mcp-server.html" class="pagination--pager" title="Build NetApp SolidFire MCP server">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
    
<div class="page__related">
  
  <h2 class="page__related-title">You may also enjoy</h2>
  <div class="grid__wrapper">
    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/2025/10/08/epa-3.5.1.html" rel="permalink">E-Series Performance Analyzer v3.5.1
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2025-10-08T00:00:00+08:00">2025-10-08 00:00</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          1 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">E-Series Performance Analyzer v3.5.1 has been released with minor improvements
</p>
  </article>
</div>

    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/2025/10/07/ecp-eseries-performance-analyzer-aka-collector.html" rel="permalink">E-Series SANtricity Collector (ESC 4)
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2025-10-07T00:00:00+08:00">2025-10-07 00:00</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          8 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">About EPA 4 (aka ESC 4) and how to pick the right one for you
</p>
  </article>
</div>

    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/2025/09/13/mcp-for-netapp-eseries.html" rel="permalink">MCP Server for NetApp E-Series arrays
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2025-09-13T00:00:00+08:00">2025-09-13 00:00</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          8 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Use case for MCP server for NetApp E-Series arrays
</p>
  </article>
</div>

    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/2025/09/12/beegfs-nfs-s3-for-dev-test-edu-edge.html" rel="permalink">Containerized BeeGFS with NFSv4 and S3
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2025-09-12T00:00:00+08:00">2025-09-12 00:00</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          8 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Revisiting that old idea for All-in-One BeeGFS stack
</p>
  </article>
</div>

    
  </div>
</div>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        

<div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
          <li><a href="https://twitter.com/scaleoutsean" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/scaleoutsean" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>


<div class="page__footer-copyright">&copy; 2020-2025 scaleoutSean.github.io | <a href="https://scaleoutsean.github.io">Acting Technologist</a> | Terms: <a href="https://creativecommons.org/licenses/by/4.0/deed.en" target="_blank">CC BY 4.0</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>






  </body>
</html>
