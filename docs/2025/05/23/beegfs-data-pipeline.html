<!doctype html>
<html lang="en">
  <head>
        <meta charset="UTF-8">
    <meta http-equiv="content-language" content="en">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="googlebot" content="noarchive">
    <meta name="googlebot" content="max-snippet:60">
    <meta name="googlebot" content="max-image-preview:small">
    <meta name="bingbot" content="noarchive">
    <meta name="bingbot" content="max-snippet:60">
    <meta name="bingbot" content="max-image-preview:small">
    <meta name="robots" content="noarchive">
    <meta name="robots" content="max-snippet:60">
    <meta name="google-site-verification" content="F6q7vIwQ2G0j8tk-KL9rAXOMLXMDMMUkEz4fRs1Nnnc" />
    <title>
        
            Data pipeline with ThinParQ BeeGFS and NetApp E-Series | Acting Technologist
      
    </title>
    <meta name="description" content="
     How 
     ">

    <!-- LINK TO ATOM FEED FOR SEO -->
    <link rel="alternate" type="application/atom+xml" href="https://scaleoutsean.github.io/feed.xml" />

    <!-- FAVICON -->
    <link rel="icon" href="/assets/favicon.ico" type="image/x-icon">
    <link rel="shortcut icon" href="/assets/favicon.ico" type="image/x-icon">

    <!-- CSS -->
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="stylesheet" href="/assets/css/syntax.css">

    <!-- Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css">

    <link href="/_pagefind/pagefind-ui.css" rel="stylesheet">
    <script src="/_pagefind/pagefind-ui.js" type="text/javascript"></script>
    <div id="search"></div>
    <script src="https://cdn.counter.dev/script.js" data-id="83dc700a-b821-4e57-9425-02b8336a9456" data-utcoffset="0"></script>
    <script>
        window.addEventListener('DOMContentLoaded', (event) => {
            new PagefindUI({ element: "#search" });
        });
    </script>

    <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Data pipeline with ThinParQ BeeGFS and NetApp E-Series | Acting Technologist</title>
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="Data pipeline with ThinParQ BeeGFS and NetApp E-Series" />
<meta name="author" content="scaleoutSean" />
<meta property="og:locale" content="en" />
<meta name="description" content="How" />
<meta property="og:description" content="How" />
<link rel="canonical" href="https://scaleoutsean.github.io/2025/05/23/beegfs-data-pipeline.html" />
<meta property="og:url" content="https://scaleoutsean.github.io/2025/05/23/beegfs-data-pipeline.html" />
<meta property="og:site_name" content="Acting Technologist" />
<meta property="og:image" content="https://scaleoutsean.github.io/assets/images/beegfs-data-pipeline-02.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-05-23T00:00:00+08:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"scaleoutSean"},"description":"How","@type":"BlogPosting","headline":"Data pipeline with ThinParQ BeeGFS and NetApp E-Series","dateModified":"2025-05-23T00:00:00+08:00","datePublished":"2025-05-23T00:00:00+08:00","image":"https://scaleoutsean.github.io/assets/images/beegfs-data-pipeline-02.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://scaleoutsean.github.io/2025/05/23/beegfs-data-pipeline.html"},"url":"https://scaleoutsean.github.io/2025/05/23/beegfs-data-pipeline.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  </head>

<body>

  <div class="container">
    <header id="header">
	<div id = site_title>
		<a href="https://scaleoutsean.github.io/">
			<h2 style="font-size:1.8em;">Acting Technologist</h2>
		</a>
		
		<h3>
			civilizations are created by individuals
		</h3>
		
	</div>

	<div id="subheader">
		
		<nav class="pages">
<a href="/about.html">About</a>

<a href="/archive.html">Archive</a>

<a href="/categories/">Categories</a>

<a href="/projects.html">Projects</a>
</nav>
		
		
		<nav class="social">
			
  
    <a href="https://www.github.com/scaleoutsean" target="_blank" id="github"><i class="fab fa-github" aria-hidden="true"></i></a>
  
   

  
    <a href="https://twitter.com/scaleoutsean" target="_blank" id="twitter"><i class="fab fa-twitter" aria-hidden="true"></i></a>
  
   

  
  
    <a href="/feed.xml" target="_blank" id="rss"><i class="fas fa-rss" aria-hidden="true"></i></a>
   

		</nav>
		
	</div>
</header>


    <div class="post-container">
      <article id = "post">
        <h1 id = "post-title">Data pipeline with ThinParQ BeeGFS and NetApp E-Series</h1>

  
  <div class = "post-info">
    <span>
        <i class="far fa-calendar" aria-hidden="true"></i> <span>23 May 2025</span> - <i class="far fa-clock"></i> 


  
  
    9 minute read
  

    </span>
  </div>
  
        <h2 id="introduction">Introduction</h2>

<p>Earlier this week I <a href="/2025/05/17/beegfs-v8-netapp-e-series-indexing-tiering-workflows.html">blogged about new (and also “recent”) BeeGFS features and how they can be leveraged in the context of the NetApp BeeGFS/E-Series solution</a>.</p>

<p>Some parts are related to data processing and pipelines, but I felt they got lost in the length of the post.</p>

<p>This post is meant to:</p>
<ul>
  <li>Highlight just the key parts</li>
  <li>Mention some ways you can use it in your data processing</li>
</ul>

<p>Please refer to the linked post above if you don’t understand this post.</p>

<h2 id="how-it-works">How it works</h2>

<p>As mentioned in the recent post, there are two key features:</p>

<ul>
  <li>BeeGFS filesystem events (and notifications) - real time events</li>
  <li>BeeGFS filesystem index - in version 8 (and previously 7) it’s not real time, but it might become if it updates get pushed from</li>
</ul>

<p>If we want to react in real-time, we need to create a listener or processor for real-time events. The previous post shows an example of a “decider” service that listens to notifications and creates jobs based on requirements.</p>

<p>With that, as users or applications/machines create data on BeeGFS, events can be fired as soon as files are closed after writing (or removed, if we react to such events).</p>

<p><img src="/assets/images/beegfs-data-pipeline-01.png" alt="Workflow" /></p>

<h2 id="dispatch">Dispatch</h2>

<p>This diagram was supposed to be nice and make it all clear, but after two hours of monkeying around with it, this is the best I got. Perhaps I shouldn’t have tried.</p>

<p><img src="/assets/images/beegfs-data-pipeline-02.png" alt="Diagram" /></p>

<p>You can open it in a new tab to see a larger version, but I’m not sure that will help. This may be better:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">dispatcher</code> (or decider, I as called it in the post linked above) is (my custom) service that listens to BeeGFS file-system events</li>
  <li>Based on event type (see previous post), it does things
    <ul>
      <li>In some cases, it creates batch processing jobs that may take minutes or hours</li>
      <li>In others, it may create initiate events that stream data into a queue for real-time processing</li>
    </ul>
  </li>
</ul>

<p>Dispatcher doesn’t do anything else, as it wouldn’t be able to and we also don’t want to do complicated things in our own application.</p>

<p>Once jobs are dispatched - whether to stream data to Kafka or process uploaded files - these tasks are persisted in some queue that can take care of processing, retries, restarts and such.</p>

<p>Dispatcher could be written in Go or other language. If you have hundreds hundreds of files every second, Python may be able to handle it. If it’s thousands, you may need multiple dispatchers, or you may use a Go watcher.</p>

<p>What is important is that you have the means of firing jobs and processing without having some front end service through which users upload data, or needing to rescan directories to find new data.</p>

<h2 id="near-real-time-and-scheduled-processing">Near real time and scheduled processing</h2>

<p>Consider some scenarios in which real-time processing may be useful:</p>

<ul>
  <li>Machine-generated loaded to BeeGFS has higher value if processed immediately</li>
  <li>High data churn makes RAG-based applications produce poor results as chat application provides outdated answers or links to non-existing files or images</li>
</ul>

<p>In such cases, immediate processing can be beneficial both technically and business-wise.</p>

<p>In other cases, we can fall back to scheduled processing: hourly, daily, weekly. For those jobs BeeGFS index doesn’t have to be up to date - it can be refreshed periodically. As long as indexes are created before scheduled batch jobs run that is enough.</p>

<h3 id="near-real-time-data-workflows">Near real-time data workflows</h3>

<p>In recent conversations with customers, I’ve heard of these problems that may be addressed with real-time processing:</p>

<ul>
  <li>AI applications (whether it’s a “vector database” or something else) have duplicate embeddings</li>
  <li>Embeddings are “out of sync” with file-system data. For example, the file may have been deleted, or there’s no embedding when there should be</li>
  <li>Workflows are difficult to automate without file-system or directory scanning</li>
</ul>

<p>This isn’t to say BeeGFS file-system event notifications are the only, or “the best” way to solve this, but that they could be used to mitigate or fix it.</p>

<p>For the “high churn RAG environment” problem, I created the following workflow:</p>

<ul>
  <li>Dispatcher watches selected path where files used by RAG are located</li>
  <li>File delete event: file is immediately removed from vector database</li>
  <li>File create or update event: embeddings are recreated immediately</li>
  <li>Other actions can be scheduled before (antivirus scan, for example, which can be a problem for non-SMB protocols) or after</li>
</ul>

<p>Concrete steps:</p>

<ol>
  <li>For new files, just make sure the file exists (as it may have been deleted right away). Calculate file checksum to determine if the file has changed (for updated) and to store it (for new) in a database for comparison later on.</li>
  <li>For new or updated non-empty files, create new embeddings. Save them wherever you usually save them (PostgreSQL, dedicated vector database, file-system, etc.)</li>
  <li>Perform other tasks required by your stack (e.g. you may want to trigger cache expiration for particular documents, users or applications)</li>
</ol>

<h3 id="scheduled-batch-jobs">Scheduled batch jobs</h3>

<p>We an use both file-system events or Hive Index refreshes to handle these.</p>

<p>I realized that in some cases I can make use of <a href="https://doc.beegfs.io/8.0/hive/hive_index.html#beegfs-hive-index">BeeGFS Hive indices</a> even in a combination of “near real time” and scheduled processing. How?</p>

<p>First, we can create an out-of-tree (BeeGFS FS directory tree) index.</p>

<p><img src="/assets/images/beegfs-data-pipeline-03.png" alt="Out-of-Tree BeeGFS Hive Index" /></p>

<p>As explained in the linked post, these are SQLite databases. Example of records from the <code class="language-plaintext highlighter-rouge">entries</code> table which has individual files and directories (just several more important columns):</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sqlite</span><span class="o">&gt;</span> <span class="k">SELECT</span> <span class="n">id</span><span class="p">,</span><span class="n">name</span><span class="p">,</span><span class="k">type</span><span class="p">,</span><span class="n">inode</span><span class="p">,</span><span class="k">mode</span><span class="p">,</span><span class="n">nlink</span><span class="p">,</span><span class="n">uid</span><span class="p">,</span><span class="n">gid</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">entries</span><span class="p">;</span>
<span class="mi">1</span><span class="o">|</span><span class="n">flights</span><span class="o">-</span><span class="mi">1</span><span class="n">m_v2</span><span class="p">.</span><span class="n">csv</span><span class="o">|</span><span class="n">f</span><span class="o">|</span><span class="mi">1909176012111188957</span><span class="o">|</span><span class="mi">33204</span><span class="o">|</span><span class="mi">1</span><span class="o">|</span><span class="mi">1000</span><span class="o">|</span><span class="mi">1000</span><span class="o">|</span>
<span class="mi">2</span><span class="o">|</span><span class="n">flights</span><span class="o">-</span><span class="mi">1</span><span class="n">m_v3</span><span class="p">.</span><span class="n">csv</span><span class="o">|</span><span class="n">f</span><span class="o">|</span><span class="mi">513814766249959608</span><span class="o">|</span><span class="mi">33204</span><span class="o">|</span><span class="mi">1</span><span class="o">|</span><span class="mi">1000</span><span class="o">|</span><span class="mi">1000</span><span class="o">|</span>
<span class="mi">3</span><span class="o">|</span><span class="n">flights</span><span class="o">-</span><span class="mi">1</span><span class="n">m</span><span class="p">.</span><span class="n">csv</span><span class="o">|</span><span class="n">f</span><span class="o">|</span><span class="mi">16836383133063020310</span><span class="o">|</span><span class="mi">33204</span><span class="o">|</span><span class="mi">1</span><span class="o">|</span><span class="mi">1000</span><span class="o">|</span><span class="mi">1000</span><span class="o">|</span>
</code></pre></div></div>

<p>As files-system notifications do not yet update Hive Index, we can insert them on our own (as long as index refresh on the same database isn’t running at the same time and we do it right).</p>

<p>But let’s consider the scenario in which the file has been updated (which was the problem one customer had - updates, edits or “replacements”). I can find it in this database, and the size will likely be different. If there’s no risk of false negatives, I can update embeddings and invalidate cache. If there is risk of false negatives:</p>

<ul>
  <li>Ensure the database table has columns <code class="language-plaintext highlighter-rouge">sha256</code> (checksum) and <code class="language-plaintext highlighter-rouge">embedding</code> (if I want to reference or store it in this database).</li>
  <li>Calculate checksum and embeddings, insert records in table</li>
  <li>Invalidate application cache</li>
</ul>

<p>In my proof of concept, I thought most users who are happy with these indexes being SQLite databases will also be happy to have embeddings in SQLite as well. To do that, I:</p>

<ul>
  <li>Calculate and store embeddings in a new table</li>
  <li>Update record in entries table to refer to embeddings for the document in “vector” table</li>
</ul>

<p>The assumption - which I haven’t tested yet, as my PoC experiment is still running - is that on next run BeeGFS Hive Index update won’t err because the table <code class="language-plaintext highlighter-rouge">entries</code> has two extra columns.</p>

<p><img src="/assets/images/beegfs-data-pipeline-04.png" alt="Having a GPU might help" /></p>

<p>(Having a GPU or using KB-sized data set would have been helpful. It seems this might take time…)</p>

<p>Alternatively - if that turns out to be a problem or if the user prefers to use a “real” database, we can load SQLite data into PostgreSQL or other database. This is vastly more complicated and in this situation we may prefer to create index entries with Dispatcher, since it knows about all filesystem events and we can get the important data (entry name, type, UID, GID, size) by ourselves without special BeeGFS commands.</p>

<p>In the case you haven’t thought about that:</p>

<ul>
  <li>BeeGFS Hive Index is “file-system focused” (duh!)</li>
  <li>Knowing that, the UID and GID information may seem unimportant “for AI” but that’s not true. Because of ACLs, they may reflect actual individual owners or “file-system tenants” and if ACLs reflect that, we can use that information to enhance effectiveness of our AI</li>
</ul>

<p>Examples:</p>

<ul>
  <li>When querying shared knowledge base data, our own (based on UID) and team (GID) files can be given priority so that we can get more relevant results</li>
  <li>UID/GID can be used to restrict search access to the user and/or group</li>
  <li>UID/GID can probably be leveraged to build per-group or per-user cache, or achieve some other interesting customizations or enhance security</li>
</ul>

<h2 id="scalability-for-real-time-processing">Scalability for real-time processing</h2>

<p>Although I’ve already explained that, I know some may still wonder if Python is up to the task here.</p>

<p>It doesn’t have to be, but (see the linked post) for the most robust event processing we should build a listener service (based on the example from ThinkParQ documentation, which uses Go) rather than use the legacy UNIX socket listener which is how I listen to BeeGFS events in the linked post.</p>

<p>Having said that, I doubt many users have many hundreds of <em>actionable file-system events</em> per second. Generic events, yes, but those are already processed by their existing stream-processing queues and databases. This is just the <em>files</em> and just those that need to be acted upon <em>immediately</em>. In this scenario, 100 per second is a lot (almost 9 million a day).</p>

<h2 id="conclusion">Conclusion</h2>

<p>Having an AI “reference platform” seems fashionable and reminds me of LAMP and other Web stacks from 20 years ago.</p>

<p>There seems to be a lot of bloat in those platforms. I’m sure there’s a lot of convenience, too, but I’d rather start with a slim stack and add what I have to.</p>

<p>While working on these PoC workflows, I used the following:</p>

<ul>
  <li>Langchain - workflows</li>
  <li>Llama Index - data integration</li>
  <li>Redis, SQLite  - cache</li>
  <li>PostgreSQL, SQLite - embeddings</li>
</ul>

<p>Then, as I installed various missing pieces, I ended up with all sorts of 3rd party packages, over 1GB of NVIDIA modules (which I didn’t even need, as I don’t have a GPU) to various other things I’ve never heard of!</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>pip list | <span class="nb">wc</span> <span class="nt">-l</span>
141
</code></pre></div></div>

<p>That is terrible! I’d rather invest time in reducing bloat than trying to implement some “AI reference stack” that will cause problems down the road. The less the better!</p>

<p>The NetApp BeeGFS solution with Series doesn’t an “AI reference platform”. But you don’t need one. This is just Linux, there’s nothing proprietary involved, so you <em>don’t need</em> any complex stack. Whatever you’ve used elsewhere will likely work here, too, and to plug into BeeGFS all you need to do is consume filesystem modification events and build/refresh Hive indexes!</p>

<p>Regarding databases, one can run <a href="/2022/08/11/nomad-pack-influxdb-beegfs.html">databases on BeeGFS</a>, but that’s less effective. Redis cache can be persisted to BeeGFS, and BeeGFS CSI can be integrated with Velero to back up that data. But for “on-disk” databases such as <a href="/2023/10/17/netapp-eseries-raid1-vs-raid6-ddp-comparison.html">PostgreSQL</a> and message queues (such as Kafka), I suggest to build a 2 or 3-node cluster that uses standard Linux file-systems as explained in the <a href="/2025/05/21/opean-ai-with-netapp-eseries.html#e-series-storage-and-solution-stack-for-opea">OPEA</a> post written this week.</p>

<p>With these two elements (BeeGFS and databases) in place, it is possible to build a fast and capable real-time data pipeline that is simple, robust and easy to maintain.</p>

      </article>
    </div>
    
      <div class="categories">
    <span><p>Categories:</p>
    
    
      <a href="
      /categories/#storage">storage</a>
      &nbsp; 
    
      <a href="
      /categories/#ai">ai</a>
      &nbsp; 
    
      <a href="
      /categories/#analytics">analytics</a>
       
    
  </span>
</div>
    

  
    <div>
      <h3>Related Posts</h3>
      <ul>
      
        <li><a href="/2025/05/21/opean-ai-with-netapp-eseries.html">OPEA AI with NetApp E-Series</a></li>
      
        <li><a href="/2025/05/17/beegfs-v8-netapp-e-series-indexing-tiering-workflows.html">ThinkParQ BeeGFS v8 with NetApp E-Series</a></li>
      
        <li><a href="/2023/11/28/postgres-pgvector-instacluster-eseries.html">Postgres, pgvector, E-Series and Instaclustr</a></li>
      
        <li><a href="/2022/04/05/nomad-beegfs-eseries.html">Nomad batch jobs with BeeGFS and E-Series</a></li>
      
        <li><a href="/2023/01/12/beegfs-eseries-hybrid-cloud-spot-ocean-spark.html">Burst on-prem GPU workloads from BeeGFS/E-Series clusters to Spot Ocean for Spark in the cloud</a></li>
      
      </ul>
    </div>
  

    
  </div><footer class= "footer">
    <p>2025-05-23 19:57 </p>
    <p>Copyright © 2025 scaleoutSean. Content is released under the CC BY license. Design: Alessio Franceschi</p>
</footer>

</body>
</html>
