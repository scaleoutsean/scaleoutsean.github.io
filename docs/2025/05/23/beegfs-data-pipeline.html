<!doctype html>
<html lang="en">
  <head>
        <meta charset="UTF-8">
    <meta http-equiv="content-language" content="en">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="googlebot" content="noarchive">
    <meta name="googlebot" content="max-snippet:60">
    <meta name="googlebot" content="max-image-preview:small">
    <meta name="bingbot" content="noarchive">
    <meta name="bingbot" content="max-snippet:60">
    <meta name="bingbot" content="max-image-preview:small">
    <meta name="robots" content="noarchive">
    <meta name="robots" content="max-snippet:60">
    <meta name="google-site-verification" content="F6q7vIwQ2G0j8tk-KL9rAXOMLXMDMMUkEz4fRs1Nnnc" />
    <title>
        
            Data pipelines with ThinParQ BeeGFS and NetApp E-Series | Acting Technologist
      
    </title>
    <meta name="description" content="
     How 
     ">

    <!-- LINK TO ATOM FEED FOR SEO -->
    <link rel="alternate" type="application/atom+xml" href="https://scaleoutsean.github.io/feed.xml" />

    <!-- FAVICON -->
    <link rel="icon" href="/assets/favicon.ico" type="image/x-icon">
    <link rel="shortcut icon" href="/assets/favicon.ico" type="image/x-icon">

    <!-- CSS -->
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="stylesheet" href="/assets/css/syntax.css">

    <!-- Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css">

    <link href="/_pagefind/pagefind-ui.css" rel="stylesheet">
    <script src="/_pagefind/pagefind-ui.js" type="text/javascript"></script>
    <div id="search"></div>
    <script src="https://cdn.counter.dev/script.js" data-id="83dc700a-b821-4e57-9425-02b8336a9456" data-utcoffset="0"></script>
    <script>
        window.addEventListener('DOMContentLoaded', (event) => {
            new PagefindUI({ element: "#search" });
        });
    </script>

    <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Data pipelines with ThinParQ BeeGFS and NetApp E-Series | Acting Technologist</title>
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="Data pipelines with ThinParQ BeeGFS and NetApp E-Series" />
<meta name="author" content="scaleoutSean" />
<meta property="og:locale" content="en" />
<meta name="description" content="How" />
<meta property="og:description" content="How" />
<link rel="canonical" href="https://scaleoutsean.github.io/2025/05/23/beegfs-data-pipeline.html" />
<meta property="og:url" content="https://scaleoutsean.github.io/2025/05/23/beegfs-data-pipeline.html" />
<meta property="og:site_name" content="Acting Technologist" />
<meta property="og:image" content="https://scaleoutsean.github.io/assets/images/beegfs-data-pipeline-02.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-05-23T00:00:00+08:00" />
<script type="application/ld+json">
{"image":"https://scaleoutsean.github.io/assets/images/beegfs-data-pipeline-02.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://scaleoutsean.github.io/2025/05/23/beegfs-data-pipeline.html"},"author":{"@type":"Person","name":"scaleoutSean"},"description":"How","@type":"BlogPosting","url":"https://scaleoutsean.github.io/2025/05/23/beegfs-data-pipeline.html","headline":"Data pipelines with ThinParQ BeeGFS and NetApp E-Series","dateModified":"2025-05-23T00:00:00+08:00","datePublished":"2025-05-23T00:00:00+08:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  </head>

<body>

  <div class="container">
    <header id="header">
	<div id = site_title>
		<a href="https://scaleoutsean.github.io/">
			<h2 style="font-size:1.8em;">Acting Technologist</h2>
		</a>
		
		<h3>
			civilizations are created by individuals
		</h3>
		
	</div>

	<div id="subheader">
		
		<nav class="pages">
<a href="/about.html">About</a>

<a href="/archive.html">Archive</a>

<a href="/categories/">Categories</a>

<a href="/projects.html">Projects</a>
</nav>
		
		
		<nav class="social">
			
  
    <a href="https://www.github.com/scaleoutsean" target="_blank" id="github"><i class="fab fa-github" aria-hidden="true"></i></a>
  
   

  
    <a href="https://twitter.com/scaleoutsean" target="_blank" id="twitter"><i class="fab fa-twitter" aria-hidden="true"></i></a>
  
   

  
  
    <a href="/feed.xml" target="_blank" id="rss"><i class="fas fa-rss" aria-hidden="true"></i></a>
   

		</nav>
		
	</div>
</header>


    <div class="post-container">
      <article id = "post">
        <h1 id = "post-title">Data pipelines with ThinParQ BeeGFS and NetApp E-Series</h1>

  
  <div class = "post-info">
    <span>
        <i class="far fa-calendar" aria-hidden="true"></i> <span>23 May 2025</span> - <i class="far fa-clock"></i> 


  
  
    20 minute read
  

    </span>
  </div>
  
        <ul>
  <li><a href="#introduction">Introduction</a></li>
  <li><a href="#beegfs-file-system-events-and-indexes">BeeGFS file-system events and indexes</a></li>
  <li><a href="#dispatch-jobs-for-data-operations">Dispatch jobs for data operations</a></li>
  <li><a href="#near-real-time-vs-scheduled-processing">Near real-time vs scheduled processing</a>
    <ul>
      <li><a href="#near-real-time-data-workflows">Near real-time data workflows</a></li>
      <li><a href="#scheduled-batch-jobs">Scheduled batch jobs</a></li>
    </ul>
  </li>
  <li><a href="#scalability-of-real-time-processing">Scalability of real-time processing</a></li>
  <li><a href="#netapp-datops-toolkit-dot">NetApp DatOps Toolkit (DOT)</a></li>
  <li><a href="#the-need-for-ai-reference-stack">The need for “AI reference stack”</a>
    <ul>
      <li><a href="#road-to-bloat">Road to bloat</a></li>
      <li><a href="#databases">Databases</a></li>
    </ul>
  </li>
  <li><a href="#csi-driver-for-e-series">CSI driver for E-Series</a></li>
  <li><a href="#two--or-three-server-node-database-clusters">Two- or three-server node database clusters</a></li>
  <li><a href="#on-demand-filesystems">On-demand filesystems</a>
    <ul>
      <li><a href="#use-cases-for-beeond">Use cases for BeeOND</a></li>
    </ul>
  </li>
  <li><a href="#conclusion">Conclusion</a></li>
</ul>

<h2 id="introduction">Introduction</h2>

<p>Earlier this week I <a href="/2025/05/17/beegfs-v8-netapp-e-series-indexing-tiering-workflows.html">blogged about new (and also “recent”) BeeGFS features and how they can be leveraged in the context of the NetApp BeeGFS/E-Series solution</a>.</p>

<p>This post will:</p>

<ul>
  <li>Focus on file-system events and indexes for data processing, workflows and pipelines</li>
  <li>Comment on TR-4890 (see below) which is becoming long in tooth due to changes in recent BeeGFS versions (especially v8.0)</li>
</ul>

<p>Please refer to the linked post above for some background on that.</p>

<p>NetApp TR-4890 (MLOps with NetApp and BeeGFS, McCormick &amp; Upton, May 2021) can (currently) be found <a href="https://www.netapp.com/media/31317-tr-4890.pdf">here</a>. I’ll refer to it as “the TR” in the rest of this post. The TR was written exactly four years ago, before BeeGFS v8 and before E-Series models E4000, EF300C and EF600C (the latter two are all-QLC arrays) existed.</p>

<h2 id="beegfs-file-system-events-and-indexes">BeeGFS file-system events and indexes</h2>

<p>As mentioned in that recent post, new ways to create semi-automated data pipelines are enabled are two key features:</p>

<ul>
  <li>BeeGFS file-system events (and notifications) - real time file-system events</li>
  <li>BeeGFS file-system index (“Hive Index”) - in version 8 (and previously 7) it’s not real time, but it might become if it ThinkParQ makes it possible to use file-system notifications to keep indexes up-to-date</li>
</ul>

<p>If we want to react to new data in real-time, we need to create a listener or processor for real-time events. The previous post shows an example of a “decider” service that listens to notifications and creates data processing jobs, or kicks off workflows, based on our requirements.</p>

<p>With that, as users or applications/machines create data on BeeGFS, events can be fired as soon as files are closed after writing (or removed, if we react to such events).</p>

<p><img src="/assets/images/beegfs-data-pipeline-01.png" alt="Workflow" /></p>

<h2 id="dispatch-jobs-for-data-operations">Dispatch jobs for data operations</h2>

<p>This diagram was supposed to look nice and make it all clear but, after two hours of monkeying around with it, this is the best I got. Perhaps I shouldn’t have tried.</p>

<p><img src="/assets/images/beegfs-data-pipeline-02.png" alt="Diagram" /></p>

<p>You can open it in a new tab to see a larger version, but I’m not sure that will help. This may be better:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">dispatcher</code> (or Decider, I as called it in the post linked above) is (my custom) service that listens to BeeGFS file-system events</li>
  <li>Based on event type (see previous post), it can do things, such as:
    <ul>
      <li>Create data processing jobs that don’t need to start right away, and may take seconds or minutes or hours to complete</li>
      <li>Stream data to other applications for real-time processing</li>
    </ul>
  </li>
</ul>

<p>Dispatcher doesn’t do anything else, as it wouldn’t be able to (it’d be a very complex dispatcher!) and goes against the primary mission, which is fast and simple dispatch. Think of it as “poor man’s Kafka”. It could be even simpler and act as “poor man’s event forwarder” that feeds events to something like Kafka where producers consume events.</p>

<p>In this implementation once jobs are dispatched - whether it’s to forward events elsewhere, stream data to Kafka, or use a service to process these files - these tasks end up persisted in some application’s queue that can take care of processing, retries, restarts and such.</p>

<p>A dispatcher could be written in Go or other language. If you have hundreds hundreds of files modified, created and deleted every second, Python may be able to handle it. If it’s thousands or tens of thousands per second, you may need multiple dispatchers, or you may use a dispatcher in Go (which should be significant faster).</p>

<p>What is important is that we have the means of firing jobs and processing data without having some front end service through which users upload data, or needing to rescan directories to find new or changed files.</p>

<h2 id="near-real-time-vs-scheduled-processing">Near real-time vs scheduled processing</h2>

<p>Consider some scenarios in which near real-time processing can be useful:</p>

<ul>
  <li>Machine-generated data whose value rapidly declines with time (e.g. stock market events)</li>
  <li>High data churn makes RAG-based applications produce poor results as chat application provides outdated answers or even refers to non-existing information (documents that have been updated or corrected, for example)</li>
</ul>

<p>In such cases immediate processing can be beneficial both technically and business-wise.</p>

<p>In other cases, we can fall back to scheduled “batch” processing: hourly, daily, weekly, or best-effort. For such jobs, BeeGFS indexes don’t have to be up to date - they can be refreshed periodically. As long as indexes are created before scheduled batch jobs run, that is enough.</p>

<h3 id="near-real-time-data-workflows">Near real-time data workflows</h3>

<p>In recent conversations with customers, I’ve heard of problems that may be addressed with real-time processing:</p>

<ul>
  <li>AI applications (whether it’s a “vector database” or something else) have duplicate embeddings or other data</li>
  <li>Embeddings are “out of sync” with file-system data. For example, a file may have been deleted, or RAG has no embeddings for a file we expect to be accessible to RAG</li>
  <li>Workflows are difficult to automate without file-system events or frequent directory scanning</li>
</ul>

<p>This isn’t to say BeeGFS file-system event notifications are the only or “best” way to solve this, but that they could be used to mitigate or fix such problems.</p>

<p>For the “high churn RAG environment” problem, I created the following PoC workflow:</p>

<ul>
  <li>Dispatcher watches selected path where files used by RAG are located</li>
  <li>File delete event: file is immediately removed from vector database</li>
  <li>File create or update event: embeddings are recreated immediately</li>
  <li>Other actions can be scheduled before (antivirus scan, for example, which can be a problem for non-SMB protocols) or after</li>
</ul>

<p>Specific steps:</p>

<ol>
  <li>For new files, just make sure the file exists (as it may have been deleted right away). Calculate file checksum to determine if the file has changed (for updated) and to store it (for new) in a database for comparison later on.</li>
  <li>For new or updated non-empty files, create new embeddings. Save them wherever you usually save them (PostgreSQL, dedicated vector database, file-system, etc.)</li>
  <li>Perform other tasks required by your stack (e.g. you may want to trigger cache expiration for particular documents, users or applications)</li>
</ol>

<h3 id="scheduled-batch-jobs">Scheduled batch jobs</h3>

<p>We can use both file-system events or Hive Index refreshes to handle these requirements.</p>

<p>I realized that in some cases I can make use of <a href="https://doc.beegfs.io/8.0/hive/hive_index.html#beegfs-hive-index">BeeGFS Hive indexes</a> even in a combination of “near real-time” and scheduled processing. How?</p>

<p>First, we can create an out-of-tree (BeeGFS FS directory tree) index.</p>

<p><img src="/assets/images/beegfs-data-pipeline-03.png" alt="An out-of-Tree BeeGFS Hive Index" /></p>

<p>As explained in the linked post, these are SQLite databases. Example of records from the <code class="language-plaintext highlighter-rouge">entries</code> table which has individual files and directories (just several more important columns):</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sqlite</span><span class="o">&gt;</span> <span class="k">SELECT</span> <span class="n">id</span><span class="p">,</span><span class="n">name</span><span class="p">,</span><span class="k">type</span><span class="p">,</span><span class="n">inode</span><span class="p">,</span><span class="k">mode</span><span class="p">,</span><span class="n">nlink</span><span class="p">,</span><span class="n">uid</span><span class="p">,</span><span class="n">gid</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">entries</span><span class="p">;</span>
<span class="mi">1</span><span class="o">|</span><span class="n">flights</span><span class="o">-</span><span class="mi">1</span><span class="n">m_v2</span><span class="p">.</span><span class="n">csv</span><span class="o">|</span><span class="n">f</span><span class="o">|</span><span class="mi">1909176012111188957</span><span class="o">|</span><span class="mi">33204</span><span class="o">|</span><span class="mi">1</span><span class="o">|</span><span class="mi">1000</span><span class="o">|</span><span class="mi">1000</span><span class="o">|</span>
<span class="mi">2</span><span class="o">|</span><span class="n">flights</span><span class="o">-</span><span class="mi">1</span><span class="n">m_v3</span><span class="p">.</span><span class="n">csv</span><span class="o">|</span><span class="n">f</span><span class="o">|</span><span class="mi">513814766249959608</span><span class="o">|</span><span class="mi">33204</span><span class="o">|</span><span class="mi">1</span><span class="o">|</span><span class="mi">1000</span><span class="o">|</span><span class="mi">1000</span><span class="o">|</span>
<span class="mi">3</span><span class="o">|</span><span class="n">flights</span><span class="o">-</span><span class="mi">1</span><span class="n">m</span><span class="p">.</span><span class="n">csv</span><span class="o">|</span><span class="n">f</span><span class="o">|</span><span class="mi">16836383133063020310</span><span class="o">|</span><span class="mi">33204</span><span class="o">|</span><span class="mi">1</span><span class="o">|</span><span class="mi">1000</span><span class="o">|</span><span class="mi">1000</span><span class="o">|</span>
</code></pre></div></div>

<p>As files-system notifications do not yet update Hive Index(es), we can insert them into Hive index DBs on our own (as long as index refresh on the same database isn’t running at the same time and we do it right).</p>

<p>But let’s consider the scenario in which the file has been updated (which was the problem one customer had - updates, edits or “replacements”). I can find it in this database, and the size will likely be different. If there’s no risk of false negatives, I can update embeddings and invalidate cache. If there is risk of false negatives:</p>

<ul>
  <li>Ensure the database table has columns <code class="language-plaintext highlighter-rouge">sha256</code> (checksum) and <code class="language-plaintext highlighter-rouge">embedding</code> (if I want to reference or store it in this database).</li>
  <li>Calculate checksum and embeddings, insert records in table</li>
  <li>Invalidate application cache</li>
</ul>

<p>In my proof of concept, I thought most users who are happy with these indexes being SQLite databases will also be happy to have embeddings in SQLite as well. To do that, I:</p>

<ul>
  <li>Calculate and store embeddings in a new table</li>
  <li>Update record in entries table to refer to embeddings for the document in “vector” table</li>
</ul>

<p>The assumption - which I haven’t tested yet, as my PoC experiment is still running - is that on next run BeeGFS Hive Index update won’t err because the table <code class="language-plaintext highlighter-rouge">entries</code> has two extra columns.</p>

<p><img src="/assets/images/beegfs-data-pipeline-04.png" alt="Having a GPU might help" /></p>

<p>(Having a GPU or using KB-sized data set would have been helpful. It seems this might take time… Update: hours later it got killed due to OoM, maybe that was one of the final phases of refreshing embeddings. I’ll need to change data, re-run and update this post, if I remember.)</p>

<p>Alternatively - if that turns out to be a problem or if the user prefers to use a “real” database, we can load SQLite data into PostgreSQL or other database. This is vastly more complicated and in this situation we may prefer to create index entries with Dispatcher, since it knows about all file-system events and we can get the important data (entry name, type, UID, GID, size) by ourselves without special BeeGFS commands.</p>

<p>In the case you haven’t thought about that:</p>

<ul>
  <li>BeeGFS Hive Index is “file-system focused” (duh!)</li>
  <li>Knowing that, the UID and GID information may seem unimportant “for AI” but that’s not true. Because of ACLs, they may reflect actual individual owners or “file-system tenants” and if ACLs reflect that, we can use that information to enhance effectiveness of our AI</li>
</ul>

<p>Examples:</p>

<ul>
  <li>When querying shared knowledge base data, our own (based on UID) and team (GID) files can be given priority so that we can get more relevant results</li>
  <li>UID/GID can be used to restrict search access to the user and/or group</li>
  <li>UID/GID can probably be leveraged to build per-group or per-user cache, or achieve some other interesting customizations or enhance security</li>
</ul>

<h2 id="scalability-of-real-time-processing">Scalability of real-time processing</h2>

<p>Although I’ve already explained that, I know some may wonder if Python is up to the task here.</p>

<p>It doesn’t have to be, but (see the linked post) for the most robust event processing we should build a listener service (based on the example from ThinkParQ documentation, which uses Go) rather than use the legacy UNIX socket listener which is how I listen to BeeGFS events in the linked post.</p>

<p>Having said that, I doubt many users have many hundreds of <em>actionable file-system events</em> per second. Generic events, yes, but those are already processed by their existing stream-processing queues and databases. These are just <em>lists of files</em> and only those that need to be acted upon <em>immediately</em>. In this scenario, 100 per second is a lot (almost 9 million a day).</p>

<p>We know from <a href="https://docs.netapp.com/us-en/beegfs/second-gen/beegfs-design-solution-verification.html#metadata-performance-test">the metadata tests in the BeeGFS-E-Series documentation</a> that file creation events may be be up to in thousands per second, but I’d say most such files are processed within compute jobs and often deleted after work is done. It is usually only the result that concerns us and for it we don’t even need to “watch” the file-system when job schedules take care of the next step in compute pipeline.</p>

<p>I think the main scenario for watching file-system for new files are things like new machine generated data copied to the file-system. Not many users have over 1,000 new files every second and those may need a fast event listener. Secondly, in many cases it will be feasible to run reindex the hot directory every minute, process the files and then move them to an archive or upload to a low cost S3 tier.</p>

<h2 id="netapp-datops-toolkit-dot">NetApp DatOps Toolkit (DOT)</h2>

<p>tldr: if you don’t use it, you don’t need it.</p>

<p>If you’re curious what DOT is see <a href="https://github.com/NetApp/netapp-dataops-toolkit">this</a>. It’s a Python wrapper for file/object copy jobs such as:</p>

<ul>
  <li>BeeGFS to ONTAP NFS or SMB (in either direction)</li>
  <li>BeeGFS to STorageGRID or ONTAP S3 (either direction)</li>
</ul>

<p>The TR mentions it, but is DOT still relevant for BeeGFS v8?</p>

<p>In the case of BeeGFS, it’s a wrapper for tools you probably already use (e.g. rsync), so it’s not strictly necessary for BeeGFS users. Some impacts of BeeGFS v8:</p>

<ul>
  <li>with BeeGFS sync (see the linked post) we can sync to/from S3 faster than with DOT or S3cmd (which the TR mentions as one of sync tools)</li>
  <li>with BeeGFS Hive Index (see the linked post) we may be able to avoid large file-system scanning with rsync and feed Hive index entries to rsync or other copy utility</li>
  <li>with BeeGFS file-system events we can avoid sync and copy-on-create, for near real-time “mirroring” of BeeGFS data to any destination (S3, NFS, SMB…)</li>
</ul>

<p>There’s probably more, but the main point is: you probably no longer need DOT, unless you’re also a heavy user of DOT with ONTAP systems.</p>

<h2 id="the-need-for-ai-reference-stack">The need for “AI reference stack”</h2>

<p>I see these “recommendations” regarding “stacks” and they mildly go on my nerves. Having an AI “reference stack” seems fashionable among storage vendors and reminds me of “LAMP” and similar stacks 20 years ago.</p>

<p>There seems to be a lot of bloat in many of those those “recommended” platforms with “batteries included”. Some users benefit from the extra convenience, but not all do.</p>

<p>I prefer to start with a <em>slim</em> stack I can understand, and add only what I have to, over deploying a <em>fat</em> stack on day 0. And as someone who makes recommendations (which others may or may not consider) I prefer to ask a few questions first rather than promote a bloated “stack” the user doesn’t necessarily need.</p>

<h3 id="road-to-bloat">Road to bloat</h3>

<p>While working on these proof-of-concept workflows with BeeGFS, I used the following:</p>

<ul>
  <li>Langchain - workflows</li>
  <li>Llama Index - data integration</li>
  <li>Redis, SQLite  - KV cache</li>
  <li>PostgreSQL, SQLite - embeddings</li>
</ul>

<p>Then, as I (indiscriminately) installed various dependencies, I ended up with all sorts of 3rd party packages - for example over 1GB of NVIDIA modules (which I didn’t even need as there’s no GPU in that particular VM) to various other things I’ve never heard of!</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>pip list | <span class="nb">wc</span> <span class="nt">-l</span>
141
</code></pre></div></div>

<p>That is terrible! I’d rather invest my time and effort in reducing unnecessary bloat than trying to implement some “AI reference stack” that will cause all sorts of problems down the road. The less the better!</p>

<p>I <em>do know</em> there’s “nothing to it, just put it all in a container image”, but I also loathe the idea of auditing dozens of update packages every month, resolving dependency conflicts,  and the rest of it.</p>

<p>The NetApp BeeGFS solution with Series is a reference platform and doesn’t have an “AI reference stack”. And you probably don’t need it, either.</p>

<p>This is just Linux where you can install applications you need. Whatever you’ve used elsewhere will likely work here, too, and to plug into BeeGFS all you need to do is consume file-system modification events and build/refresh Hive indexes.</p>

<p>The NetApp BeeGFS MLOps TR does mention <a href="https://www.kubeflow.org/docs/components/">Kubeflow</a>, but doesn’t encourage the reader to look at it as a recommended “stack”. From page 24 of the TR:</p>

<blockquote>
  <p>based on the requirements of an actual internal project, each component can be easily interchanged or swapped depending on the individual project requirements.</p>
</blockquote>

<p>That’s how I see it as well.</p>

<p>Some users will use <a href="https://www.kubeflow.org/docs/components/spark-operator/overview/">Kubeflow Spark Operator</a> in the same Kubeflow cluster where they run <a href="https://www.kubeflow.org/docs/components/katib/overview/">AutoML</a>, while others have a different team who prefers to run Spark on a separate Red Hat OpenShift cluster.</p>

<p>AI users should be skeptical about any generic “stack recommendations” when recommendations are made without knowing detailed requirements.</p>

<p>VMware vSphere was a great generic recommendation 15 years ago (great choice for 9 in 10 VI users - amazing accuracy!). There’s nothing like that in AI and analytics today.</p>

<h3 id="databases">Databases</h3>

<p>The TR doesn’t mention databases, as MLOps is usually about files (even when they’re databases, e.g. Parquet on S3) and perhaps databases are seen as somewhat “out of scope” or taken care of somewhere else (e.g. on dedicated ONTAP all-SAN array, perhaps).</p>

<p>That may be true, but there’s rarely just one good way to achieve something. For example:</p>

<ul>
  <li>You may have a smaller all-in-one environment where rack space, cost or other constraints force you to consider alternatives</li>
  <li>You feel more comfortable solving your problems on the host (as many HPC and AI shops do)</li>
  <li>You value some unique E-Series features enough to use E-Series for databases as well</li>
</ul>

<p>So, you may want to look at databases on E-Series. What’s next?</p>

<p>One can run <a href="/2022/08/11/nomad-pack-influxdb-beegfs.html">databases on BeeGFS</a>. While InfluxDB works, Redis database works even better - it normally doesn’t need any disk space, but can be persisted to BeeGFS and BeeGFS CSI can be integrated with Velero to back up that data.</p>

<p>But for traditional RDBMS “on-disk” databases such as <a href="/2023/10/17/netapp-eseries-raid1-vs-raid6-ddp-comparison.html">PostgreSQL</a> and queuing systems (such as Kafka), I suggest building a 2 or 3-node cluster that uses standard Linux file-systems as explained in the <a href="/2025/05/21/opean-ai-with-netapp-eseries.html#e-series-storage-and-solution-stack-for-opea">OPEA</a> post written this week.</p>

<p><img src="/assets/images/beegfs-data-pipeline-05.svg" alt="BeeGFS for MLOps and Kubernetes for Analytics and Data Lakehouse" /></p>

<p>Smaller environments could use a single hybrid EF600 for both BeeGFS and Kubernetes. I’d only suggest to not try to put all of your all-flash LUNs on one disk group because - as the diagram above suggests - it’s prudent to leave metadata LUNs alone (at least own disk groups, if not own disk array).</p>

<p>On QLC-only all-flash models (EF600, EF300C), it isn’t possible to easily create multiple disk groups (they’re DDP (pool)-only designs), so for those I’d still suggest:</p>

<ul>
  <li>BeeGFS metadata on R1-style LUNs on DDP</li>
  <li>BeeGFS data on R6-style LUNs on DDP</li>
  <li>Non-BeeGFS (i.e. Kubernetes) LUNs on R1- or R6-style LUNs as appropriate (e.g. DB or DB write logs on R1-style LUNs, apps and DB data on R6-style LUNs)</li>
</ul>

<p>If you have between 22 and 24 disks in your E-Series all-flash array, you can even create two DDPs, in which case I’d suggest one for R1-style disks if possible, although that’s probably not going to be realistic considering that 80-90% of capacity may need to go to R6-style data disks for BeeGFS. Again, if you need to micromanage, an EF300 (for MD and databases, on TLC) and EF600C (for Data, on QLC) is a good pattern.</p>

<h2 id="csi-driver-for-e-series">CSI driver for E-Series</h2>

<p>If you wonder where to download E-Series CSI drivers for Kubernetes, the answer is they don’t exist.</p>

<p>The good news is there are enough good community CSI drivers we can use. Read <a href="/2025/05/21/opean-ai-with-netapp-eseries.html#docker-vs-kubernetes">the Kubernetes part in my OPEA post</a> on what CSI drivers we can use with E-Series.</p>

<h2 id="two--or-three-server-node-database-clusters">Two- or three-server node database clusters</h2>

<p>The diagram depicts a three-node Kubernetes cluster. This is generally recommended for bare metal and virtualized Kubernetes. In this cluster, databases are expected to be made highly available using database features (such as master-slave mirroring or three-way replication), except in the case where you run Kubernetes in VMs on a VMware and rely on VMware HA.</p>

<p>Another option is two-node Linux cluster with a VM-based Docker or VM-based Kubernetes. In this case we would probably use Pacemaker + Corosync for monitoring and failover. That’s traditional active-passive HA, with its own pro- and counter-points unrelated to E-Series.</p>

<p>Note: the TR mentions BeeGFS CSI driver. It’s available (for BeeGFS, obviously), but it’s now maintained by ThinkParQ.</p>

<h2 id="on-demand-filesystems">On-demand filesystems</h2>

<p>The TR mentions BeeOND (I also mentioned and demonstrated in the post linked at the top), which is a way to provide temporary “on-demand” BeeGFS file-systems.</p>

<p>Before BeeGFS v8, ACLs were an “enterprise feature”, but now they’re available in the community edition. Prior to version 8, users of the community edition couldn’t create ACL-protected temporary file-systems, but today that is possible.</p>

<h3 id="use-cases-for-beeond">Use cases for BeeOND</h3>

<p>I’m sure the BeeGFS documentation explains these better, but I’ll make several comments BeeOND use cases here.</p>

<p>Temporary file-systems often store data that already exists in another BeeGFS file-system. It may be a NL-SAS system where processing would be expensive, or it may be on an all-flash file-system that’s not meant for jobs with extreme IO intensity.</p>

<p>Some vendors use “tiers”, which BeeGFS also has (“pools”, which has been, and still is in v8, an enterprise feature), so data are still “copied” from a slower tier for processing. But almost no one has a RAID 0 tier. You can have a temporary BeeGFS on a tier made of E-Series RAID 0 volumes, which is pretty amazing. And you can use striping/chunking settings that “normal” BeeGFS file-systems never use.</p>

<p>Now, just like everyone else, you copy data from some slower file-system or “pool” to this BeeOND file-system. But, unlike everyone else, you have a temporary file-system that’s 2x or 3x faster than other guy’s “temporary” directories or file-systems, while using 20-30% less disk capacity (if you use LUNs on RAID 0 on E-Series with BeeOND). And, you can see in the linked post, it takes seconds to create and delete them.</p>

<p>Imagine you have a data set of 20 TB. Let’s say that copying it to BeeOND at 5 GB/s takes 4,000 seconds while others take at 5,000 seconds. Then we process data which, with an optimal BeeOND configuration and underlying RAID 0, takes 1 hour instead of 1.5 hours on a “regular” Hot Tier in 3rd party storage systems (in many cases using RAID 6-equivalent), and copy back a small 1 GB result. Savings: 1,000 seconds less to copy and 1,800 seconds less to compute in a workflow that takes less than 10,000 seconds.</p>

<p>Obviously it’s a made up “example”, but such workloads and use cases do exist.</p>

<p>To avoid having “stranded” capacity for a scratch file-system we can create a “permanent temporary file-system” with BeeOND and make it available to BeeGFS CSI users, so that PVCs can be created on it as needed. Or we could create BeeOND and small K8s clusters on demand. If it can save time or provide flexibility for regularly occurring data processing jobs, it can be valuable.</p>

<h2 id="conclusion">Conclusion</h2>

<p>With the new file-system event notifications, Hive indexes and the ability to use CSI and temporary file-systems, it is possible to build a fast and capable real-time data pipeline that is simple, robust and easy to maintain.</p>

<p>Some storage (and compute) vendors highlight “reference stacks” simply because they can’t test everything. But, as you can see in the BeeGFS TR referenced here, even the BeeGFS solution team highlighted the fact that the user can use whatever works for them.</p>

<p>This flexibility stems from the fact that with Linux, BeeGFS and E-Series (block devices) there is nothing that stops you from doing things better (or worse).</p>

<p>Vendors who sell proprietary solutions have to test and create guard-rails.</p>

<p>With BeeGFS and E-Series, you can work smarter if you know how to work smarter and don’t engineer a foot-gun.</p>

<p>As always, <strong>E-Series does almost everything you need, and almost nothing you don’t</strong>. BeeGFS is a great match for this approach.</p>

<p>More freedom, less bloat, and more responsibility!</p>

      </article>
    </div>
    
      <div class="categories">
    <span><p>Categories:</p>
    
    
      <a href="
      /categories/#storage">storage</a>
      &nbsp; 
    
      <a href="
      /categories/#ai">ai</a>
      &nbsp; 
    
      <a href="
      /categories/#analytics">analytics</a>
       
    
  </span>
</div>
    

  
    <div>
      <h3>Related Posts</h3>
      <ul>
      
        <li><a href="/2025/05/21/opean-ai-with-netapp-eseries.html">OPEA AI with NetApp E-Series</a></li>
      
        <li><a href="/2025/05/17/beegfs-v8-netapp-e-series-indexing-tiering-workflows.html">ThinkParQ BeeGFS v8 with NetApp E-Series</a></li>
      
        <li><a href="/2023/11/28/postgres-pgvector-instacluster-eseries.html">Postgres, pgvector, E-Series and Instaclustr</a></li>
      
        <li><a href="/2022/04/05/nomad-beegfs-eseries.html">Nomad batch jobs with BeeGFS and E-Series</a></li>
      
        <li><a href="/2023/01/12/beegfs-eseries-hybrid-cloud-spot-ocean-spark.html">Burst on-prem GPU workloads from BeeGFS/E-Series clusters to Spot Ocean for Spark in the cloud</a></li>
      
      </ul>
    </div>
  

    
  </div><footer class= "footer">
    <p>2025-06-05 03:19 </p>
    <p>Copyright © 2025 scaleoutSean. Content is released under the CC BY license. Design: Alessio Franceschi</p>
</footer>

</body>
</html>
