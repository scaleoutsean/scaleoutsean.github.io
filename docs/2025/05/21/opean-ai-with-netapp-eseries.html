<!doctype html>
<html lang="en">
  <head>
        <meta charset="UTF-8">
    <meta http-equiv="content-language" content="en">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="googlebot" content="noarchive">
    <meta name="googlebot" content="max-snippet:60">
    <meta name="googlebot" content="max-image-preview:small">
    <meta name="bingbot" content="noarchive">
    <meta name="bingbot" content="max-snippet:60">
    <meta name="bingbot" content="max-image-preview:small">
    <meta name="robots" content="noarchive">
    <meta name="robots" content="max-snippet:60">
    <meta name="google-site-verification" content="F6q7vIwQ2G0j8tk-KL9rAXOMLXMDMMUkEz4fRs1Nnnc" />
    <title>
        
            OPEA AI with NetApp E-Series | Acting Technologist
      
    </title>
    <meta name="description" content="
     How OPEA can work with NetApp E-Series
     ">

    <!-- LINK TO ATOM FEED FOR SEO -->
    <link rel="alternate" type="application/atom+xml" href="https://scaleoutsean.github.io/feed.xml" />

    <!-- FAVICON -->
    <link rel="icon" href="/assets/favicon.ico" type="image/x-icon">
    <link rel="shortcut icon" href="/assets/favicon.ico" type="image/x-icon">

    <!-- CSS -->
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="stylesheet" href="/assets/css/syntax.css">

    <!-- Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css">

    <link href="/_pagefind/pagefind-ui.css" rel="stylesheet">
    <script src="/_pagefind/pagefind-ui.js" type="text/javascript"></script>
    <div id="search"></div>
    <script src="https://cdn.counter.dev/script.js" data-id="83dc700a-b821-4e57-9425-02b8336a9456" data-utcoffset="0"></script>
    <script>
        window.addEventListener('DOMContentLoaded', (event) => {
            new PagefindUI({ element: "#search" });
        });
    </script>

    <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>OPEA AI with NetApp E-Series | Acting Technologist</title>
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="OPEA AI with NetApp E-Series" />
<meta name="author" content="scaleoutSean" />
<meta property="og:locale" content="en" />
<meta name="description" content="How OPEA can work with NetApp E-Series" />
<meta property="og:description" content="How OPEA can work with NetApp E-Series" />
<link rel="canonical" href="https://scaleoutsean.github.io/2025/05/21/opean-ai-with-netapp-eseries.html" />
<meta property="og:url" content="https://scaleoutsean.github.io/2025/05/21/opean-ai-with-netapp-eseries.html" />
<meta property="og:site_name" content="Acting Technologist" />
<meta property="og:image" content="https://scaleoutsean.github.io/assets/images/opea-eseries-01.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-05-21T00:00:00+08:00" />
<script type="application/ld+json">
{"image":"https://scaleoutsean.github.io/assets/images/opea-eseries-01.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://scaleoutsean.github.io/2025/05/21/opean-ai-with-netapp-eseries.html"},"author":{"@type":"Person","name":"scaleoutSean"},"description":"How OPEA can work with NetApp E-Series","@type":"BlogPosting","url":"https://scaleoutsean.github.io/2025/05/21/opean-ai-with-netapp-eseries.html","headline":"OPEA AI with NetApp E-Series","dateModified":"2025-05-21T00:00:00+08:00","datePublished":"2025-05-21T00:00:00+08:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  </head>

<body>

  <div class="container">
    <header id="header">
	<div id = site_title>
		<a href="https://scaleoutsean.github.io/">
			<h2 style="font-size:1.8em;">Acting Technologist</h2>
		</a>
		
		<h3>
			civilizations are created by individuals
		</h3>
		
	</div>

	<div id="subheader">
		
		<nav class="pages">
<a href="/about.html">About</a>

<a href="/archive.html">Archive</a>

<a href="/categories/">Categories</a>

<a href="/projects.html">Projects</a>
</nav>
		
		
		<nav class="social">
			
  
    <a href="https://www.github.com/scaleoutsean" target="_blank" id="github"><i class="fab fa-github" aria-hidden="true"></i></a>
  
   

  
    <a href="https://twitter.com/scaleoutsean" target="_blank" id="twitter"><i class="fab fa-twitter" aria-hidden="true"></i></a>
  
   

  
  
    <a href="/feed.xml" target="_blank" id="rss"><i class="fas fa-rss" aria-hidden="true"></i></a>
   

		</nav>
		
	</div>
</header>


    <div class="post-container">
      <article id = "post">
        <h1 id = "post-title">OPEA AI with NetApp E-Series</h1>

  
  <div class = "post-info">
    <span>
        <i class="far fa-calendar" aria-hidden="true"></i> <span>21 May 2025</span> - <i class="far fa-clock"></i> 


  
  
    9 minute read
  

    </span>
  </div>
  
        <ul>
  <li><a href="#introduction">Introduction</a></li>
  <li><a href="#what-is-the-opea-project">What is the OPEA project</a></li>
  <li><a href="#the-stack">The stack</a>
    <ul>
      <li><a href="#docker-vs-kubernetes">Docker vs. Kubernetes</a></li>
    </ul>
  </li>
  <li><a href="#wheres-the-storage">Where’s the storage?</a></li>
  <li><a href="#e-series-storage-and-solution-stack-for-opea">E-Series storage and solution stack for OPEA</a></li>
  <li><a href="#multi-tenancy">Multi-tenancy</a></li>
  <li><a href="#conclusion">Conclusion</a></li>
</ul>

<h2 id="introduction">Introduction</h2>

<p>NetApp and <a href="https://opea.dev">OPEA</a> (Open Platform for Enterprise AI) recently <a href="https://opea.dev/netapp-and-intel-partner-to-redefine-ai-for-enterprises-opea-inside/">announced</a> a joint solution.</p>

<p>In this post I’ll describe how an OPEA &amp; E-Series can work together.</p>

<h2 id="what-is-the-opea-project">What is the OPEA project</h2>

<p>This is the part where I could insert a keyword-peppered junk snippet created by AI, except I don’t do that here.</p>

<p>You can read about OPEA <a href="https://github.com/opea-project">here</a>. Their docs site has a brief mission statement:</p>

<blockquote>
  <p>OPEA streamlines the implementation of enterprise-grade Generative AI by efficiently integrating secure, performant, and cost-effective Generative AI workflows into business processes.</p>
</blockquote>

<p><em>Cost-effective</em> seems like a code word for “maybe you can avoid buying NVIDIA GPUs or at least software”, which in turn means relatively more DIY.</p>

<p>What does that actually mean?</p>

<h2 id="the-stack">The stack</h2>

<ol>
  <li>The good (and bad) news is any Kubernetes will do. Some variance.</li>
  <li>Kubernetes install instructions are limited to several approaches. More variance.</li>
  <li>Reference OS is just one. The good thing is it’s Ubuntu, the bad thing is it’s (only) Ubuntu. YMMV. Of course, you are free to use any other that works for you. More variance.</li>
  <li>Deployments can also happen on Docker. More flexibility, more freedom, and more variance!</li>
</ol>

<p><img src="/assets/images/opea-eseries-01.png" alt="OPEA stack" /></p>

<p>Source: <a href="https://github.com/opea-project/GenAIExamples?tab=readme-ov-file#deployment-guide">GenAIExample README.md as of 2025/05/21</a></p>

<p>That’s what “DIY” means.</p>

<p>There’s a lot of freedom and flexibility, but at the same time you may end up with a lot of experimenting and no two deployments that look exactly the same.</p>

<h3 id="docker-vs-kubernetes">Docker vs. Kubernetes</h3>

<p>If you’re not doing OPEA at scale - say, you’re a department or a ROBO site with 2 Intel servers, Docker may be fine or even better.</p>

<ul>
  <li>it costs nothing</li>
  <li>it’s easy to deploy in seconds</li>
  <li>it doesn’t require Kubernetes management personnel</li>
  <li>it can run on one (no HA) or two (HA) servers with “classic” HA failover</li>
  <li>it can <em>easily</em> share server and storage with whatever other stuff you have in your ROBO site (VMware, etc.)</li>
</ul>

<h2 id="wheres-the-storage">Where’s the storage?</h2>

<p>Oh, the storage section is just below that Kubernetes section in the README file - I couldn’t capture a larger screenshot, sorry! Actually, that’s <strong>not true</strong>. There’s no storage section!</p>

<p>You can use any storage that works. It’s completely transparent to OPEA - as long as the OS can use it, you’re good to go.</p>

<p>More precisely in terms of E-Series:</p>

<ul>
  <li>For OPEA in Docker, all you need is for your OS (whether it’s the reference OS or some other) to be on the NetApp E-Series IMT for the protocol you use. For example, <a href="/2022/10/26/e-series-rocky-linux.html">Rocky Linux</a> is on the list, but (gotcha!) there’s no Docker RPM for Rocky Linux (as of now). Luckily - because you’re a fiddler (otherwise you wouldn’t be using OPEA) - that’s not a problem for you so you’ll install the CentOS or Red Hat RPM.</li>
  <li>For OPEA on Kubernetes, things get more complicated
    <ul>
      <li>Kubernetes in VMware - this becomes just VMware with E-Series because vSphere CSI driver takes care of storage provisioning. See <a href="/2022/05/18/vmware-tanzu-netapp-eseries.html">this</a>. As long as your vSphere is on the IMT list for E-Series, the rest is up to vSphere CSI driver. If you need NFS, deploy a Linux VM and let VMware HA take care of HA.</li>
      <li>Kubernetes in other VI - several choices
        <ul>
          <li>use <a href="/2022/04/09/beegfs-csi-introduction.html">BeeGFS CSI</a> for shared data access (either on VMFS or block devices from E-Series)</li>
          <li>setup a Linux (VM) NFS server and <a href="https://github.com/kubernetes-csi/csi-driver-nfs">CSI driver for NFS</a>. If you need HA, let VI HA (or set up own Pacemaker/Corosync) take care of it.</li>
          <li>Proxmox, LXD, and Incus are all good choices. They’re not in the E-Series IMT, but they don’t need to be; the OS is and that’s enough. There’s no specific plugin that would require these to be specifically listed in the IMT.</li>
        </ul>
      </li>
      <li>Kubernetes on bare metal Linux - see <a href="/2022/12/09/directpv-topolvm-csi-lvm-das-k8s-with-eseries.html">this</a>. Of course, BeeGFS CSI is also available.</li>
    </ul>
  </li>
</ul>

<h2 id="e-series-storage-and-solution-stack-for-opea">E-Series storage and solution stack for OPEA</h2>

<p>Storage services:</p>

<ul>
  <li>Single-host block storage (<strong>native</strong>)
    <ul>
      <li>any E-Series block devices (SAS, iSCSI, FC, NVMe, etc.)</li>
    </ul>
  </li>
  <li>NFS service
    <ul>
      <li>ONTAP Select 9 (VM that can run on VMware or Redhat-based KVM hypervisor)</li>
      <li>Classic Linux NFS server VM (HA with Pacemaker/Corosync or VMware HA)</li>
      <li>ZFS-based Linux NFS server VM (same HA recipe, but rich snapshot and efficiency features)</li>
    </ul>
  </li>
  <li>Parallel file system (shared block storage)
    <ul>
      <li>BeeGFS (on bare metal servers, in <a href="/2024/02/15/storagegrid-on-vmware.html">VMs</a>, or in <a href="/2023/12/02/containerized-beegfs-with-netapp-eseries.html">Docker</a> or Kubernetes); Enterprise and Community Editions are available</li>
    </ul>
  </li>
  <li>S3 service
    <ul>
      <li>NetApp StorageGRID (bare metal or VMs (at least 3))</li>
      <li><a href="/2023/09/20/versity-gw-s3-posix-gateway-beegfs-eseries.html">Versity S3 Gateway</a> (can be S3 gateway to Linux NFS server or BeeGFS directory/filesystem) or generic S3 gateway on your Linux VM</li>
      <li><a href="/2023/09/03/minio-erasure-coding-and-netapp-e-series.html">MinIO</a> server (HA with 3 VM/container deployment (EC 2+1) or 1 VM with VMware HA (no EC needed))</li>
      <li><a href="/2022/07/06/apache-ozone-netapp-eseries.html">Apache Ozone S3</a></li>
    </ul>
  </li>
</ul>

<p>Inferencing may not need much more than just block devices, but if you do need other services then you can deploy them as necessary. For example, RAG may need a file server or object store.</p>

<p>Some other useful and relevant details in the context of AI services:</p>

<ul>
  <li>Storage performance and event monitoring: <a href="/2023/11/04/eseries-perf-analyzer-epa-330.html">E-Series Performance Analyzer</a>, <a href="https://scaleoutsean.github.io/2023/09/25/monitoring-netapp-eseries-with-prtg.html">PRTG</a>, <a href="https://scaleoutsean.github.io/2023/09/17/netapp-e-series-snmp-trap-notifications.html">SNMP</a> (event only)</li>
  <li>Access control
    <ul>
      <li>Linux NFS server - Yes</li>
      <li>BeeGFS (even in the free community edition 8) - Yes</li>
      <li>S3 gateway - Yes for all three mentioned above</li>
    </ul>
  </li>
  <li>Antivirus
    <ul>
      <li>ClamAV - works with standard Linux filesystems, can be containerized, and for BeeGFS it can work as a service that acts on BeeGFS filesystem notifications. Note that on-access scanning for S3 would have to be done on the filesystem level, and that any scenario (local filesystem, shared filesystem, NFS) on-access scanning probably isn’t feasible due to performance impact</li>
      <li>Other, commercial solution that work with Linux distribution you have. There may be none for S3, but for local filesystems they should work well and probably better than ClamAV</li>
    </ul>
  </li>
  <li>Snapshots
    <ul>
      <li>E-Series can take snapshots of any block devices, individually or in a <a href="/2023/10/05/snapshots-and-consistency-groups-with-netapp-e-series.html">consistency group</a>. As controls are exposed only to management network, these are considered indelible from server and storage network. See the <a href="/2022/04/03/restic-server-netapp-eseries.html">Rest Server post</a> for a recipe</li>
      <li>Hypervisors, CSI drivers or filesystems may offer their own (obviously, these can be deleted on the servers)</li>
    </ul>
  </li>
  <li>Backup
    <ul>
      <li>VMware-based - any VMware-compatible backup</li>
      <li>Linux-based - any Linux-compatible backup
        <ul>
          <li>You can create a separate (e.g. HDD-based) disk group for <a href="/2024/01/24/netapp-eseries-as-veeam-hardened-repository.html">Veeam Hardened Repository</a></li>
          <li>If you run a single server with S3, Veeam Hardened Repository or <a href="/2022/04/03/restic-server-netapp-eseries.html">Rest Server</a>, you get a physically separate domain for your backups</li>
        </ul>
      </li>
      <li>Kubernetes - Velero (works with generic CSI, which includes <a href="/2022/09/30/velero-backup-192.html#non-csi-with-restic-and-netapp-beegfs-csi">BeeGFS CSI driver</a>), probably Kasten K10, too</li>
    </ul>
  </li>
  <li>Data synchronization and replication
    <ul>
      <li>Whatever Linux utilities and tools exist out there, e.g. rsync, rclone, and more</li>
      <li>BeeGFS has its own highly optimized sync solution that can copy data to/from S3. Like rclone, but better when you use BeeGFS.</li>
      <li>These let you securely burst to the cloud or distribute code and data to/from edge locations</li>
    </ul>
  </li>
  <li>Automation
    <ul>
      <li>SANtricity API and CLI should be enough, as these are expected to be simple deployments (probably one big disk group/pool with 3-4 LUNs presented to all storage clients)</li>
      <li>Ansible modules for SANtricity storage and client-side automation</li>
    </ul>
  </li>
  <li>Storage area network (SAN) is <strong>optional</strong></li>
</ul>

<p>I want to elaborate on this last point: with FC, iSCSI or SAS, it’s possible to attach several servers to E-Series and completely avoid buying storage switches. Since OPEA is (at least initially) aiming for ROBO and departmental use, this is perfect:</p>
<ul>
  <li>Saves $10K-ish (pair of 25G switches)</li>
  <li>Saves 1 or 2 U in rack space. With one EF300 (2U) and 3 1U servers (3U), that represents savings of 1/6 or 2/7 (16% or 28%, respectively, depending on whether storage networking switches take 1 or 2 RUs)</li>
</ul>

<h2 id="multi-tenancy">Multi-tenancy</h2>

<p>From a compute perspective, we don’t get to decide - that’s up to the stack (and mind you, OPEA isn’t <em>just</em> Intel, so YMMV).</p>

<p>From a storage perspective, VM-based solutions give you better segregation than Docker or Kubernetes, but you still need to be make sure multi-tenancy works in your compute stack. On the storage side, services can be stuffed into containers or VMs and VMs put on separate compute (service) networks so you can get full segregation, but without encryption (perhaps the only exception is client-side encryption for S3, where available).</p>

<p>If a storage network is used, physically segregate hosts can be attached to E-Series, and segregation is done by selective presentation (“zoning”) of LUNs. E-Series iSCSI doesn’t support VLANs, so there’s no way to limit access to iSCSI clients by presenting storage on different VLAN-tagged LANs.</p>

<p>For multi-tenant inferencing or similar cases where block devices are sufficient and shared storage not required, local (LUKS) encryption of VMs may be sufficient, depending on use case (I’m sure this wouldn’t be enough for BFSI or defense use cases).</p>

<h2 id="conclusion">Conclusion</h2>

<p>Even NVIDIA GPU-focused stacks require a fair amount of work to deploy, integrate and support. Because Kubernetes is usually required for NVIDIA stack and storage systems like E-Series don’t support it, they stand out as hard(er) to integrate.</p>

<p>OPEA requires even more work, so the extra effort to add E-Series to the stack is not comparatively significant. Most of the items are standard Linux solutioning and deployment, and most are documented on this blog with enough API examples and source code that do 80% of the work always aiming to use the minimal number of dependencies which eases the cost and effort of integration and support while lowering security risks.</p>

<p>Because OPEA isn’t highly prescriptive, it’s almost inevitable that a lot be left as “an exercise for the user”. Due to more variance in the stack and configuration, I also think people need to be ready to deal with “rare bugs” (as in: 0 search results) and come up with own workarounds. Of course, those who can handle it also get the benefits.</p>

<p>E-Series is a good match for use cases from the OPEA project.</p>

<p>After skimming through the documentation, I would start looking at the following approaches (in no particular order):</p>

<ul>
  <li>Service providers: OPEA on Kubernetes (with 100/200G switches, can be physically segregate servers or at least one Kubernetes cluster per tenant)</li>
  <li>ROBO
    <ul>
      <li>Dockerized OPEA in VMs on vSphere</li>
      <li>Dockerized OPEA in VMs on LXD or Proxmox</li>
      <li>Kubernetes can work as well, of course, but why bother if it’s not bare metal and you already have VI to maintain?</li>
    </ul>
  </li>
  <li>Single user/org environment could use BeeGFS; shared (multi-tenant) could deploy NFS or S3 VM (or Docker/Kubernetes cluster) per each tenant and have one centralized S3 for models</li>
</ul>

      </article>
    </div>
    
      <div class="categories">
    <span><p>Categories:</p>
    
    
      <a href="
      /categories/#storage">storage</a>
      &nbsp; 
    
      <a href="
      /categories/#ai">ai</a>
       
    
  </span>
</div>
    

  
    <div>
      <h3>Related Posts</h3>
      <ul>
      
        <li><a href="/2025/05/23/beegfs-data-pipeline.html">Data pipelines with ThinParQ BeeGFS and NetApp E-Series</a></li>
      
        <li><a href="/2025/05/17/beegfs-v8-netapp-e-series-indexing-tiering-workflows.html">ThinkParQ BeeGFS v8 with NetApp E-Series</a></li>
      
        <li><a href="/2023/11/28/postgres-pgvector-instacluster-eseries.html">Postgres, pgvector, E-Series and Instaclustr</a></li>
      
        <li><a href="/2024/04/11/netapp-eseries-containerized-beegfs-nfs-s3-all-in-one.html">NetApp E-Series with containerized BeeGFS, NFS, S3</a></li>
      
        <li><a href="/2024/04/23/econfig-update.html">EConfig v2</a></li>
      
      </ul>
    </div>
  

    
  </div><footer class= "footer">
    <p>2025-06-07 17:43 </p>
    <p>Copyright © 2025 scaleoutSean. Content is released under the CC BY license. Design: Alessio Franceschi</p>
</footer>

</body>
</html>
