<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.27.3 by Michael Rose
  Copyright 2013-2025 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Ceph with NetApp E-Series | Acting Technologist</title>
<meta name="description" content="Ceph on E-Series in 60 seconds">


  <meta name="author" content="Sean">
  
  <meta property="article:author" content="Sean">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Acting Technologist">
<meta property="og:title" content="Ceph with NetApp E-Series">
<meta property="og:url" content="https://scaleoutsean.github.io/2025/12/28/ceph-with-netapp-eseries.html">


  <meta property="og:description" content="Ceph on E-Series in 60 seconds">



  <meta property="og:image" content="https://scaleoutsean.github.io/assets/images/kinetica-eseries-pod-with-ddp-01.png">



  <meta name="twitter:site" content="@scaleoutSean">
  <meta name="twitter:title" content="Ceph with NetApp E-Series">
  <meta name="twitter:description" content="Ceph on E-Series in 60 seconds">
  <meta name="twitter:url" content="https://scaleoutsean.github.io/2025/12/28/ceph-with-netapp-eseries.html">

  
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:image" content="https://scaleoutsean.github.io/assets/images/kinetica-eseries-pod-with-ddp-01.png">
  

  



  <meta property="article:published_time" content="2025-12-28T00:00:00+08:00">






<link rel="canonical" href="https://scaleoutsean.github.io/2025/12/28/ceph-with-netapp-eseries.html">







  <meta name="google-site-verification" content="F6q7vIwQ2G0j8tk-KL9rAXOMLXMDMMUkEz4fRs1Nnnc" />


  <meta name="msvalidate.01" content="7cf5b7d96a77410a8ad035f764dc81b3">





<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Acting Technologist Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  window.enable_copy_code_button = true;
</script>

<script src="https://cdn.counter.dev/script.js" data-id="83dc700a-b821-4e57-9425-02b8336a9456" data-utcoffset="0"></script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <link href="https://fonts.googleapis.com/css?family=PT+Sans+Narrow:400,700|PT+Serif:400,700,400italic" rel="stylesheet" type="text/css">


  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
          <a class="site-logo" href="/"><img src="/assets/images/scaleoutsean-acting-technologist.png" alt="Acting Technologist"></a>
        
        <a class="site-title" href="/">
          Acting Technologist
          <span class="site-subtitle">human action through technology</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/archive/"
                
                
              >Archive</a>
            </li><li class="masthead__menu-item">
              <a
                href="/categories/"
                
                
              >Categories</a>
            </li><li class="masthead__menu-item">
              <a
                href="/tags/"
                
                
              >Tags</a>
            </li><li class="masthead__menu-item">
              <a
                href="/projects.html"
                
                
              >Projects</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main">
  


  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Ceph with NetApp E-Series">
    <meta itemprop="description" content="How to use Ceph with NetApp E-Series SANtricity storage">
    <meta itemprop="datePublished" content="2025-12-28T00:00:00+08:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://scaleoutsean.github.io/2025/12/28/ceph-with-netapp-eseries.html" itemprop="url">Ceph with NetApp E-Series
</a>
          </h1>
          

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2025-12-28T00:00:00+08:00">2025-12-28 00:00</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          15 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#introduction">Introduction</a></li><li><a href="#tr-4549-ceph-on-e-series-reference-architecture-for-ceph-clusters-using-netapp-e-series">TR-4549: Ceph on E-Series Reference Architecture for Ceph Clusters Using NetApp E-Series</a></li><li><a href="#ceph-with-e-series-now">Ceph with E-Series now</a><ul><li><a href="#small-ceph-cluster-or-ceph-on-e-building-block-pattern">Small Ceph cluster or "Ceph-on-E building block" pattern</a></li><li><a href="#medium-and-large-ceph-on-e-cluster">Medium and large Ceph-on-E cluster</a></li></ul></li><li><a href="#architecture-and-solution-notes">Architecture and solution notes</a><ul><li><a href="#comparison-with-existing-solutions">Comparison with existing solutions</a></li><li><a href="#advantages-of-sds">Advantages of SDS</a></li><li><a href="#storage-layout">Storage layout</a></li></ul></li><li><a href="#distributions-and-packaging">Distributions and packaging</a></li><li><a href="#deploy-in-seconds">Deploy in seconds</a></li><li><a href="#ceph-e-and-kubernetes">Ceph, E and Kubernetes</a></li><li><a href="#conclusion">Conclusion</a></li></ul>
            </nav>
          </aside>
        
        <h2 id="introduction">Introduction</h2>

<p>Once upon a time, NetApp E-Series had this nicely documented in Technical Reports ("TR").</p>

<p>Almost a decade later those are less relevant, but still useful if you want to read the value proposition as it was at the time.</p>

<p>I don't know if <a href="https://community.netapp.com/fukiw75442/attachments/fukiw75442/e-series-santricity-and-related-plug-ins-discussions/507/1/TR-4549-0717%20Ceph%20on%20E-Series.pdf">this TR-4549</a> is the last released TR on this topic, but it is the only one I could find within minutes.</p>

<p>It's posted in an unusual place (the comunity site) so they haven't managed to remove it yet, I suppose.</p>

<p>Let's review this TR-4549 from today's perspective and take a look at "latest and greatest" situation.</p>

<h2 id="tr-4549-ceph-on-e-series-reference-architecture-for-ceph-clusters-using-netapp-e-series">TR-4549: Ceph on E-Series Reference Architecture for Ceph Clusters Using NetApp E-Series</h2>

<p>Now, I could be a moron and create an AI summary. Or I could read the TR and maybe learn something. I'll go with the latter.</p>

<p>Note that this TR was done at the time of RHEL 7.3, Ceph Jewel Release Version 10.2.2-38 and E-Series E5600.</p>

<blockquote>
  <p>Greatly improves stability and manageability while allowing customers to meet their cost targets.</p>
</blockquote>

<p>Verdict: still true.</p>

<p>This doesn't say any everyone should run Ceph backed by protected storage (let alone E-Series). It says many users dislike low-level storage management (JBODs, firmware quirks, OS maintenance, and so on).</p>

<p>And this comes up fairly often. A CTO may like the concept of SDS S3 such as MinIO (slightly less these days), but in the end there are just 2 guys who already manage many PBs of storage across block for VI and NOSQL and file or S3 for analytics and AI and are busy enough.</p>

<blockquote>
  <p>Only the replica protection option is currently supported when using RBD or CephFS</p>
</blockquote>

<p>Erasure Coding (<code class="language-plaintext highlighter-rouge">k+m</code>) has been introduced since then, which benefits Ceph-E-Series as well. Whereas in 2017 we'd make two copies (RF2) of all files, now we can make just 1.5 (EC 2+1) or even less.</p>

<p>Storage setup from the TR:</p>

<ul>
  <li>NL-SAS: 20 R5 (4+1) for data</li>
  <li>SSD (SAS): two R10 (2+2) for (mirrored) metadata</li>
</ul>

<p>Verdict: still fine.</p>

<ul>
  <li>Yes, not <em>all</em> Ceph-on-E clusters would be designed this way, but many (especially those used in analytics, media and so on) would</li>
  <li>Today we may use different media, and take advantage of EC if the workload justifies it, but the above is how we could do Ceph on EF600 today and it wouldn't be wrong</li>
</ul>

<p>Narrow RAID 5 makes smaller writes perform better. One LUN per R5 avoids contention and disk group-level interference among servers and targets.</p>

<p>SSDs for metadata is still the right default. BeeGFS on E-Series defaults to R10 volumes on flash storage as well. But note that this isn't <em>required</em>. For example, S3 storage for few hundred container images could use NL-SAS for all volumes.</p>

<blockquote>
  <p>For each OSD server, a 100GB volume is created. This volume is then divided into five Linux partitions to support five OSD volumes.</p>
</blockquote>

<p>Verdict: this is weird. Why not create 5 LUNs? It's not like you'd end up with hundreds of volumes or hit any "limit".</p>

<blockquote>
  <p>Once the volumes are created, they must be mapped by using the storage partitioning feature in SANtricity. A host is defined for each OSD server by using World Wide Identifiers of the SAS HBA ports.</p>
</blockquote>

<p>Verdict: this is fine.</p>

<p>Today we'd use NVMe over something (IB, RoCEv2), especially with faster E-Series arrays.</p>

<p>SAS is usualy DAS (it's been years since I've heard of anyone mentioning SAS storage switches). It avoids the need to buy storage switches when/if the number of ports (usually proportional to the number of servers) is enough. In the TR with four servers per E-Series array array ports were sufficient (eight dual-ported servers with SAS and two E5600 arrays).</p>

<blockquote>
  <p>Each OSD server has five 10.894TB RAID 5 volumes and one 100GB RAID 10 volume assigned to it. Two SAS ports are connected to each server.</p>
</blockquote>

<p>Verdict: this is fine.</p>

<p>It doesn't <em>always</em> have to be that way, and the good news is unlike with physical JBOD disks we can create multiple patterns within one or any clusters using the same shared, protected storage.</p>

<blockquote>
  <p>For multipathing support, a host type of ALUA was selected, and user friendly names disabled in the multipath.conf file.</p>
</blockquote>

<p>Verdict: fine.</p>

<p>ALUA is still the way to go and - according to the superficial E-Series multipathing documentation for SANtricity 11.90 - no changes are needed to "default" multipath.conf on the Linux distributions listed in the NetApp Interoperability Matrix ("IMT") which means RHEL, Rocky, SLES.</p>

<p>This doesn't mean you can't use Ubuntu or Debian - but to get it officially supported you need to ask NetApp to approve it. There's nothing that would "prevent" another distribution or version from working, but it may have quirks. I used <a href="/2023/09/22/ubuntu-lts-netapp-eseries-iser.html">Ubuntu 22.04 with iSER</a> - it's quirky, but works. My reasoning was: given how superficial the official documentation is, I have to figure it out by myself in any case, so I might as well figure it out using the distribution I prefer to use.</p>

<blockquote>
  <p>By using the RAID engine with E-Series, the default number of Ceph replicas can be decreased from three to two to increase capacity utilization without losing redundancy.</p>
</blockquote>

<p>Verdict: true.</p>

<p>That's how you recover the cost of using enterprise storage while at the same time getting fewer rebuilds and other benefits such as time and labor savings.</p>

<blockquote>
  <p>The CRUSH map's hierarchy enables users to control how data is replicated based on the attributes of a particular environment. For example, the CRUSH map typically contains types such as OSD, host, rack, row, and data center.</p>
</blockquote>

<p>Verdict: true.</p>

<p>This information about storage devices and failure domain hierarchy is known to placement groups (PGs) <a href="https://docs.ceph.com/en/tentacle/rados/operations/placement-groups/#use-of-placement-groups">now</a> which store and access data according to settings.</p>

<p>Note that E-Series has no single point of failure (SPOF), so we wouldn't replicate across two E-Series arrays simply for volume data availability purposes (we could make a replica on a single E-Series array, using one or more disk groups or DDP pools).</p>

<p>The reasons to use these PGs with E-Series mostly have to do with rack and network redundancy and sizing. A side benefit is that performance utilization is balanced and network proximity achieved for lower read latency. It's the same thing you can find in <a href="/2022/06/22/e-series-hdfs.html">Hadoop with E-Series</a> and NOSQL blog posts.</p>

<h2 id="ceph-with-e-series-now">Ceph with E-Series now</h2>

<p>It is the same pattern that we see elsewhere (Elasticsearch, Splunk, HDFS, Kinetica, etc. with E-Series):</p>

<ul>
  <li>SDS is becoming smarter (compression, erasure coding, decent interations)</li>
  <li>Applications that need to persist data are also becoming smarter (replicas, compression, backup-to-S3, stretch clustersâ€¦)</li>
</ul>

<p>And yet, many still buy appliances or protected storage because if you want to find someone competent to manage SDS, it could very well cost more than buying protected storage as long sa you don't save millions.</p>

<p>Some IT teams do away with SDS alltogether and just use dedicated storage appliances, others offload to SDS but still buy protected storage (strange but valid example: ONTAP Select on VMware vSAN).</p>

<p>Very few do everything with <em>just</em> SDS. "It works" is different from "it's works reliably and there are two admins who know the system inside out". Technical debt, poor security, questionable recovery from hardware failures and whatnot can catch up with you eventually.</p>

<h3 id="small-ceph-cluster-or-ceph-on-e-building-block-pattern">Small Ceph cluster or "Ceph-on-E building block" pattern</h3>

<p>This would be something like <a href="/2025/05/21/opean-ai-with-netapp-eseries.html#wheres-the-storage">OPEA</a> or similar ROBO or white box setups. This image is from a recent <a href="/2025/12/22/kinetica-with-netapp-eseries.html">post on Kinetica</a> (the vector database) and applies just the same here - hot (metadata) on R1, warm (data) on R6 LUNs.</p>

<p><img src="/assets/images/kinetica-eseries-pod-with-ddp-01.png" alt="Three node Ceph cluster with E-Series EF300 or EF600" /></p>

<p>I expect these would be common Ceph-on-E deployment patterns in terms of storage media and layout:</p>
<ul>
  <li>all Ceph storage (data and metadata) on TLC SSDs or hybrid (EF300, EF600)
    <ul>
      <li>hybrid media pattern: metadata on TLC in controller shelf, data in SAS expansion shelves (NL-SAS)</li>
      <li>all-flash pattern (everything in controller shelf, up to 24 disks); metadata on DDP-based R1 or dedicated flash (RAID 10 or R0)</li>
      <li>other niche configurations</li>
    </ul>
  </li>
  <li>all Ceph storage (data and metadata) on QLC SSDs (EF300C, EF600C)
    <ul>
      <li>DDP pool with metadata on R1, and data on R6 volumes</li>
    </ul>
  </li>
</ul>

<p>Whether there's a small cluster or several building blocks, it may be advantageous to use DAS (and avoid buying extra switch ports).</p>

<h3 id="medium-and-large-ceph-on-e-cluster">Medium and large Ceph-on-E cluster</h3>

<p>For clusters larger than half racks, you may need two E-Series systems anyway.</p>

<p>If you have two, you can achieve rack redundancy (power, ToR switches).</p>

<p>For this we use building block pattern.</p>

<h2 id="architecture-and-solution-notes">Architecture and solution notes</h2>

<h3 id="comparison-with-existing-solutions">Comparison with existing solutions</h3>

<p>NetApp has a <a href="/2022/08/28/configuring-netapp-e-series-solution-for-beegfs.html">BeeGFS-on-E</a> solution. How's this different?</p>

<p>Ceph isn't useful for HPC workloads. There may be some overlap in generic "analytics" workloads, the same way as some Big Data workloads we could run more or less the same way on Ceph as we do with HDFS, but others wouldn't work well at all.</p>

<p>Ceph is more feature-rich, has S3, NFS and is also suitable for low performance and low-cost use cases (virtualization, containerization).</p>

<p>Check out this diagram of a Bee-on-E cluster with two building blocks (each has a pair of servers directly attached to E-Series storage array):</p>

<p><img src="/assets/images/beegfs-layout-two-two.png" alt="BeeGFS with E-Series" /></p>

<p>Striping across arrays: check. Good performance for media straeming: check. But there's no replication in BeeGFS, so if these span racks and a rack has its power cut, you get downtime with this BeeGFS solution, and no downtime with Ceph with RF2 or EC, or <a href="/2022/07/06/apache-ozone-netapp-eseries.html">Ozone</a>, or HDFS with RF2. But if your servers can get more done <em>with</em> BeeGFS (rack failures notwithstanding) than with Ceph and 100% uptime, BeeGFS may still be better. As they say, "it depends."</p>

<h3 id="advantages-of-sds">Advantages of SDS</h3>

<p>Like BeeGFS, ONTAP Select and StoragGRID, Ceph can run in VMs. That means that you can deploy it <em>alongside</em> whatever other storage you run on E-Series.</p>

<p>For example:</p>
<ul>
  <li>You have VMware with VFMs on EF300, want to migrate to some other platform. Start with Ceph on VMs (on VMFS) and nested virtualization on vSphere, once you feel confident with it, add more storage and move workloads to the new VI platform with physical servers and Ceph.</li>
  <li>You have E-Series and want low-cost object storage in containers or VMs. You can get StorageGRID SDS for VMs, but if that doesn't work for you, you can deploy Ceph in VMs, containers or bare metal servers.</li>
  <li>You may have just 20 TB of spare capacity in a R6 disk group. You can still create 8 x 2.5 TB LUNs and deploy a small Ceph S3 cluster on this thing. It will be slow and you should probably setup replication to Glacier for DR purposes, but it will work and you will probably avoid extra cost associatd with buying or renting this capacity elsewhere.</li>
</ul>

<h3 id="storage-layout">Storage layout</h3>

<p>The TR doesn't mention RAID 0. E-Series supports RAID 0. I ran MinIO with EC on it, and if you want to rely just on SDS to protect data, you can.</p>

<p>As mentioned above, you can go with EC, 2 replica or a mix (per pool).</p>

<p>PGs are definitively advisable for larger clusters with two or more E-Series arrays.</p>

<p>There are many ways to layout storage for Ceph depending on requirements</p>
<ul>
  <li>Medium, large clusters: distinct data and metadata media, narrow (small IO) R5 or R6 groups or wide (large IO) R6 groups or R6 on DDP pools</li>
  <li>Smaller clusters, especially on all-flash storage: DDP with the R1 and R6 LUN pattern mentioned earlier</li>
  <li>Various adjustments can be made for HA, resilience or other concerns. You can achieve rack redundancy across two racks with two EF300 and just a handful of single processor 1U servers</li>
</ul>

<h2 id="distributions-and-packaging">Distributions and packaging</h2>

<ul>
  <li>Default - <a href="https://docs.ceph.com/en/latest/install/">it's available</a>. Different distributions package their own.</li>
  <li><a href="https://github.com/rook/rook">Rook</a> - "for Kubernetes". Rook is an CNCF incubation project.</li>
  <li><a href="https://www.redhat.com/en/technologies/storage/ceph">Red Hat Ceph</a> - enterprise-targeting Ceph distribution for Kubernetes (OpenShift) and more.</li>
  <li><a href="https://documentation.ubuntu.com/microcloud/latest/microceph/">MicroCeph</a> - Ubuntu's take, "Ceph for normal people" or "use Ceph without knowing anything about Ceph" kind of thing. Used by growing VI/CT projects such as Proxmox, LXD, Incus and others.</li>
</ul>

<h2 id="deploy-in-seconds">Deploy in seconds</h2>

<p>Small Ceph clusters can be deployed in seconds. Larger take longer, but it's just about adding more building blocks with some extra Placement Group considerations.</p>

<p>Workflow for Ceph-on-E is normally installed with <a href="https://docs.ceph.com/projects/ceph-ansible/en/latest/">Ansible</a>. You know the drill: enter variables, run a playbook. There's a UI and upgrades are also done with Ansible. It's the same thing we do with <a href="https://github.com/netapp/beegfs">BeeGFS</a> except there's no Web UI for BeeGFS yet.</p>

<p>Note that Ceph-Ansible won't configure E-Series for you:</p>
<ul>
  <li>If you deploy medium or large clusters, use the BeeGFS-on-E playbook to prepare storage and hosts
    <ul>
      <li>Small clusters, unless you redeploy them or re-create them, aren't worth automating with complex tools. Use a CLI or Web UI to configure storage array.</li>
    </ul>
  </li>
  <li>Then use <code class="language-plaintext highlighter-rouge">ceph-ansible</code> to deploy Ceph</li>
</ul>

<p>MicroCeph takes a more interesting approach - it doesn't put Ceph in the first plan. You'd probably use it with stacks like Proxmox without caring much about it (apart from deciding which internal disks to assign for Ceph). Proxmox doesn't use <code class="language-plaintext highlighter-rouge">ceph-ansible</code> to deploy, so to use Ceph on E-Series with MicroCeph, simply:</p>
<ul>
  <li>Create disk groups (RAID or DDP) and volumes (LUNs)</li>
  <li>Create host groups for cluster nodes required to access storage</li>
  <li>Present LUNs to hosts</li>
  <li>Deploy Proxmox (or other stack) with MicroCeph as usual</li>
</ul>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>snap <span class="nb">install </span>microceph
<span class="nv">$ </span><span class="nb">sudo </span>snap refresh <span class="nt">--hold</span> microceph
</code></pre></div></div>

<p>On one of the nodes, bootstrap singleton cluster and then add other nodes</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>microceph cluster bootstrap
<span class="nv">$ </span><span class="nb">sudo </span>microceph cluster add node-2<span class="p">;</span> <span class="nb">sudo </span>microceph cluster add node-3 <span class="c"># add nodes 2 and 3</span>
</code></pre></div></div>

<p>The other nodes will also install and hold <code class="language-plaintext highlighter-rouge">microceph</code>, but not bootstrap. Instead, they'll join with the tokens from previous command.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>microceph cluster <span class="nb">join</span> <span class="k">${</span><span class="nv">TOKEN</span><span class="k">}</span>
</code></pre></div></div>

<p>We don't have storage yet. Create a SANtricity pool or volume group. Since this is a small cluster, we'll have just one.</p>

<p>We can use a Python client (better) or the official SMcli or Swagger to find the pool ID which we need for volume placement.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>santricity pools list
</code></pre></div></div>

<p>We need this pool ID, <code class="language-plaintext highlighter-rouge">04000000600A098000E3C1B000002CED62CF874D</code>, to tell SANtricity where to create volumes. We need three volumes for our cluster with EC 2+1. We'll use R6-like volumes. We'd execute three commands like this ("full command").</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>santricity volumes create  <span class="nt">--base-url</span> https://10.1.1.1:8443/devmgr/v2  <span class="se">\</span>
  <span class="nt">--username</span> admin <span class="nt">--password</span> s3cr3t <span class="se">\</span>
  <span class="nt">--no-verify</span> <span class="nt">--pool-id</span> 04000000600A098000E3C1B000002CED62CF874D <span class="se">\</span>
  <span class="nt">--name</span> s1_ceph01 <span class="nt">--size</span> 1 <span class="nt">--size-unit</span> tb <span class="nt">--tag</span> <span class="nv">workload</span><span class="o">=</span>ceph <span class="se">\</span>
  <span class="nt">--raid-level</span> raid6
</code></pre></div></div>

<p>This returns a volume JSON object that contains volume ID, <code class="language-plaintext highlighter-rouge">02000000600A098000E3C1B0000035B6694E542F</code>.</p>

<p>Now we need to present each of these to a Ceph host. We may have a "group" where we present to a group ID, or individual hosts. We get both host and host groups with this command (whereas in the API and Ansible it's two different methods, one for hosts, another for host groups).</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>santricity hosts membership 
</code></pre></div></div>

<p>The first host is <code class="language-plaintext highlighter-rouge">84000000600A098000E3C1B000302F0C650C2668</code> and our volume ID from above is <code class="language-plaintext highlighter-rouge">02000000600A098000E3C1B0000035B6694E542F</code>.</p>

<p>Map the first volume to the first host, and do likewise for the other two host-volume pairs.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>santricity mappings create <span class="se">\</span>
  <span class="nt">--volume-ref</span> 02000000600A098000E3C1B0000035B6694E542F <span class="se">\</span>
  <span class="nt">--host-ref</span> 84000000600A098000E3C1B000302F0C650C2668
</code></pre></div></div>

<p>Now rescan storage (in case of iSCSI or FC) and login to new target on each of the nodes, confirm multipathing (if in place) is usable and only then move on to adding volumes from Ceph. MicroCeph docs say:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>microceph disk add /dev/vdb
</code></pre></div></div>

<p>What we really want to do before that step above is to make sure multipathing works. SANtricity devices should be added by their permanent path such as <code class="language-plaintext highlighter-rouge">/dev/disk/by-id/scsi-3600a098000e3c1b0000035b6694e542f</code>. As I've often said I prefer to not partition LUNs at all but if you do then you could also use <code class="language-plaintext highlighter-rouge">/dev/disk/by-uuid</code> paths to <code class="language-plaintext highlighter-rouge">kpartx</code>-created partitions.</p>

<p>What about InfiniBand? What about NVMe/RoCE devices? Well, multi-pathing may be done slightly differently. Both E-Series and MicroCeph dumb down this critical phase of storage configuration.</p>

<p>Note that the MicroCeph documentation does not mention anything about erasure coding. Pool configuration provides just one related option, which is <code class="language-plaintext highlighter-rouge">set-rf</code> (set RF) and with MicroCeph distribution with E-Series <code class="language-plaintext highlighter-rouge">set-rf 2</code> would be in line with common practices for protected storage. Use a full-featured distribution if you require EC.</p>

<p>Enable S3 gateway:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>microceph <span class="nb">enable </span>rgw
</code></pre></div></div>

<p>Is that "in seconds"? It's not if you run each command manually, but if you script them or use Proxmox, you could do deploy MicroCeph in under a minute because stacks like Proxmox abstract MicroCeph and all we need to do is loop over three hosts to create and map volumes and ensure they're visible to hosts.</p>

<p>The fact is, the <code class="language-plaintext highlighter-rouge">santricity</code> CLI I use above doesn't even exist "out there". But that's a Python CLI for essential (volumes, mappings) storage operations with E-Series that I've created this week for use cases like this (more about this in another post) - all you want is to create and present volumes without the hassle and overheads of Ansible.</p>

<p><img src="/assets/images/santricity-cli-03-mappings.png" alt="SANtricity Python 3 CLI - volume mappings command" /></p>

<p>So, it does exist and will be released. There's also Ansible (again, take advantage of storage-side configuration BeeGFS-on-E playbooks perform). More is coming in this area and I'll revisit this topic in 2026.</p>

<h2 id="ceph-e-and-kubernetes">Ceph, E and Kubernetes</h2>

<p>As I mentioned in <a href="/2025/12/22/reautomating-eseries.html">Reautomating E-Series</a>, E-Series has no official CSI driver. If you're the kind of guy who prefers to use SDS with protected storage (which is how StorageGRID works as well, by the way), Ceph gives you another choice - not just for S3, but for block and file in VM and Kubernetes environments.</p>

<p>For non-clustered CSI you can use <a href="/2022/12/09/directpv-topolvm-csi-lvm-das-k8s-with-eseries.html">TopoLVM</a>, ZFS (with or without replication), static volumes and more. I recommend recommended these for "few and heavy" workloads such as NOSQL and NuSQL databases and ZFS is good for some DevOps use cases.</p>

<p>For clustered CSI, there's BeeGFS CSI (HPC- and AI-focused) an there's <a href="https://github.com/ceph/ceph-csi">Ceph CSI driver</a> for generic Kubernetes workloads and VI.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Ceph with E-Series still provides value.</p>

<p>Not much has changed since TR-4549. Notable exceptions:</p>
<ul>
  <li>EC is now available and we can take advantage of it - on the whole I'd say EC benefits E-Series more than it benefits Ceph.</li>
  <li>Ceph has improved and doesn't require a scientist to deploy and maintain it (althought it may, to fix it), especially on protected storage</li>
  <li>S3 on Ceph wasn't evaluated, but that has become more visible since then, and may become even more after MinIO's <a href="/2025/06/06/whats-minio-up-to.html">big rug pull of 2025</a></li>
  <li>Ceph CSI is the best choice for general Kubernetes workloads on E-Series and also for emerging virtualization stacks (Proxmox, LXD, HPE Morpheus, Incus)</li>
</ul>

<p>The cost of experimentation with SDS is low and, since free distributions of Ceph are readily available and can be deployed in seconds, you can give Ceph a try without buying anything at first. If it works, keep it. The same as with ZFS and other storage software I write about in the context of E-Series.</p>

<p>What E-Series gives you is a rock-solid and versatile platform for any SDS while eliminating low-level, low-value added work of fiddling with disk and PERC firmware and patching (CVEs). Derisking includes the ability to switch to another SDS without writing off your storage investment.</p>

<p>For smaller sites and/or basic three-node clusters E-Series has enough performance to drive multiple SDS options at the same time. <a href="/2024/02/28/incus-zfs-netapp-eseries.html">Use ZFS for some</a> and MicroCeph for other workloads? Why not! One reliable, high performance platform with low mainteance for SDS, NOSQL, Big Data, AI.</p>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/ceph" class="page__taxonomy-item p-category" rel="tag">ceph</a><span class="sep">, </span>
    
      <a href="/tags/netapp" class="page__taxonomy-item p-category" rel="tag">netapp</a><span class="sep">, </span>
    
      <a href="/tags/santricity" class="page__taxonomy-item p-category" rel="tag">santricity</a><span class="sep">, </span>
    
      <a href="/tags/storage" class="page__taxonomy-item p-category" rel="tag">storage</a><span class="sep">, </span>
    
      <a href="/tags/tr-4549" class="page__taxonomy-item p-category" rel="tag">TR-4549</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/eseries" class="page__taxonomy-item p-category" rel="tag">eseries</a><span class="sep">, </span>
    
      <a href="/categories/storage" class="page__taxonomy-item p-category" rel="tag">storage</a>
    
    </span>
  </p>


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time class="dt-published" datetime="2025-12-28T00:00:00+08:00">2025-12-28 00:00</time></p>

      </footer>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://x.com/intent/tweet?via=scaleoutSean&text=Ceph+with+NetApp+E-Series%20https%3A%2F%2Fscaleoutsean.github.io%2F2025%2F12%2F28%2Fceph-with-netapp-eseries.html" class="btn btn--x" aria-label="Share on X" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on X">
    <i class="fab fa-fw fa-x-twitter" aria-hidden="true"></i><span> X</span>
  </a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://scaleoutsean.github.io/2025/12/28/ceph-with-netapp-eseries.html" class="btn btn--linkedin" aria-label="Share on LinkedIn" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn">
    <i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span>
  </a>

</section>


      
  <nav class="pagination">
    
      <a href="/2025/12/24/trident-enable-concurrency-solidfire.html" class="pagination--pager" title="Trident concurrency with SolidFire">Previous</a>
    
    
      <a href="/2025/12/29/santricity-client.html" class="pagination--pager" title="Python client library for E-Series SANtricity">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
    
<div class="page__related">
  
  <h2 class="page__related-title">You may also enjoy</h2>
  <div class="grid__wrapper">
    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/2026/01/11/windows-hyper-v-netapp-eseries-santricity.html" rel="permalink">Windows Server 2025 and Hyper-V with NetApp E-Series
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2026-01-11T00:00:00+08:00">2026-01-11 00:00</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          7 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Notes on Windows Server 2025 and E-Series SANtricity arrays in iSCSI and NVMe/RoCEv2 environments
</p>
  </article>
</div>

    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/2026/01/10/minio-aistor-eseries-patterns.html" rel="permalink">E-Series and MinIO AIStor Enterprise Lite patterns
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2026-01-10T00:00:00+08:00">2026-01-10 00:00</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          8 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Why single node clusters are now an Enterprise approach and earlier E-Series patterns stil relevant
</p>
  </article>
</div>

    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/2026/01/10/eseries-santricity-odx-vaai.html" rel="permalink">Windows ODX with E-Series SANtricity
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2026-01-10T00:00:00+08:00">2026-01-10 00:00</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          2 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Windows Offloaded Data Transfer (ODX) with NetApp E-Series SANtricity
</p>
  </article>
</div>

    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/2026/01/06/eseries-santricity-powershell.html" rel="permalink">santricity-powershell for Day 1+ E-Series automation
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2026-01-06T00:00:00+08:00">2026-01-06 00:00</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          2 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Replace SMcli for most Day 1+ operations
</p>
  </article>
</div>

    
  </div>
</div>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        

<div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
          <li><a href="https://twitter.com/scaleoutsean" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/scaleoutsean" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>


<div class="page__footer-copyright">&copy; 2020-2025 scaleoutSean.github.io | <a href="https://scaleoutsean.github.io">Acting Technologist</a> | Terms: <a href="https://creativecommons.org/licenses/by/4.0/deed.en" target="_blank">CC BY 4.0</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>






  </body>
</html>
