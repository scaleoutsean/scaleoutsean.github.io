<!doctype html>
<html lang="en">
  <head>
        <meta charset="UTF-8">
    <meta http-equiv="content-language" content="en">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="googlebot" content="noarchive">
    <meta name="googlebot" content="max-snippet:60">
    <meta name="googlebot" content="max-image-preview:small">
    <meta name="bingbot" content="noarchive">
    <meta name="bingbot" content="max-snippet:60">
    <meta name="bingbot" content="max-image-preview:small">
    <meta name="robots" content="noarchive">
    <meta name="robots" content="max-snippet:60">
    <meta name="google-site-verification" content="F6q7vIwQ2G0j8tk-KL9rAXOMLXMDMMUkEz4fRs1Nnnc" />
    <title>
        
            Simple file share and S3 analytics and MCP ops | Acting Technologist
      
    </title>
    <meta name="description" content="
     If you're not Google, maybe there's a simpler way
     ">

    <!-- LINK TO ATOM FEED FOR SEO -->
    <link rel="alternate" type="application/atom+xml" href="https://scaleoutsean.github.io/feed.xml" />

    <!-- FAVICON -->
    <link rel="icon" href="/assets/favicon.ico" type="image/x-icon">
    <link rel="shortcut icon" href="/assets/favicon.ico" type="image/x-icon">

    <!-- CSS -->
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="stylesheet" href="/assets/css/syntax.css">

    <!-- Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css">

    <link href="/_pagefind/pagefind-ui.css" rel="stylesheet">
    <script src="/_pagefind/pagefind-ui.js" type="text/javascript"></script>
    <div id="search"></div>
    <script src="https://cdn.counter.dev/script.js" data-id="83dc700a-b821-4e57-9425-02b8336a9456" data-utcoffset="0"></script>
    <script>
        window.addEventListener('DOMContentLoaded', (event) => {
            new PagefindUI({ element: "#search" });
        });
    </script>

    <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Simple file share and S3 analytics and MCP ops | Acting Technologist</title>
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="Simple file share and S3 analytics and MCP ops" />
<meta name="author" content="scaleoutSean" />
<meta property="og:locale" content="en" />
<meta name="description" content="If you’re not Google, maybe there’s a simpler way" />
<meta property="og:description" content="If you’re not Google, maybe there’s a simpler way" />
<link rel="canonical" href="https://scaleoutsean.github.io/2025/06/05/simple-filesystem-and-s3-analytics-and-mcp.html" />
<meta property="og:url" content="https://scaleoutsean.github.io/2025/06/05/simple-filesystem-and-s3-analytics-and-mcp.html" />
<meta property="og:site_name" content="Acting Technologist" />
<meta property="og:image" content="https://scaleoutsean.github.io/assets/images/filesystem-analytics-ai-mcp-01-smb-share.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-06-05T00:00:00+08:00" />
<script type="application/ld+json">
{"image":"https://scaleoutsean.github.io/assets/images/filesystem-analytics-ai-mcp-01-smb-share.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://scaleoutsean.github.io/2025/06/05/simple-filesystem-and-s3-analytics-and-mcp.html"},"author":{"@type":"Person","name":"scaleoutSean"},"description":"If you’re not Google, maybe there’s a simpler way","@type":"BlogPosting","url":"https://scaleoutsean.github.io/2025/06/05/simple-filesystem-and-s3-analytics-and-mcp.html","headline":"Simple file share and S3 analytics and MCP ops","dateModified":"2025-06-05T00:00:00+08:00","datePublished":"2025-06-05T00:00:00+08:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  </head>

<body>

  <div class="container">
    <header id="header">
	<div id = site_title>
		<a href="https://scaleoutsean.github.io/">
			<h2 style="font-size:1.8em;">Acting Technologist</h2>
		</a>
		
		<h3>
			civilizations are created by individuals
		</h3>
		
	</div>

	<div id="subheader">
		
		<nav class="pages">
<a href="/about.html">About</a>

<a href="/archive.html">Archive</a>

<a href="/categories/">Categories</a>

<a href="/projects.html">Projects</a>
</nav>
		
		
		<nav class="social">
			
  
    <a href="https://www.github.com/scaleoutsean" target="_blank" id="github"><i class="fab fa-github" aria-hidden="true"></i></a>
  
   

  
    <a href="https://twitter.com/scaleoutsean" target="_blank" id="twitter"><i class="fab fa-twitter" aria-hidden="true"></i></a>
  
   

  
  
    <a href="/feed.xml" target="_blank" id="rss"><i class="fas fa-rss" aria-hidden="true"></i></a>
   

		</nav>
		
	</div>
</header>


    <div class="post-container">
      <article id = "post">
        <h1 id = "post-title">Simple file share and S3 analytics and MCP ops</h1>

  
  <div class = "post-info">
    <span>
        <i class="far fa-calendar" aria-hidden="true"></i> <span>05 Jun 2025</span> - <i class="far fa-clock"></i> 


  
  
    17 minute read
  

    </span>
  </div>
  
        <ul>
  <li><a href="#introduction">Introduction</a></li>
  <li><a href="#smb-and-nfs-data-migration">SMB and NFS data migration</a>
    <ul>
      <li><a href="#shell-scripts-and-utilities">Shell scripts and utilities</a></li>
      <li><a href="#xcp">XCP</a></li>
      <li><a href="#elaborate-schemes">Elaborate schemes</a></li>
      <li><a href="#enterprise-software">Enterprise software</a></li>
      <li><a href="#example-smb-share">Example: SMB share</a></li>
    </ul>
  </li>
  <li><a href="#s3-reporting-and-replication">S3 reporting and replication</a>
    <ul>
      <li><a href="#example-s3-usage-reporting">Example: S3 usage reporting</a></li>
    </ul>
  </li>
  <li><a href="#combined-file-and-object-shares">Combined file and object shares</a></li>
  <li><a href="#conclusion">Conclusion</a></li>
  <li><a href="#demo">Demo</a></li>
</ul>

<h2 id="introduction">Introduction</h2>

<p>Sometimes storage admins have challenges related to data movement, such as moving stuff from A to B, or deleting old stuff from A.</p>

<p>Normally, that’s easy - find the sucker, hit Delete. But sometimes the problem is you don’t know where the sucker is (or the suckers are). And there may be millions of them.</p>

<p>Another frequently found problem is file system reporting, which isn’t novel either and needs to be solved before you can solve the two problems mentioned earlier.</p>

<p>Things to highlight before we begin:</p>

<ul>
  <li>Solutions have existed for decades. I haven’t invented anything new.</li>
  <li>One has to know what he’s doing. Sometimes the right thing to do is to buy, other times it may be to build your own.</li>
</ul>

<p>As that famous blog post said (“you’re not Google”), storage administrators often imagine their problems require solutions used by largest enterprises (or even Google).</p>

<p>Recently one of the new and upcoming storage vendors has been telling everyone they’re Google, so to speak, but that’s not true either.</p>

<h2 id="smb-and-nfs-data-migration">SMB and NFS data migration</h2>

<p>An SMB share with 100 million files takes a long time to rescan. The files need to be scanned for two purposes: sometimes it’s to delete old junk. Other times it’s to move specific files to an archive (and even this case may be complicated, because copying a fat directory tree may take a while).</p>

<h3 id="shell-scripts-and-utilities">Shell scripts and utilities</h3>

<p>They work up to a point, but if data is growing rapidly it becomes harder and harder to keep up.</p>

<p>After a while it makes sense to buy a commercial - if not “enterprise” utility and move on.</p>

<h3 id="xcp">XCP</h3>

<p>For the latter case NetApp has its own tool called XCP. I’ve blogged about it before. It can move/copy, but it can’t delete. However, you can scan and store in a PostgreSQL DB and query the database to obtain a list of files you need to delete.</p>

<p>Great, there’s a solution! But, as an SMB user, now you need a Linux VM for PostgreSQL, another for XCP server, and maybe two RHEL licenses and a Linux admin. You multiply annual cost of that times 5 years… and maybe you keep looking.</p>

<p>It’s a great solution (free, fast), but it may be more attractive for one-off migrations. For daily use, there are additional considerations.</p>

<h3 id="elaborate-schemes">Elaborate schemes</h3>

<p>Maybe you think you’ll have better luck with several tools, including shell scripts, databases and such.</p>

<p>That’s commendable if it’s justified. Back in 2023 <a href="/2023/08/01/fscrawler-filesystem-analytics-elasticsearch.html">I looked at one such approach</a> with mixed results. Admittedly, it was a more complicated scenario - it involved indexing and search - but it just wasn’t something that most commercial users could accept.</p>

<h3 id="enterprise-software">Enterprise software</h3>

<p>At some point - whether due to the scale, (legal and other) responsibility or other reasons, one has to tap out and get Datadobi, StorageX, Starfish Software and similar.</p>

<p>As soon as it looks justified, I immediately recommend these. If you work with patient or financial data at scale, it’s often better to buy because getting sued could cost a lot more.</p>

<h3 id="example-smb-share">Example: SMB share</h3>

<p>In recent days I’ve been dealing with a cataloging aka “file-system enumeration” problem. Based on previous experience and gut feeling I figured the problem was solvable with a better mouse trap (“Elaborate Schemes Lite”).</p>

<p>It took me some time to set things up and then I got to work.</p>

<p>I don’t have a lot of capacity and file-system enumeration is really a metadata-intensive workload, so I started with 500,000 files.</p>

<p>Then, to check the scalability of this new mouse trap, I increased that to slightly over 2,000,000 files:</p>

<pre><code class="language-raw">4 top level dirs x 4 mid x 8 bottom x 16,000 files per each bottom directory
TOTAL: 2,048,000 files in 128 directories
</code></pre>

<p><img src="/assets/images/filesystem-analytics-ai-mcp-01-smb-share.png" alt="SMB share with 2 million files" /></p>

<p>I tried half a dozen approaches with both 0.5 and 2 million files and I was surprised to see that PowerShell 7 on Ubuntu 24.04 was faster than PowerShell 7 on Windows Server 2025.</p>

<p>Since I had the same SMB share mounted on both systems at the same time, it was very easy to compare. Windows was 2-4x slower.</p>

<p>But - I’ve mentioned this before - I’ve experienced the mysteries and wonders of .NET on Linux and when it comes to sensitive data operations like archiving of SMB data, I’d rather use Windows.</p>

<p>After evaluating half a dozen approaches, I settled on this one:</p>

<ul>
  <li>Enumerate several top levels of directories (e.g. top 2, in my case 4 x 4 x 8 = 128)</li>
  <li>Create a number of parallel jobs to scan these directories. If you have 8 Windows VMs, that’s easy, if you have just one like I did, maybe create 8 jobs, each of which sequentially scans 16 directories</li>
  <li>Save data to CSV or flat file DB files (SQLite, for example, to avoid the PostgreSQL management thing I mentioned above). This would be over 100 flat database files</li>
  <li>Optionally create a database of databases (yep, it’s downhill from here) or merge all those into one big database</li>
</ul>

<p>Obviously, there are subtleties in this and what’s “best” depends on many factors, but with using the above approach I was able to get the following:</p>

<ul>
  <li>80 seconds to enumerate all directories and files and create database files</li>
  <li>20 seconds to run a query to find files that match certain criteria (file name, path, size, etc)</li>
  <li>PowerShell memory utilization rarely goes over 500 MB (some of my bad attempts complete exhausted VM’s RAM, so this was a big win)</li>
</ul>

<p><img src="/assets/images/filesystem-analytics-ai-mcp-02-smb-scan.gif" alt="SMB scan of 2 million files" /></p>

<p>As scanning completes and indexes are built, built-in check shows 16,001 (16,000 + 1 new CSV file) in one of bottom-level directories.</p>

<p><img src="/assets/images/filesystem-analytics-ai-mcp-03-smb-scan-verification.png" alt="SMB index verification" /></p>

<p>Two million wasn’t more than 4 times slower than 0.5 million and memory utilization remained almost the same due to simply more jobs of essentially the same size (number of files per folder went from 2,000 to 16,000, for example, but it wasn’t reflected in PowerShell memory consumption).</p>

<p>Finally, the script searches for files older than a certain cut-off point, reading all 100+ CSV “database” files. This takes just 16 seconds and 89,935 files (out of 2.048 million) matched the condition.</p>

<p><img src="/assets/images/filesystem-analytics-ai-mcp-04-file-index-search.png" alt="SMB file index search" /></p>

<p>I can’t “guarantee” it’d scale the same way to 100 million, but it seems that way.</p>

<p>Later I did another test, with 4 million files (<code class="language-plaintext highlighter-rouge">8 x 8 x 4 x 16,000</code>) to get an idea.</p>

<p><img src="/assets/images/filesystem-analytics-ai-mcp-12-smb-count-4million-files.png" alt="SMB with 4 million files" /></p>

<p>Enumeration/cataloging performed the same. Even PowerShell didn’t consume more resources. With more (sub)directories to scan, there were more jobs to do, but the script batches those anyway so it’s effectively zero impact.</p>

<p><img src="/assets/images/filesystem-analytics-ai-mcp-10-smb-scan-verification-4million-files.png" alt="SMB file run with 4 million files" /></p>

<p>Search was roughly 2x slower compared to 2 million files. No slowdown from 0.5 to 2 to 4 million, and this is a worst case scenario (many CSV files).</p>

<p><img src="/assets/images/filesystem-analytics-ai-mcp-11-smb-scan-search-4million-files.png" alt="SMB file search with 4 million files" /></p>

<p>If one needs daily or weekly actions (archiving, deletion, migration, etc.), based on the above tests, shares with 100 or 500 million files could probably be scanned daily and certainly weekly.</p>

<p>Also, I’ve been positively surprised by the “flat file DB” approach. The ability to query CSV files has fully met my expectations:</p>

<ul>
  <li>No “DB server” (or indeed, DB <em>service</em>) to setup, backup, or maintain</li>
  <li>No need to backup as they’re so easy to recreate, but since we can keep those flat file databases on SMB shares, they get backed up without any issues</li>
  <li>Excellent performance</li>
  <li>Easy to work with. As I’ve mentioned above, we can merge or import them into one big DB (great up to a few hundred GBs, I suppose), or build a DB of DBs, which was the approach I took: I’d just query the DB of DBs for CSV DB locations and then query CSV files. Even if 100 CSV “databases” are searched, it’s quick enough for tasks such as file deletion or migration.</li>
</ul>

<p>I think this has been a successful example of a correct guesstimate that a slightly more elaborate DIY approach might work well.</p>

<h2 id="s3-reporting-and-replication">S3 reporting and replication</h2>

<p>S3 isn’t that different in terms of addressing challenges, but I want to mention two additional things:</p>

<ul>
  <li>Komprise is one of the enterprise vendors that have been doing well in this space</li>
  <li>Most people know about rclone as a “famous” free utility for listing/cataloging, moving and copying of object data.</li>
</ul>

<h3 id="example-s3-usage-reporting">Example: S3 usage reporting</h3>

<p>This was a recent ask by a customer and I’ve seen colleagues asking the same.</p>

<p>In my case it was fairly simple:</p>

<ul>
  <li>Want to see who’s doing what in terms of capacity and object count utilization</li>
  <li>Want to determine differences between buckets (some may be used for testing or S3-to-S3 “backup”)</li>
  <li>Hundred million objects per bucket, 3-4 buckets</li>
</ul>

<p>This also seemed perfectly doable so I didn’t even bother to check if there’s a product “feature” (on ONTAP S3 or StorageGRID) that can do it.</p>

<ul>
  <li>I can list around 1 million object (names and other system metadata) per minute. 100 million is doable for daily, and especially weekly, use</li>
  <li>Once I get hold of that, nothing can stop me from doing the rest</li>
</ul>

<p>And what would that “rest” be?</p>

<ul>
  <li>Bucket listing is stored to a file with a bunch of one-line JSON “documents” (one per object)</li>
  <li>We import this to a flat file database</li>
  <li>Then we query data for object count, object size, project size, and we could even run extra queries to check tags and versions (although this may be too much for daily use)</li>
  <li>With that information, we can share/present it:
    <ul>
      <li>Send usage alerts (high capacity, high object count, etc.) to a log analysis or TSDB system</li>
      <li>Expose database(s) via Jupyter or other Web UI, API, CLI, etc.</li>
      <li>Build MCP server to share data through AI chat bots</li>
    </ul>
  </li>
</ul>

<p>For performance monitoring, I would use InfluxDB (my SFC and EPA both use it), but what was available was Loki. Loki is primarily for logs, we really want to send events, and not so much performance metrics.</p>

<p>In this example I look at different “projects” (which may be just specific S3 prefixes such as <code class="language-plaintext highlighter-rouge">top_level_1/</code>) and send object count and object size by “project”, with some KV pairs including event severity, service name and such.</p>

<p><img src="/assets/images/filesystem-analytics-ai-mcp-05-s3-service-performance-event-monitoring.png" alt="S3 service monitoring for projects" /></p>

<p>If I spot more than 0.5 million objects in a project, that may be an error event that should be checked. Or we can just log everything as info-level severity and let others decide what to do with it.</p>

<p><img src="/assets/images/filesystem-analytics-ai-mcp-06-s3-service-performance-event-monitoring.png" alt="S3 service monitoring for projects" /></p>

<p>For more “free form” data querying, we could simply put those CSV (or other) database files on S3 and let people query them with <a href="/2022/03/04/storagegrid-s3-select.html">S3 Select</a>. ONTAP S3 can’t provide S3 SELECT, but StorageGRID can. Also, there are good flat file databases that can be queried directly from S3 without downloading them.</p>

<p>We simply query a URL to CSV “database” and it works like a charm! For several million files or objects it doesn’t even have to be an optimized file format.</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">SELECT</span> <span class="k">COUNT</span><span class="p">(</span><span class="o">*</span><span class="p">)</span> <span class="k">AS</span> <span class="n">total_objects</span><span class="p">,</span> 
  <span class="k">SUM</span><span class="p">(</span><span class="k">size</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span><span class="p">)</span> <span class="k">AS</span> <span class="n">total_size_mb</span> 
  <span class="k">FROM</span> <span class="n">objects</span> <span class="k">WHERE</span> <span class="k">key</span> <span class="k">LIKE</span> <span class="s1">'top_level_1/%'</span><span class="nv">"
# total_objects = 480039
# total_size_mb = 1940.2514181137085

</span></code></pre></div></div>

<p>One question I got from a colleague was “why not use (object storage) system metrics for S3”?</p>

<p>Because they’re meant for storage admins, not for operations or LoB. Of course, we can get both, if that helps.</p>

<p>Here the top row is storage system metrics - “per bucket” object capacity and object count. But what about “per top-level prefix” stats? You can’t get those from a storage system. Or maybe your “projects” are under some prefix two levels deep (e.g. <code class="language-plaintext highlighter-rouge">s3/bucket/projects/hr-proj-1/</code>).</p>

<p>Top row (S3 system metrics) tells you object count in the bucket is increasing, but you have no clue why. That’s where the 2nd row comes in: S3 API-level metrics where we can do whatever we need and get insights daily or weekly (for fairly large buckets).</p>

<p><img src="/assets/images/filesystem-analytics-ai-mcp-13-smb-count-4million-files.png" alt="S3 service metrics vs. S3 bucket metrics" /></p>

<p>Without “in-bucket” analytics, you’re blind and have to do “1 bucket per project” to make storage system S3 stats work for you. Sometimes that’s OK (each user has many TB of data), sometimes not OK (500 smaller projects would require 500 buckets).</p>

<p><strong>Implementation note:</strong> here my bucket metrics are sent to InfluxDB 3 rather than Loki, which lets me easily create nice dashboards. InfluxDB 3 also supports SQL, so I could run similar SQL queries as above.</p>

<p>And finally, it’s time for the obligatory AI crap!</p>

<p>I’m not sure if that’s a thing - I wouldn’t use it since I already know how to query data and would prefer to just do it from shell scripts - but we have to assume maybe there are users who don’t deal with this stuff often and may want an easy way to find their way through all the various information. Enter MCP.</p>

<p>Since I have a database of all objects, I just need a server and although VS Code + Copilot seem a bit behind, they’re popular and Copilot somehow manages to work with MCP. Conservatively, I might add - as you can see here, it refuses to do things, instead it prefers to pass the bucket by giving you the exact command.</p>

<p>I ask what tools are available, and it answers promptly (I have three - one for list of projects I watch as S3 administrator, one for individual project metrics, and one for “top projects” to see who’s eating my storage).</p>

<p><img src="/assets/images/filesystem-analytics-ai-mcp-07-s3-service-mcp-server.png" alt="MCP server for S3 service in VSCode - chat" /></p>

<p>For specific tool commands that need to be issued to the server, Copilot spits it out and then I copy those and paste them in VS Code terminal to query the MCP server.</p>

<p><img src="/assets/images/filesystem-analytics-ai-mcp-09-s3-service-mcp-server-copy-paste.png" alt="MCP server for S3 service in VSCode - Q&amp;A with RPC" /></p>

<p>As you can see above, although JSON-RPC style advice is annoying, it minimizes the likelihood of catastrophic errors and mistakes.</p>

<p>Notice how Copilot allows me to ask questions in “conversational language” - as long as it’s understandable, it provides the right RPC command. (I told it once to add <code class="language-plaintext highlighter-rouge">| jq</code> to its command examples, and it consistently did that after that ask.)</p>

<p>And if you get confused by long JSON output (sometimes it can be multiple lines long), you can copy that to Copilot and tell it to process it for you (filter, format, etc.).</p>

<p><img src="/assets/images/filesystem-analytics-ai-mcp-08-s3-service-mcp-server.png" alt="MCP server for S3 service in VSCode - pretty table" /></p>

<p>One can see how, once this “preview” phase of “Copilot with MCP” is over, Copilot can successfully and seamlessly bridge AI chat with MCP services and AI agents available to it.</p>

<p>Other MCP-capable chat bots already do this much better, of course, but this is from VSCode and for many IT folks it’s deployed, licensed and subscribed today.</p>

<p>I wasn’t completely sure how to tell it to use my MCP server (which I called “my-s3”), as some instructions I found on the Web didn’t work, so I did things like:</p>

<p><code class="language-plaintext highlighter-rouge">/mcp my-s3 give me top projects by object count</code></p>

<p>That “/mcp” prefix wasn’t really necessary, it seems. “my-s3”, the name of my MCP server, was enough for Copilot to tell me to do copy-paste JSON-RPC command like this:</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="w"> </span><span class="p">{</span><span class="nl">"jsonrpc"</span><span class="p">:</span><span class="w"> </span><span class="s2">"2.0"</span><span class="p">,</span><span class="w"> </span><span class="nl">"method"</span><span class="p">:</span><span class="w"> </span><span class="s2">"tools/call"</span><span class="p">,</span><span class="w"> 
  </span><span class="nl">"params"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"list_top_projects"</span><span class="p">,</span><span class="w"> 
    </span><span class="nl">"arguments"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="nl">"metric"</span><span class="p">:</span><span class="w"> </span><span class="s2">"count"</span><span class="p">}</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>Of course, I wrote that API function and wouldn’t normally ask this question myself, but a chat bot and this MCP server can help <em>anyone</em> answer these questions.</p>

<p>Related to replication for DR and testing purposes, the same bucket listing and databases we create from it can be used to create replication jobs and move or copy data to other buckets or other sites.</p>

<p>Similarly to the SMB approach above, once we work out a query we work out a way to make it parallel, multi-threaded or whatever it takes, and mid-sized buckets in low-risk environments can be handled with these DIY approaches.</p>

<h2 id="combined-file-and-object-shares">Combined file and object shares</h2>

<p>During above experimentation I realized that one of the customers could expose their SMB shares via multiple protocols (File/Object).</p>

<p>The advantage of that - mentioned earlier - is that any migration, scanning or archiving can be performed via S3, making it possible to use generic, robust S3 clients and greatly simplify the problem. For example, a 300-lines long shell script that deals with file shares can be replaced by a 30 line script that deals with objects without losing, or perhaps while gaining robustness and performance.</p>

<h2 id="conclusion">Conclusion</h2>

<p>These recent challenges have been interesting and reminded me that “you’re not Google” post from more than a decade ago.</p>

<p>Sometimes we just need to try a bit harder and a DIY approach can work. Other times it’s smarter to give up ASAP and pay for proper tools and services. And it takes experience (and some luck) to know when to do what.</p>

<p>I was more skeptical about the ability to catalog SMB shares with hundreds of millions of files (maybe because of FSCrawler), but even that turned out to be likely possible even with small resources.</p>

<p>I was more optimistic about S3 - because “scanning” happens inside of S3 server, so it should work much faster - and indeed, listing 1 million objects per minute doesn’t require any programming, it simply works with common S3 clients (not the crappy ones, perhaps).</p>

<p>Last month I started playing with MCP servers (<a href="/2025/05/20/get-started-with-netapp-solidfire-mcp-server.html">this POC with SolidFire</a>), but I didn’t have a reason to do something practical with it. The S3 reporting/analysis requirement was very much appreciated as I had a reason to use MCP again.</p>

<p>Neither StorageGRID nor ONTAP S3 have internal tools for bucket data analysis - it’s kind of “out of scope”. Overall bucket capacity and object count may be <a href="/2025/02/17/minimal-prometheus-exporter-with-ontap-harvest.html">available</a> (I haven’t even looked), but if you want to get information by tag or by path, you probably have to build something anyway (in the case of StorageGRID probably with <a href="/2023/07/20/storagegrid-and-elaticsearches.html">Elasticsearch</a> which is similar to FSCrawler in NAS environments, which I mentioned at the top).</p>

<p>Using simple object list commands - when it’s feasible, as it was in this case - plus some modern scripting tools works very well with both StorageGRID and ONTAP S3 and means I can use it with either without any modifications.</p>

<p>Lastly, for very high file count or high data volume environments, BeeGFS with E-Series has very good scanning and reporting features built-in. I blogged about them <a href="/2025/05/17/beegfs-v8-netapp-e-series-indexing-tiering-workflows.html">here</a> and <a href="/2025/05/23/beegfs-data-pipeline.html">here</a>.</p>

<h2 id="demo">Demo</h2>

<p>This demo video (6m32s) has 2.5 parts:</p>

<ul>
  <li>Part 1 - shows how “live” queries can be repeatedly executed and send to some event log (such as Loki)
    <ul>
      <li>It also shows how Loki works with (tiers cold data to) object storage. (This is off topic here, but this trend <a href="/2023/11/06/netapp-eseries-sizing-for-splunk-smartstore.html">benefits “fast &amp; simple” storage arrays like NetApp E-Series</a>. It’s the same trend that you see with Splunk, InfluxDB 3 and other modern architectures).</li>
    </ul>
  </li>
  <li>Part 2 - shows that, for buckets with many objects, it’s probably better to list objects on a fixed schedule (daily or weekly), store results and summaries in a database which can then be exposed through several front-ends, whether it’s Jupyter, MCP, CLI or whatever works for users</li>
</ul>

<p>Link: <a href="https://rumble.com/v6uiorh-getting-extra-information-about-s3-buckets-on-netapp-object-storage.html">https://rumble.com/v6uiorh-getting-extra-information-about-s3-buckets-on-netapp-object-storage.html</a></p>

      </article>
    </div>
    
      <div class="categories">
    <span><p>Categories:</p>
    
    
      <a href="
      /categories/#storage">storage</a>
      &nbsp; 
    
      <a href="
      /categories/#ai">ai</a>
      &nbsp; 
    
      <a href="
      /categories/#analytics">analytics</a>
       
    
  </span>
</div>
    

  
    <div>
      <h3>Related Posts</h3>
      <ul>
      
        <li><a href="/2025/05/20/get-started-with-netapp-solidfire-mcp-server.html">Build NetApp SolidFire MCP server</a></li>
      
        <li><a href="/2025/05/17/beegfs-v8-netapp-e-series-indexing-tiering-workflows.html">ThinkParQ BeeGFS v8 with NetApp E-Series</a></li>
      
        <li><a href="/2023/08/01/fscrawler-filesystem-analytics-elasticsearch.html">FSCrawler for basic filesystem analytics in Elasticsearch</a></li>
      
        <li><a href="/2025/05/23/beegfs-data-pipeline.html">Data pipelines with ThinParQ BeeGFS and NetApp E-Series</a></li>
      
        <li><a href="/2023/11/30/elasticsearch-ilm-netapp-eseries.html">Snapshots and ILM with Elasticsearch 8</a></li>
      
      </ul>
    </div>
  

    
  </div><footer class= "footer">
    <p>2025-07-23 23:20 </p>
    <p>Copyright © 2025 scaleoutSean. Content is released under the CC BY license. Design: Alessio Franceschi</p>
</footer>

</body>
</html>
