<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.27.3 by Michael Rose
  Copyright 2013-2025 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Improved pipelines with BeeGFS FS Event Notifications in v8 | Acting Technologist</title>
<meta name="description" content="More on improvements in BeeGFS file-system event notifications">


  <meta name="author" content="scaleoutSean">
  
  <meta property="article:author" content="scaleoutSean">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Acting Technologist">
<meta property="og:title" content="Improved pipelines with BeeGFS FS Event Notifications in v8">
<meta property="og:url" content="https://scaleoutsean.github.io/2025/06/15/pipeline-with-beegfs-file-system-notifications-v2.html">


  <meta property="og:description" content="More on improvements in BeeGFS file-system event notifications">





  <meta name="twitter:site" content="@scaleoutSean">
  <meta name="twitter:title" content="Improved pipelines with BeeGFS FS Event Notifications in v8">
  <meta name="twitter:description" content="More on improvements in BeeGFS file-system event notifications">
  <meta name="twitter:url" content="https://scaleoutsean.github.io/2025/06/15/pipeline-with-beegfs-file-system-notifications-v2.html">

  
    <meta name="twitter:card" content="summary">
    
  

  



  <meta property="article:published_time" content="2025-06-15T00:00:00+08:00">






<link rel="canonical" href="https://scaleoutsean.github.io/2025/06/15/pipeline-with-beegfs-file-system-notifications-v2.html">







  <meta name="google-site-verification" content="F6q7vIwQ2G0j8tk-KL9rAXOMLXMDMMUkEz4fRs1Nnnc" />


  <meta name="msvalidate.01" content="7cf5b7d96a77410a8ad035f764dc81b3">





<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Acting Technologist Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  window.enable_copy_code_button = true;
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <link href="https://fonts.googleapis.com/css?family=PT+Sans+Narrow:400,700|PT+Serif:400,700,400italic" rel="stylesheet" type="text/css">


  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
          <a class="site-logo" href="/"><img src="/assets/images/scaleoutsean-acting-technologist.png" alt="Acting Technologist"></a>
        
        <a class="site-title" href="/">
          Acting Technologist
          <span class="site-subtitle">human action through technology</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/archive/"
                
                
              >Archive</a>
            </li><li class="masthead__menu-item">
              <a
                href="/categories/"
                
                
              >Categories</a>
            </li><li class="masthead__menu-item">
              <a
                href="/tags/"
                
                
              >Tags</a>
            </li><li class="masthead__menu-item">
              <a
                href="/projects.html"
                
                
              >Projects</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main">
  


  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Improved pipelines with BeeGFS FS Event Notifications in v8">
    <meta itemprop="description" content="  Introduction  What's new in BeeGFS file-system events in BeeGFS v8          Dude, where's my gRPC server?      Can BeeGFS watch server, erhm, client, drop messages      File-system notification messages        Use case: data pipeline for StorageGRID          Advantages      Limitations      Scheduled scanning        Performance (and security)  Bonus lightweight approach with Web hooks  Bonus use case: data pipeline for the Versity S3 Gateway  Conclusion and ideas for future work with BeeGFS FS event notifications  Appendix A: batching and filtering  Appendix B: scanning  Appendix C: likely bottlenecks">
    <meta itemprop="datePublished" content="2025-06-15T00:00:00+08:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://scaleoutsean.github.io/2025/06/15/pipeline-with-beegfs-file-system-notifications-v2.html" itemprop="url">Improved pipelines with BeeGFS FS Event Notifications in v8
</a>
          </h1>
          

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2025-06-15T00:00:00+08:00">2025-06-15 00:00</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          18 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
        <ul>
  <li><a href="#introduction">Introduction</a></li>
  <li><a href="#whats-new-in-beegfs-file-system-events-in-beegfs-v8">What's new in BeeGFS file-system events in BeeGFS v8</a>
    <ul>
      <li><a href="#dude-wheres-my-grpc-server">Dude, where's my gRPC server?</a></li>
      <li><a href="#can-beegfs-watch-server-erhm-client-drop-messages">Can BeeGFS watch server, erhm, client, drop messages</a></li>
      <li><a href="#file-system-notification-messages">File-system notification messages</a></li>
    </ul>
  </li>
  <li><a href="#use-case-data-pipeline-for-storagegrid">Use case: data pipeline for StorageGRID</a>
    <ul>
      <li><a href="#advantages">Advantages</a></li>
      <li><a href="#limitations">Limitations</a></li>
      <li><a href="#scheduled-scanning">Scheduled scanning</a></li>
    </ul>
  </li>
  <li><a href="#performance-and-security">Performance (and security)</a></li>
  <li><a href="#bonus-lightweight-approach-with-web-hooks">Bonus lightweight approach with Web hooks</a></li>
  <li><a href="#bonus-use-case-data-pipeline-for-the-versity-s3-gateway">Bonus use case: data pipeline for the Versity S3 Gateway</a></li>
  <li><a href="#conclusion-and-ideas-for-future-work-with-beegfs-fs-event-notifications">Conclusion and ideas for future work with BeeGFS FS event notifications</a></li>
  <li><a href="#appendix-a-batching-and-filtering">Appendix A: batching and filtering</a></li>
  <li><a href="#appendix-b-scanning">Appendix B: scanning</a></li>
  <li><a href="#appendix-c-likely-bottlenecks">Appendix C: likely bottlenecks</a></li>
</ul>

<h2 id="introduction">Introduction</h2>

<p>I actually blogged about this right here: <a href="/2025/05/17/beegfs-v8-netapp-e-series-indexing-tiering-workflows.html#file-system-events">File-system events</a>, so I'll skip 90% of what I wrote in that post and just build on that here.</p>

<p>Secondly, some years ago I wrote about implementing <a href="/2024/01/29/antivirus-scanning-for-on-premises-s3.html">anti-virus scanning for StorageGRID</a>. I'll revisit that as a use case - maybe a 'stretch use case', but fun - for using BeeGFS file-system event notifications.</p>

<h2 id="whats-new-in-beegfs-file-system-events-in-beegfs-v8">What's new in BeeGFS file-system events in BeeGFS v8</h2>

<p>If you're interested in some background, see the related section of that recent post above.</p>

<p>To keep it simple, here's how I'd explain it (without reading the FS events code from v7, which puts me at a disadvantage):</p>

<ul>
  <li>Old notifications weren't easily accessible (see the article above)</li>
  <li>New (v8) notifications use gRPC client to send them, and can easily and reliably dispatch thousands of notifications per second to multiple remote servers</li>
  <li>Implementing the server is "an exercise for the reader", as they say</li>
</ul>

<h3 id="dude-wheres-my-grpc-server">Dude, where's my gRPC server?</h3>

<p>You have to write it. The reason(s) is/are explained in <code class="language-plaintext highlighter-rouge">beegfs-go</code>, along with other useful information. Based on <a href="https://github.com/ThinkParQ/beegfs-go/commit/78a7f84a88d4473b702be31cc4a4d76b7c1b6eff">current</a> README file, it appears ThinkParQ didn't want to get too prescriptive too early and that is smart because they can avoid wasted development. For example, I have suggested to create a way for the client to drop some events at source and optionally spray them evenly across several destinations and maybe they're getting similar suggestions from other users.</p>

<p>For now users are expected to roll their own gRPC server (only gRPC notifications can be sent), but as use cases and requirements get surfaced to them, they will likely improve it and may make other protocols available.</p>

<h3 id="can-beegfs-watch-server-erhm-client-drop-messages">Can BeeGFS watch server, erhm, client, drop messages</h3>

<p>Currently - also from that read-me - it stores (and buffers) messages in memory, so it won't lose sent-and-acknowledged messages, but it could lose not-yet-sent ones. As the read-me explains, the reason is they thought the best way is to build this into metadata service rather than to watch service, which is a to-do item.</p>

<h3 id="file-system-notification-messages">File-system notification messages</h3>

<p>The read-me was confusing to me, as it took me a while to realize what's what. Frankly speaking, I don't like that Watch service (so, a server) is a client (a gRPC client), while gRPC server is a "subscriber" (which sounds very client-ish to me). This wasted me some time when setting this up. (Not that I'm a gRPC expert, but it <em>added</em> to my usual stumbling.)</p>

<p>Anyhow, the linked repository has gRPC protocol definition files, so not unexpectly messages look exactly like the "proto" files intend them to look.</p>

<pre><code class="language-raw">seq_id: 1341
meta_id: 1
v2 {
  type: OPEN_READ
  num_links: 1
  path: "/main.log"
  entry_id: "0-682A0F1C-1"
  parent_entry_id: "root"
  msg_user_id: 1000
  timestamp: 1749980308698647132
}
</code></pre>

<p>Here I simply ran <code class="language-plaintext highlighter-rouge">cat /mnt/beegfs/main.log</code> and Watcher service sent the above via gRPC.</p>

<p>On the server ("subscriber") you have to ack these to make them go away i.e. to have Watch server (that is, the gRPC client) know the message was well received.</p>

<p>Then, with it, you can do whatever you want. Prefix your mount point (in my case, <code class="language-plaintext highlighter-rouge">/mnt/beegfs/</code>) to those names if you need full path to file(s).</p>

<p>I ranted about that (the over-hyping of what is essentially several steps you can create by yourself in days using BeeGFS as seen here) in the BeeGFS-related post at the top, and also the subsequent one <a href="/2025/05/23/beegfs-data-pipeline.html">here</a> (related to the annoying "reference stacks" and "pipelines" that every vendor likes to boast about as if no enterprise user can run <code class="language-plaintext highlighter-rouge">'docker compose'</code> without having a "vendor-certified" copy-paste example.).</p>

<h2 id="use-case-data-pipeline-for-storagegrid">Use case: data pipeline for StorageGRID</h2>

<p>A pipeline of the special kind - AV and anti-malware scanning - that is.</p>

<p>This was an example from the StorageGRID virus and malware scanning post linked at the top.</p>

<p><img src="/assets/images/s3-av-scan-example.png" alt="StorageGRID Kafka notifications for AV scanner architecture" /></p>

<p>There's nothing wrong with it, it's as good or as bad as any other.</p>

<p>But, since I already ranted about data pipelines and "reference architectures" <strong>and</strong> I've always wanted to revisit S3 bucket AV scanning (if you skim through that older post, I did a "PoC" but it was a simple Python script), I thought to use that example as a use case in this post.</p>

<p>Of course, it should also not be a fake example - it should be superior in some aspects, including the reason that BeeGFS may be slightly more complicated to run than single-host file-systems (although I found <a href="/2025/05/17/beegfs-v8-netapp-e-series-indexing-tiering-workflows.html#beegfs-copy-tool">BeeGFS on-demand file-systems</a> very easy to setup and tear down). Here's an example:</p>

<p><img src="/assets/images/storagegrid-s3-scanner-beegfs.png" alt="S3 AV and malware scanning on BeeGFS" /></p>

<p>S3 service sends notifications or uses Web hooks to let us know of changes. That can go to Kafka, although I'd use something less nightmarish but compatible (StorageGRID supports Kafka and Elasticsearch endpoints, remember?).</p>

<p>There's a Kafka consumer to subscribe/poll these topics named after StorageGRID buckets that those notifications originate from.</p>

<p>The "new" element - compared to the earlier approach - is that because we have a <em>very fast</em> cluster file-system and S3 happens to contain large objects, this is a great match: we can achieve excellent performance and scalability by downloading objects to BeeGFS (can be BeeOND, i.e. temporary BeeGFS cluster/filesystem created on-demand).</p>

<p>I don't have a concrete proof for the superiority of this approach except second-hand knowledge of gRPC and a bit of first hand experience (I didn't <em>do</em> anything with BeeGFS Watcher events, I just acknowledged them, but in any case those are just external overheads and even that ack-ing messages is enough to see several messages can be exchanged within the same millisecond.)</p>

<p><img src="/assets/images/grpc_python_resp.png" alt="gRPC receive and send with BeeGFS Watch and gRPC server in Python" /></p>

<p>To be sure, time saved with gRPC shouldn't play a big role in overall scanning performance, but there's one Watch server per BeeGFS file-system, so I want the fastest one to make sure notifications themselves aren't bottlenecked at the FS level. I can scale downloading and scanning by adding more containers, but I can't scale Watch Service (which is why it's great that in version 8 it's gRPC based - at least they do the best they can).</p>

<p>After receiving a bucket notification event and making sure it's of interest (i.e. it's an S3 PUT rather than GET or DELETE) in Step 3 we dispatch S3 GET (download) jobs to all or a group of "download containers" which download objects to (say) <code class="language-plaintext highlighter-rouge">/mnt/beegfs/av/BUCKET_NAME/</code>.</p>

<p>BeeGFS file-system notifications are useful because we can learn of new objects from Watch service quickly and economically (Step 4). It is important to know the file has landed onto shared file-system.</p>

<p>Once we receive these file-system notifications, we know that downloaded files are visible to all containers (in the "AV scanning" group, if we run a dedicated job queue for that) so we can fire AV scan jobs immediately. Our gRPC server could even run on BeeGFS metadata node; normally that's not a best practice, but gRPC client is very lightweight and on BeeOND systems this would be perfectly fine.</p>

<p>There's another optimization we could do in this case: with a BeeOND-per-bucket, we could probably create large singleton BeeOND clusters with 8-16 containers and use gRPC over UNIX sockets to eliminate TCP in Step 5.</p>

<p>AV scan results are later sent to another Kafka topic (e.g. BUCKETNAME-scan-results) or elsewhere (Loki, etc.).</p>

<p>If we're worried some jobs may fail silently, we could log successful completions using a gRPC sequence number so that we can easily query all submissions for which no successful jobs referencing the same sequence ID can be found. Alternatively, there are "event databases" for such workflows, and we wouldn't use Kafka at all to avoid the need to "manually" deal with failed events.</p>

<p>To be honest, I wouldn't expect anyone to run a DIY AV service that's mission critical - if it's very important, it's better to use a commercial solution (and get latest and greatest virus definitions and detection engines).</p>

<p>We already mentioned that the main disadvantage is now we have a small cluster to run, but that isn't hard with BeeOND. Let's take a look at the main advantages (as I see them).</p>

<h3 id="advantages">Advantages</h3>

<p>It's easier to scale AV scanning because pods on all BeeGFS nodes can see any object regardless of where it was downloaded (if we wanted to do that).</p>

<p>Normally we'd achieve this by using a file share, but this should run much faster even for GB-sized objects. Or maybe you'd do a download and scan on the same client, but we may need to wait a few seconds longer for an object to download and it would be harder to schedule jobs: scheduling just 2 per container could turn out to be wrong because both files could be very large.</p>

<p>Notice how here, thanks to file-system event notifications, we don't have to wait for a "download pod" to notify us. The moment we see <code class="language-plaintext highlighter-rouge">LAST_WRITER_CLOSED</code> (event) in BeeGFS file-system path of interest, we can scan from any available "scanning pod". Also, based on <code class="language-plaintext highlighter-rouge">PATH</code> value, which includes <code class="language-plaintext highlighter-rouge">BUCKET_NAME</code>, we can dispatch events to the appropriate "scanner group" if we use ACLs to segregate scanning groups by bucket name.</p>

<p>I also think that having separate "download" and "scanning" pods is better for performance because AV scanners aren't burdened by IO and memory utilization spikes from S3 GET jobs.</p>

<h3 id="limitations">Limitations</h3>

<p>It's more of a choice than a limitation: we don't do anything with infected files. We had the same "limitation" without BeeGFS.</p>

<p>As I mentioned in the first antivirus scanning post, I can't say I like the idea of being able to write to buckets and overwrite (and possibly delete) objects that are already there. Furthermore, versioning or even Object Lock may be enabled.</p>

<p>Those who seek to scan for viruses and malware <em>before</em> files get uploaded to S3 can do that in a Web front-end (which could also use BeeGFS) or client application.</p>

<p>I've also seen some approaches that update S3 object metadata to mark it "clean" but that, too, seems too complicated. As if it's not bad enough to have a place that can download all your objects, now we want to be able to update its tags?</p>

<p>I think it's appropriate enough, and also much easier to do, to just scan and notify. If you need a mission-critical AV scanning service, I would definitively not suggest a DIY approach with ClamAV.</p>

<h3 id="scheduled-scanning">Scheduled scanning</h3>

<p>Just a note on that approach: it doesn't require a different architecture: we simply hoard FS notifications in a DB and start working on them at 1am, for example.</p>

<p>Because many TBs of objects may have been uploaded during the day, we need to remember to delete downloaded objects after scanning them. Or we could loop over a list of buckets and create a new BeeGFS for each, rather than have one big file-system landing area for everything that was uploaded that day.</p>

<h2 id="performance-and-security">Performance (and security)</h2>

<p>Another interesting detail may be performance. gRPC ought to be faster than other approaches, so this approach should scale better than some others.</p>

<p>I took this screenshot from the gRPC project, and it shows that even Python lets us process 1,000 messages per second.</p>

<p><img src="/assets/images/grpc_python.png" alt="gRPC server in Python" /></p>

<p>I think this is Python-to-Python, whereas BeeGFS isn't Python, so it may be faster.</p>

<p>You don't <em>have to</em> implement the server in Python, but we can.</p>

<p>If queue-to-Kubernetes and gRPC-to-Kubernetes are private network and/or file names aren't secret, TLS can be dropped, helping some more. We may be able to dispatch a few hundred messages per second, which is enough for "average" mid-size StorageGRID cluster.</p>

<p>For users who desire to use TLS simply because BeeGFS nodes might download "important objects" (although that's not really related to TLS; downloads from StorageGRID would use TLS - no need to compromise there): you can take advantage of dedicated smaller BeeOND clusters (e.g. 3 VMs with 6 GB each for one particular bucket). For less militant segregation, BeeGFS has ACLs.</p>

<p>If you want to run AV scanning as batch jobs, BeeOND clusters could be created at start, and wiped after scanning is done, one by one bucket. (You can see in the BeeGFS post at the top, it takes 1 minute to get a BeeOND filesystem going).</p>

<p>Lastly, one interesting thing is that with ClamAV using containers is an interesting way to at least partially segregate scanners from host OS and it may be better for performance as well. I <em>haven't tested that on BeeGFS</em>, but sending files from scanners to ClamAV server goes over TCP network, whereas - when every file is visible to all containers on all BeeGFS nodes - here I can use UNIX sockets and completely avoid TCP. No need for TCP and (even more) HTTPS!</p>

<h2 id="bonus-lightweight-approach-with-web-hooks">Bonus lightweight approach with Web hooks</h2>

<p>If you have a well-maintained Kafka cluster and your S3 storage supports Kafka notifications, there's no reason to not use it.</p>

<p>If you don't have it, your you don't quite like it, you can use Elasticsearch (at the very top, my preferred approach at the time).</p>

<p>If there's was a way for S3 to notify using Web hooks, you could use something like this:</p>

<ul>
  <li>Web-hook-to-some other (such as NATS) messaging service</li>
  <li>From there, proceed as usual (without the JVM hogs (Kafka, Elasticsearch))</li>
</ul>

<p><img src="/assets/images/storagegrid-s3-webhook-av-scanner.png" alt="StorageGRID Web hook notifications for AV scanner architecture" /></p>

<p>In other words, don't build a Kafka or Elasticsearch cluster just for AV scanning. Those services aren't for the faint of heart.</p>

<p>Here's how I un-hogged my "S3 event notification-to-AV-scanner" workflow using Web hooks (which have the same format that StorageGRID can send to Kafka today):</p>

<ul>
  <li>Tab 1 - NATS server v2.11</li>
  <li>Tab 2 - Web hook-to-NAS service (DIY Python service)</li>
  <li>Tab 3 - Web hook generator (DIY script - fakes proper Kafka-style SNS notifications)</li>
  <li>Tab 4 - NATS message service consumer (and AV scan job dispatcher, also DIY. AV scan jobs could be sent to some "famous" job scheduler. Maybe some out there can already talk to NATS.)</li>
</ul>

<p>(You may open this image in new tab for easier viewing.)</p>

<p><img src="/assets/images/webhooks-to-nats-to-av.gif" alt="S3 web hooks to NATS to AV scanner with Python" /></p>

<p>Now we don't have to deal with certain Kafkaesque services.</p>

<h2 id="bonus-use-case-data-pipeline-for-the-versity-s3-gateway">Bonus use case: data pipeline for the Versity S3 Gateway</h2>

<p>I came up with this only after I wrote this post (when I finished adding Appendix A) - I guess writing about MinIO's <a href="/2025/06/06/whats-minio-up-to.html">erratic behavior</a> got my S3-related brainstorming algorithms working overtime in recent days…</p>

<p>As objects land on the Versity S3 Gateway server, BeeGFS events kick off automatically. This is wild!</p>

<p>There's no step 2 here: this sucker goes straight to "<strong>Profit!</strong>", folks. This deserves its own post.</p>

<h2 id="conclusion-and-ideas-for-future-work-with-beegfs-fs-event-notifications">Conclusion and ideas for future work with BeeGFS FS event notifications</h2>

<p>New BeeGFS file-system notifications in version 8 are greatly improved compared to previous implementation in version 7.</p>

<p>We get more performance, more reliable delivery, and an easier data format (gRPC events vs. DIY parsing) to work with.</p>

<p>Combined with BeeOND, it opens possibilities for secure, high-performance processing steps that require a POSIX file system. Antivirus scanning is one such case, but there are others.</p>

<p>Using a DIY recipe with ClamAV is good enough for basic checking - it probably can stop Tier 2+ attackers. Mission-critical use in environments with compliance requirements should consider a commercial offering which could start after Step 1 (that is, you'd just have to send notifications to Kafka, and they'd take it from there).</p>

<p>When I find time I'd like to evaluate performance on low-end hardware (i.e. my home lab) to get a worst-case performance baseline for BeeGFS gRPC notifications. I'd also look at the details of Step 5 (gRPC -&gt; Kubernetes scan jobs) - Hashicorp Nomad is <a href="/2022/04/24/nomad-batch-job-scale-out-parallel-filesystem-beegfs-e-netapp-series.html">absolutely great for that</a>, but I don't have a favorite for Kubernetes yet.</p>

<h2 id="appendix-a-batching-and-filtering">Appendix A: batching and filtering</h2>

<p>Days after writing this post I revisited this and added batching and filtering.</p>

<p>How many events to batch? I guess "that depends" is the right answer. I used 8 while prototyping.</p>

<p><img src="/assets/images/grpc_python_batching.png" alt="gRPC event batching in Python" /></p>

<p>After batches of desired size are gathered, I process them like so:</p>

<ul>
  <li>Filtering (based on criteria such as file extension, event type, path, user ID, etc - along the lines demonstrated in the post with notifications from BeeGFS version 7)</li>
  <li>After filtering, events are sent to process files in intended way (such as AV scanning on LAST_WRITER_CLOSED). This I currently do in the same script, but like I said in the older demo, these events could be immediately sent to a message queue or job scheduler and let other services deal with them later.</li>
</ul>

<p>Another reason to "hoard" events or file lists is that scanning can be batched and/or delayed, for example. That can sometimes be useful.</p>

<p>For example, files/objects in certain buckets may be overwritten multiple times (so maybe there's no point of scanning them multiple times), or deleted quickly so there's no point of scanning them.</p>

<p>Knowing <code class="language-plaintext highlighter-rouge">path</code> to each file, we can develop filters and criteria that allow us to implement a variety of approaches in gRPC server or downstream by message queue subscribers.</p>

<p>While playing with batching I realized gRPC works much faster than I can write files to BeeGFS, so I probably won't be able to find any gRPC bottlenecks that way. Not unexpected.</p>

<p>I did a test with <code class="language-plaintext highlighter-rouge">OPEN_READ</code> (faster than writing) which:</p>

<ul>
  <li>Finds all files in a directory tree and loops through the list (8192 files)</li>
  <li>Reads and discards each file (1kB)</li>
  <li>Track gRPC server events</li>
</ul>

<p>It's fast.</p>

<table>
  <thead>
    <tr>
      <th>Step</th>
      <th>Timestamp (UTC)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Enumeration and <code class="language-plaintext highlighter-rouge">cat</code> start</td>
      <td>Sat Jun 21 16:59:38 2025</td>
    </tr>
    <tr>
      <td>Completed</td>
      <td>Sat Jun 21 16:59:51 2025</td>
    </tr>
    <tr>
      <td>Last event seen by gRPC</td>
      <td>2025-06-21T16:59:51Z</td>
    </tr>
    <tr>
      <td>Time between Last event and Completed</td>
      <td>0 seconds</td>
    </tr>
  </tbody>
</table>

<p>This didn't give me much and I found some of the files were larger, so I focused on just the smallest files (1024 of them). Elapsed time: 1.434 seconds.</p>

<pre><code class="language-raw">Bash script end time:   1750526463.384
gRPC server last event: 1750526463.384
</code></pre>

<p>Well, even milliseconds aren't granular enough. It's safe to say "there's practically no delay under moderate load" (few hundred events per second). As I guesstimated early on.</p>

<p>This doesn't include processing, but that doesn't necessarily need to happen in gRPC server. If I do additional experiments with processing (AV scanning, primarily), I'll add those notes in Appendix B.</p>

<h2 id="appendix-b-scanning">Appendix B: scanning</h2>

<p>This is the last step in our antivirus pipeline, where events are gathered in batches (3), then type <code class="language-plaintext highlighter-rouge">LAST_WRITER_CLOSED</code> (2) is filtered and scanned using Clam AV. Infected files (3) are detected, while clean files (not shown) are not.</p>

<p><img src="/assets/images/grpc_python_actions.png" alt="gRPC file scanning in Python" /></p>

<p>(Some messages appear seemingly "out of nowhere". That's because AV scanning is asynchronous.)</p>

<p>I don't think I can precisely estimate "real life" performance, but there's effectively no delay and scans complete within the same second (0.001-0.003 seconds per small file). That time would be longer for larger files, but multiple ClamAV workers can be used.</p>

<p>A better test is needed for precise measurement, but what I looked at was when my shell script finished running and when the last event from the last batch was processed.</p>

<pre><code class="language-raw">Last file event on FS: 1750566483.692
Last file scanned    : 1750566501.383
</code></pre>

<p>We can see (501-483) seconds have passed. This isn't too bad for 1,000 files and a single queue on a small VM.</p>

<p>gRPC events come in, get queued, and files (that have been written) are then scanned asynchronously, so nothing is being blocked. Millions can be queued in several GB of RAM. Or we could use Redis to queue them up and persist to disk before acknowledging receipt to BeeGFS gRPC client.</p>

<p>As I've mentioned earlier, N BeeGFS clients can scan 1/N files each. If we can scan 50 files per second, ten VMs *(2 vCPU, 8GB RAM) may give us aggregate scanning performance of 500 files (or S3 objects) per second for files already on file-system.</p>

<p>I created another test, same files and everything, but removed ClamAV from the loop so that there's only an async sleep of 0.001 seconds (to simulate a quick external "action" such as calling a Web hook) for each event processed. This took 9 seconds, so 100 write events per second including filtering and (external) action.</p>

<pre><code class="language-raw">Last file event on FS: 1750571129.416
Last file scanned    : 1750571138.230
</code></pre>

<p>I should mention that there are more than 1,000 events when 1,000 files are being overwritten (as in this case). There's a <code class="language-plaintext highlighter-rouge">TRUNCATE</code>, followed by a <code class="language-plaintext highlighter-rouge">FLUSH</code>, and then a <code class="language-plaintext highlighter-rouge">LAST_WRITER_CLOSED</code>. Also, if I removed the numerous print statements from the gRPC server, it should be even faster.</p>

<p>For S3 AV scanning (StorageGRID use case) we'd first have to download objects and then scan, so it'd be slower. For Versity S3 Gateway, no "download-from-S3" would be necessary. I assume in real life we wouldn't scan objects with extensions known to be safe (.json, .mp4…) which could translate into very few scan-able objects per second.</p>

<p>To scale even further we could have M gRPC servers and make every server process 1/M of all events.</p>

<h2 id="appendix-c-likely-bottlenecks">Appendix C: likely bottlenecks</h2>

<p>I expect that S3 object download would be the slowest part (e.g. 10 seconds), followed by ClamAV (&lt; 1 second). Both of these would require multiple BeeGFS (or non-BeeGFS, if you use something else) clients (VMs, containers), but can be done by simply adding more.</p>

<p>Web hooks are lightweight and not a problem.</p>

<p>At some point, the gRPC server may become a bottleneck itself.</p>

<p>With my Python-based gRPC server I receive gRPC requests, parse and filter events, and finally dispatch scan jobs. This happens in parallel, but maybe around 100 objects (which could be 1,000 BeeGFS events) per second, I would need to improve it.</p>

<p>That, too, should not be hard - we could simply use gRPC server(s) (as BeeGFS can send to multiple) to receive requests and pass them to a message server, and have the rest done by clients of that message server (e.g. NATS, Kafka). That way we could probably scale out by 10x to 1,000 objects per second (and by that time we'd need a bunch of BeeGFS/ClamAV nodes!)</p>

<p>At some even larger scale, it would be helpful to rewrite the gRPC server in Go and scale it even further, but I don't know of anyone who has that many new objects created every second in an on-premises environment. Even if they do, different buckets could send event notifications to different Webhooks or message stores, so there are plenty of chances to parallelize AV scanning.</p>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/beegfs" class="page__taxonomy-item p-category" rel="tag">beegfs</a><span class="sep">, </span>
    
      <a href="/tags/clamav" class="page__taxonomy-item p-category" rel="tag">clamav</a><span class="sep">, </span>
    
      <a href="/tags/eseries" class="page__taxonomy-item p-category" rel="tag">eseries</a><span class="sep">, </span>
    
      <a href="/tags/netapp" class="page__taxonomy-item p-category" rel="tag">netapp</a><span class="sep">, </span>
    
      <a href="/tags/pipeline" class="page__taxonomy-item p-category" rel="tag">pipeline</a><span class="sep">, </span>
    
      <a href="/tags/s3" class="page__taxonomy-item p-category" rel="tag">s3</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/ai" class="page__taxonomy-item p-category" rel="tag">ai</a><span class="sep">, </span>
    
      <a href="/categories/analytics" class="page__taxonomy-item p-category" rel="tag">analytics</a><span class="sep">, </span>
    
      <a href="/categories/storage" class="page__taxonomy-item p-category" rel="tag">storage</a>
    
    </span>
  </p>


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time class="dt-published" datetime="2025-06-15T00:00:00+08:00">2025-06-15 00:00</time></p>

      </footer>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://x.com/intent/tweet?via=scaleoutSean&text=Improved+pipelines+with+BeeGFS+FS+Event+Notifications+in+v8%20https%3A%2F%2Fscaleoutsean.github.io%2F2025%2F06%2F15%2Fpipeline-with-beegfs-file-system-notifications-v2.html" class="btn btn--x" aria-label="Share on X" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on X">
    <i class="fab fa-fw fa-x-twitter" aria-hidden="true"></i><span> X</span>
  </a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://scaleoutsean.github.io/2025/06/15/pipeline-with-beegfs-file-system-notifications-v2.html" class="btn btn--linkedin" aria-label="Share on LinkedIn" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn">
    <i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span>
  </a>

</section>


      
  <nav class="pagination">
    
      <a href="/2025/06/07/powershell-linux-long-startup-time-history-file.html" class="pagination--pager" title="Fix slow PowerShell startup time on Linux due to misconfigured environment">Previous</a>
    
    
      <a href="/2025/06/18/sfc-2-dot-1.html" class="pagination--pager" title="SFC (SolidFire Collector) 2.1">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
    
<div class="page__related">
  
  <h2 class="page__related-title">You may also enjoy</h2>
  <div class="grid__wrapper">
    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/2025/08/11/epa-4-beta.html" rel="permalink">E-Series Performance Analyzer (EPA) v4 beta
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2025-08-11T00:00:00+08:00">2025-08-11 00:00</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          9 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">
  Introduction
  EPA 4
  EPA Collector-specific
  Details related to storage requirements
  What's next

</p>
  </article>
</div>

    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/2025/08/11/bing-index-issue.html" rel="permalink">Got binged in July 2025
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2025-08-11T00:00:00+08:00">2025-08-11 00:00</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          3 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">I don't track users (see Privacy page), don't have ads and don't care about "likes" or "followers", but generally I want my content to be in search engine in...</p>
  </article>
</div>

    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/2025/07/30/solidfire-windows-admin-center-extension.html" rel="permalink">SolidFire Extension for Windows Admin Center 2025
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2025-07-30T00:00:00+08:00">2025-07-30 00:00</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          19 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">
  Introduction
  Walk-through
  And now…
  Conclusion
  Appendix A: demo
    
      Animated GIF (no playback control)
      Video demo with voice-over
    ...</p>
  </article>
</div>

    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/2025/07/26/solidfire-windows-admin-center-gateway.html" rel="permalink">SolidFire Gateway for Windows Admin Center 2025
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2025-07-26T00:00:00+08:00">2025-07-26 00:00</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          5 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">
  Introduction
  What it is
  What it does
  IIS Setup
  Why this matters
  What about it?
  Conclusion

</p>
  </article>
</div>

    
  </div>
</div>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        

<div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
          <li><a href="https://twitter.com/scaleoutsean" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/scaleoutsean" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>


<div class="page__footer-copyright">&copy; 2020-2025 scaleoutSean.github.io | <a href="https://scaleoutsean.github.io">Acting Technologist</a> | Terms: <a href="https://creativecommons.org/licenses/by/4.0/deed.en" target="_blank">CC BY 4.0</a>.</div>
<script src="https://cdn.counter.dev/script.js" data-id="83dc700a-b821-4e57-9425-02b8336a9456" data-utcoffset="0"></script>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>






  </body>
</html>
