<!doctype html>
<html lang="en">
  <head>
        <meta charset="UTF-8">
    <meta http-equiv="content-language" content="en">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="googlebot" content="noarchive">
    <meta name="googlebot" content="max-snippet:60">
    <meta name="googlebot" content="max-image-preview:small">
    <meta name="bingbot" content="noarchive">
    <meta name="bingbot" content="max-snippet:60">
    <meta name="bingbot" content="max-image-preview:small">
    <meta name="robots" content="noarchive">
    <meta name="robots" content="max-snippet:60">
    <meta name="google-site-verification" content="F6q7vIwQ2G0j8tk-KL9rAXOMLXMDMMUkEz4fRs1Nnnc" />
    <title>
        
            Cloudera Base with NetApp E-Series | Acting Technologist
      
    </title>
    <meta name="description" content="
     Notes on Cloudera with E-Series
     ">

    <!-- LINK TO ATOM FEED FOR SEO -->
    <link rel="alternate" type="application/atom+xml" href="https://scaleoutsean.github.io/feed.xml" />

    <!-- FAVICON -->
    <link rel="icon" href="/assets/favicon.ico" type="image/x-icon">
    <link rel="shortcut icon" href="/assets/favicon.ico" type="image/x-icon">

    <!-- CSS -->
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="stylesheet" href="/assets/css/syntax.css">

    <!-- Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css">

    <link href="/_pagefind/pagefind-ui.css" rel="stylesheet">
    <script src="/_pagefind/pagefind-ui.js" type="text/javascript"></script>
    <div id="search"></div>
    <script src="https://cdn.counter.dev/script.js" data-id="83dc700a-b821-4e57-9425-02b8336a9456" data-utcoffset="0"></script>
    <script>
        window.addEventListener('DOMContentLoaded', (event) => {
            new PagefindUI({ element: "#search" });
        });
    </script>

    <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Cloudera Base with NetApp E-Series | Acting Technologist</title>
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="Cloudera Base with NetApp E-Series" />
<meta name="author" content="scaleoutSean" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Notes on Cloudera with E-Series" />
<meta property="og:description" content="Notes on Cloudera with E-Series" />
<link rel="canonical" href="https://scaleoutsean.github.io/2025/04/16/cloudera-with-netapp-e-series.html" />
<meta property="og:url" content="https://scaleoutsean.github.io/2025/04/16/cloudera-with-netapp-e-series.html" />
<meta property="og:site_name" content="Acting Technologist" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-04-16T00:00:00+08:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"scaleoutSean"},"description":"Notes on Cloudera with E-Series","@type":"BlogPosting","headline":"Cloudera Base with NetApp E-Series","dateModified":"2025-04-16T00:00:00+08:00","datePublished":"2025-04-16T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://scaleoutsean.github.io/2025/04/16/cloudera-with-netapp-e-series.html"},"url":"https://scaleoutsean.github.io/2025/04/16/cloudera-with-netapp-e-series.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  </head>

<body>

  <div class="container">
    <header id="header">
	<div id = site_title>
		<a href="https://scaleoutsean.github.io/">
			<h2 style="font-size:1.8em;">Acting Technologist</h2>
		</a>
		
		<h3>
			civilizations are created by individuals
		</h3>
		
	</div>

	<div id="subheader">
		
		<nav class="pages">
<a href="/about.html">About</a>

<a href="/archive.html">Archive</a>

<a href="/categories/">Categories</a>

<a href="/projects.html">Projects</a>
</nav>
		
		
		<nav class="social">
			
  
    <a href="https://www.github.com/scaleoutsean" target="_blank" id="github"><i class="fab fa-github" aria-hidden="true"></i></a>
  
   

  
    <a href="https://twitter.com/scaleoutsean" target="_blank" id="twitter"><i class="fab fa-twitter" aria-hidden="true"></i></a>
  
   

  
  
    <a href="/feed.xml" target="_blank" id="rss"><i class="fas fa-rss" aria-hidden="true"></i></a>
   

		</nav>
		
	</div>
</header>


    <div class="post-container">
      <article id = "post">
        <h1 id = "post-title">Cloudera Base with NetApp E-Series</h1>

  
  <div class = "post-info">
    <span>
        <i class="far fa-calendar" aria-hidden="true"></i> <span>16 Apr 2025</span> - <i class="far fa-clock"></i> 


  
  
    12 minute read
  

    </span>
  </div>
  
        <ul>
  <li><a href="#introduction">Introduction</a></li>
  <li><a href="#reference-architectures">Reference architectures</a>
    <ul>
      <li><a href="#high-level-design-and-best-practices">High-level design and best practices</a></li>
    </ul>
  </li>
  <li><a href="#whats-new-in-cloudera-base">What’s new in Cloudera Base</a></li>
  <li><a href="#deployment-topology">Deployment topology</a></li>
  <li><a href="#sizing-and-hardware-selection">Sizing and hardware selection</a></li>
  <li><a href="#failure-handling">Failure handling</a></li>
  <li><a href="#operating-system-best-practices">Operating system best practices</a></li>
  <li><a href="#networking-and-security">Networking and security</a>
    <ul>
      <li><a href="#example-topologies">Example topologies</a></li>
    </ul>
  </li>
  <li><a href="#third-party-filesystems">Third party filesystems</a>
    <ul>
      <li><a href="#storage-scale-with-e-series">Storage Scale with E-Series</a></li>
    </ul>
  </li>
  <li><a href="#faqs">FAQs</a></li>
</ul>

<h2 id="introduction">Introduction</h2>

<p>First off, NetApp doesn’t validate or certify E-Series for Cloudera. Maybe you can skip the rest if you need just that info.</p>

<p>Second, while reading their documentation I realized Cloudera doesn’t seem too interested in the topic of external storage either. My guess is the KISS principle leads them to simply proposing DAS if the user doesn’t have some particular concern or requirement.</p>

<p>DAS works, it’s predictable, and it’s inexpensive.</p>

<p>But at the same time Cloudera also doesn’t say some external storage won’t work. And obviously it does work when sized and solutioned correctly.</p>

<h2 id="reference-architectures">Reference architectures</h2>

<p>For on-premises clusters, see <a href="https://docs.cloudera.com/cdp-reference-architectures/latest/cdp-pvc-base-ra.html">these reference architectures</a>.</p>

<p>I’ll go through this and make E-Series-related comments.</p>

<h3 id="high-level-design-and-best-practices">High-level design and best practices</h3>

<blockquote>
  <p>Cloudera Base on premises supports a variety of hybrid solutions where compute tasks are separated from data storage and where data can be accessed from remote clusters.</p>
</blockquote>

<p>E-Series is great for this because, unlike “modern” arrays, it supports a variety of RAID levels. Need RAID 10 for a DB without buying another array? Bring it on!</p>

<p>They mention three groups of workloads:</p>

<table>
  <thead>
    <tr>
      <th>Workload</th>
      <th>Recommended RAID Levels</th>
      <th>Media</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Data Engineering</td>
      <td>RAID 6 or DDP</td>
      <td>HDD or QLC</td>
    </tr>
    <tr>
      <td>Data Mart</td>
      <td>RAID 6 or DDP</td>
      <td>HDD or QLC</td>
    </tr>
    <tr>
      <td>Operational Database</td>
      <td>RAID 5 or RAID 10</td>
      <td>TLC or QLC</td>
    </tr>
  </tbody>
</table>

<p>Depending on requirements you could have multiple E-Series arrays. For example, you need enough throughput for 5,000 CPU cores - you can’t do this with one E-Series array. But for small clusters you could very well use just one (say, EF600) which is a hybrid box (NVMe TLC in controller shelf, SAS TLC and HDD in expansion shelves).</p>

<p><code class="language-plaintext highlighter-rouge">dfs.replication</code> (see <a href="https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml">here</a>) would be set to 2 for simple mirroring rather than the default RF3. RF2 was also <a href="https://www.netapp.com/media/16420-tr-3969.pdf">recommended</a> in the prehistoric TR-3969.</p>

<h2 id="whats-new-in-cloudera-base">What’s new in Cloudera Base</h2>

<p>This is from <a href="https://docs.cloudera.com/cdp-reference-architectures/latest/cdp-pvc-base-ra/topics/ra-cdpdc-whats-new.html">here</a> and I guess may be new in Cloudera 7, although I don’t really care. Let’s just get to the point:</p>

<blockquote>
  <p>Ozone is a scalable, redundant, and distributed object store, optimized for big data workloads. Apart from scaling to billions of objects of varying sizes, Ozone can function effectively in containerized environments such as Kubernetes and YARN.</p>
</blockquote>

<p><a href="/2022/07/06/apache-ozone-netapp-eseries.html">Apache Ozone S3 and NetApp E-Series</a> was a topic here back in 2022, so you can read about it and E-Series in that post. As mentioned in that post, it appears Cloudera provides support for Ozone (best to confirm with them), so storage just needs to provide sufficient bandwidth.</p>

<p>There’s a dedicated page on <a href="https://docs.cloudera.com/cdp-private-cloud-base/7.3.1/howto-next-gen-storage.html">“Next-Gen”</a> (aka Ozone) storage where you can see how it can be used with Cloudera. <a href="https://docs.cloudera.com/cdp-private-cloud-upgrade/latest/hdfs-ozone-migration/topics/hdfs-ozone-migration-intro.html">This page</a> shows how to migrate from HDFS to Ozone. I’d like to highlight this:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">ofs</code>: A Hadoop-compatible filesystem (HCFS) allowing any application that expects an HDFS-like interface to work against Ozone with no API changes. Frameworks like Apache Spark, YARN and Hive work against Ozone without the need of any change.</li>
</ul>

<p>What that means is:</p>

<ul>
  <li>Storage protocol simplicity - if you consider file and object (NFS, S3) to be simpler than block - is here for E-Series users. “<code class="language-plaintext highlighter-rouge">mc cp -r /data/in s3gw://datamart/etl</code>” and you’re done - new data is available to <code class="language-plaintext highlighter-rouge">ofs</code> clients!</li>
  <li>Look, Kubernetes! As I’ve been saying all along - you <em>don’t need</em> NetApp Trident to support E-Series in a Kubernetes environment. Why? Because Ozone has Erasure Coding, so PV failover isn’t critical.Host and worker reboots (and Ozone container restarts) can be handled transparently to S3 users. <a href="/2023/09/03/minio-erasure-coding-and-netapp-e-series.html#e-series-with-unprotected-media">Like with MinIO</a>, you can run Ozone on RAID 0 (which E-Series supports) and protect data exclusively with Ozone. Or you can do a combination (wide DDP on E-Series plus EC on Ozone on top of that).</li>
  <li>If you prefer to use S3 such as MinIO, <a href="https://community.cloudera.com/t5/Community-Articles/Working-with-S3-Compatible-Data-Stores-via-Apache-NiFi/ta-p/244584">that seems to work as well</a>. You can read about MinIO Erasure Coding with E-Series <a href="/2023/09/03/minio-erasure-coding-and-netapp-e-series.html">here</a>.</li>
</ul>

<p>If you still wonder about CSI drivers for E-Series, read <a href="/2022/12/09/directpv-topolvm-csi-lvm-das-k8s-with-eseries.html">this post</a>.</p>

<h2 id="deployment-topology">Deployment topology</h2>

<p>This is from <a href="https://docs.cloudera.com/cdp-reference-architectures/latest/cdp-pvc-base-ra/topics/ra-cdpdc-deployment-topology.html">here</a>. Cloudera recommends Spine &amp; Leaf for easy rack redundancy and scaling to many racks.</p>

<p>This has no effect on E-Series apart from the obvious:</p>

<ul>
  <li>If you want rack redundancy, you need multiple E-Series (e.g. two EF300 in two racks rather than one EF600 in one of three racks)</li>
  <li>If you use one or two E-Series across three racks you probably can’t use SAS on clients because maximum supported SAS cable length may prevent access to nodes in the rack without EF-Series; you want iSCSI or NVMe or IB or FC</li>
  <li>For a switchless storage design, it may be possible to use iSCSI or FC or NVMe, but only as long as the number of ports on E-Series is enough for direct-attach. For example, 4 hosts with 1 x 100G NVMe on each could connect to EF600 without a switch. Three racks, three EF600, 12 hosts.</li>
</ul>

<h2 id="sizing-and-hardware-selection">Sizing and hardware selection</h2>

<ul>
  <li><a href="https://docs.cloudera.com/cdp-reference-architectures/latest/cdp-pvc-base-ra/topics/ra-cdpdc-cluster-sizing-best-practices.html">Sizing</a></li>
  <li><a href="https://docs.cloudera.com/cdp-reference-architectures/latest/cdp-pvc-base-ra/topics/ra-cdpdc-cluster-hardware-selection-best-practices.html">Hardware selection</a></li>
</ul>

<p>Summary: don’t be stupid and go with large HDDs to meet the capacity requirement.</p>

<p>This documentation seems a bit aged, but to make it simple, for Data Engineering and Data Mart, let’s use 100 MB/s per core.</p>

<p>Let’s say we have 8 hosts with 32 cores = 256 x 0.1 GB/s = 25.6 GB/s. Since this more than E5760 can provide, we’d need roughly two E5760, but if we need to provide 2 copies, then twice as much (4 arrays). Because it’s 8 hosts, 4 arrays and 2 copies, we could split this in two racks.</p>

<p>Secondly, as we won’t need all disk slots, we can add some SAS SSDs for Operational Database workloads (R5 or R10). Or, for a more luxury approach, get one or (rack redundancy version) two EF300 and use E-Series or (better) native database replication to protect databases.</p>

<p>Thirdly, we can brainstorm about other options such as 9 hosts and 3 racks:</p>

<ul>
  <li>Instead of 4 E5760, get 3 E5760</li>
  <li>Instead of 2 replicas, use Erasure Coding on HDFS (6+3) or use Ozone with EC (6+3)</li>
  <li>EC 6+3 overhead is 50%, so 25.6 x 1.5 = 40 GB/s, which is roughly enough (or certainly enough, if you use EF600C (QLC) and don’t need more capacity than what 3 EF600C can provide). With EF300C or EF600C you may also be able to avoid dedicated EF for Operational Database</li>
</ul>

<p>Lastly, some points regarding throughput estimation:</p>

<ul>
  <li>if data format is compressed (by say 60%), actual writes with RF2 won’t be 2x, but 1 x 60% x 2x or only 1.2x. Make sure you consider this</li>
  <li>storage usually handles reads better, and E-Series is much faster with read, too. When sizing, consider whether you’re sizing for 100% write, 100% read or a mix of both</li>
</ul>

<p>Then there’s <a href="https://docs.cloudera.com/cdp-reference-architectures/latest/cdp-pvc-base-ra/topics/ra-cdpdc-data-density-per-drive.html">this</a>:</p>

<blockquote>
  <p>Cloudera does not support drives larger than 8 TB for HDFS data.</p>
</blockquote>

<p>This seems outdated. What’s wrong with 15.3 TB QLC SSDs?</p>

<p>There’s also this:</p>

<blockquote>
  <p>Running Cloudera Base on premises on storage platforms other than direct-attached physical disks can provide suboptimal performance.</p>
</blockquote>

<p>Driving in a car may result in a car crash. If we size correctly, there won’t be suboptimal performance.</p>

<h2 id="failure-handling">Failure handling</h2>

<p>That’s described <a href="https://docs.cloudera.com/cdp-reference-architectures/latest/cdp-ra-operations/topics/cdp-ra-failures.html">here</a> and with external storage not everything is the same.</p>

<p>External storage should have better availability and lower impact than DAS. One thing I mentioned earlier was the idea to “cut corners” for a switchless storage design, in which we’d max out hosts-per-array by using a single 100G link. Obviously, that means link failure results in loss of access to all disks for the host.</p>

<p>That must sound bad to “enterprise” users, but Cloudera claims it’s not a big deal.</p>

<table>
  <thead>
    <tr>
      <th>Failure</th>
      <th>Impact</th>
      <th>Note</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Multi-disk failure (Worker node)</td>
      <td>Low</td>
      <td>Allow automated Cloudera cluster recovery features to trigger</td>
    </tr>
  </tbody>
</table>

<p>You may disagree it’s “low”, but then you’d also begin to wonder what else they’re wrong about.</p>

<p>Another scenario in this vein is that a failed E-Series controller would take out all Cloudera workers connected to that array without redundant paths to the surviving controller.</p>

<p>If you’re worried about that, use multiple paths and add a pair of switches (e.g. dedicated FC, or allocate a few from existing Etherenet switches used by Compute Cluster).</p>

<p>The rest is more or less the same even across racks, if you have rack redundancy for storage (which would be 2 copies on protected RAID 6 LUNs on 2 arrays, or 3 copies on 3 arrays).</p>

<h2 id="operating-system-best-practices">Operating system best practices</h2>

<p>Cloudera supports <a href="https://docs.cloudera.com/cdp-reference-architectures/latest/cdp-pvc-base-ra/topics/ra-cdpdc-operating-system-best-practices.html">RHEL, SLES, Ubuntu</a> so compare that against the NetApp IMT.</p>

<p>As an aside, in a <a href="https://docs.cloudera.com/documentation/other/reference-architecture/topics/ra_cdh5_vmware_isilon.html#concept_htl_ybs_f2b">different topic</a> (also outdated, for Cloudera 5), there’s this note about OS boot disk in a VMware environment.</p>

<blockquote>
  <p>If storage is SAN-based, for 20 nodes, reserve 100 GB LUNs/datastores/VMDKs to each node.</p>
</blockquote>

<p>This means that if you run Cloudera in VMware, you could create a RAID 5 volume group with 3 TB usable, and cut it in 150 GB LUNs for a separate data store for each Clodera VM. Or create 3 LUNs for 3 Datastores, each for VMware in its own rack (with 3 racks), for example.</p>

<h2 id="networking-and-security">Networking and security</h2>

<p>Read about it <a href="https://docs.cloudera.com/cdp-private-cloud-upgrade/latest/upgrade/topics/cdpdc-networking-security-requirements.html">here</a>.</p>

<ul>
  <li>Cloudera can encrypt in-flight data also supports encryption for <a href="https://docs.cloudera.com/cdp-private-cloud-upgrade/latest/upgrade/topics/cdpdc-data-at-rest-encryption-requirements.html">data at rest</a></li>
  <li>E-Series is managed out of band (physically separate 1GigE LAN), so it doesn’t introduce new security concerns either in management or data encryption level. FIPS and FED disks are supported, but you probably don’t need them if that’s taken care of by Cloudera</li>
</ul>

<h3 id="example-topologies">Example topologies</h3>

<p>Let’s take a look at some examples.</p>

<p>Older Cloudera 5 with Ceph in OpenStack environment:</p>

<p><img src="/assets/images/cloudera-ceph-eseries.png" alt="Cloudera storage network with E-Series" /></p>

<p>Two points about storage access (unrelated to OpenStack and Ceph):</p>

<ul>
  <li>E-Series iSCSI would connect the same way, via IPv4. To avoid using Ethernet, we can use use IB or FC</li>
  <li>Cloudera itself would use a single network for its services</li>
</ul>

<p>Why we need to be careful out IP networking:</p>

<blockquote>
  <p>Multihoming Cloudera Runtime or Cloudera Manager is not supported outside specifically certified Cloudera partner appliances… Cloudera finds that current Hadoop architectures combined with modern network infrastructures and security practices remove the need for multihoming.</p>
</blockquote>

<p>Source: <a href="https://docs.cloudera.com/cdp-private-cloud-base/7.3.1/cdp-private-cloud-base-installation/topics/cdpdc-networking-security-requirements.html">here</a></p>

<p>There’s a “workaround”, but if Cloudera doesn’t support it there’s no need to consider it. More on this multihoming thing:</p>

<blockquote>
  <p>By default HDFS endpoints are specified as either hostnames or IP addresses. In either case HDFS daemons will bind to a single IP address making the daemons unreachable from other networks.</p>
</blockquote>

<p>Source: <a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsMultihoming.html#Multihoming_Background">here</a></p>

<p>This is with Isilon and older <a href="https://docs.cloudera.com/documentation/other/reference-architecture/topics/ra_cdh5_isilon.html">Cloudera 5</a> but probably still valid.</p>

<p><img src="/assets/images/cloudera-isilon-eseries.jpg" alt="Cloudera storage network with Isilon" /></p>

<p>Note that there’s no rack redundancy here. I guess the benefit of this approach is that <em>if</em> sufficient (non-blocking) bandwidth is provided, it doesn’t make much difference in terms of performance.</p>

<p>E-Series - especially if there’s just one box - would use this approach as well. But you have an option of multiple (entry-level) arrays with RF2 or RF3 with <a href="https://docs.cloudera.com/cdp-private-cloud-base/latest/kudu-configuration/topics/kudu-rack-awareness.html">rack awareness</a>. With RF2 and two racks all “local” workers would prefer to read from the replica stored on array in “local” rack.</p>

<p>With 3 racks and 2 E-Series, one rack would always read “remote” data, but this doesn’t worse than <em>all</em> storage traffic reading over multiple hops. It’s the same (writes) or better (reads for 66% of cases).</p>

<h2 id="third-party-filesystems">Third party filesystems</h2>

<p><a href="https://docs.cloudera.com/cdp-private-cloud-upgrade/latest/upgrade/topics/cdppvc-filesystems-IBM.html">These</a> are some of the validated non-standard ones: PowerScale (Isilon) and StorageScale (Spectrum Scale, GPFS).</p>

<h3 id="storage-scale-with-e-series">Storage Scale with E-Series</h3>

<p>E-Series supports GPFS, so you could buy <a href="https://www.ibm.com/docs/en/storage-scale/5.2.2">GPFS</a> and use it instead of HDFS.</p>

<p>If you use Ozone, maybe you don’t need a 3rd party filesystem, but GPFS has better support and more features. Note that on E-Series GPFS would work best with protected volumes (RAID 6, 8+2, usually, and RAID1 on SSDs for metadata)</p>

<p><img src="/assets/images/cloudera_gpfs_netapp_eseries.png" alt="GPFS on E-Series" /></p>

<p>This image depicts HDFS service for clients which translates requests to GPFS which uses “disks”. GPFS in this case runs on dedicated GPFS servers, which use (protected) E-Series LUNs. Cloudera workers don’t need to have much storage besides R1 boot media, although they could have local (internal) read-only cache that GPFS supports.</p>

<p>You can read more about GPFS with E-Series in <a href="https://www.netapp.com/media/22029-tr-4859.pdf">TR-4859</a>. This TR also has some indicative performance figures.</p>

<h2 id="faqs">FAQs</h2>

<p>Some comments on the <a href="https://docs.cloudera.com/cdp-reference-architectures/latest/cdp-pvc-base-ra/topics/ra-cdpdc-faqs.html">FAQs</a>.</p>

<blockquote>
  <p>The HDFS data directories should use local storage, which provides all the benefits of keeping compute resources close to the storage and not reading remotely over the network.</p>
</blockquote>

<p>This is nonsense.</p>

<p>If you read via NVMe without even a network switch in data path, what “latency” is there? Not to mention that - since NL-SAS is recommended - the latency of “remote” storage is much lower than IO latency of NL-SAS.</p>

<p>Secondly, the moment you use Cloudera in a cluster spanning multiple racks, the question becomes: do you still want to use DAS and RF3? Maybe you do, but you’re sending 200% more writes over network. If you use Erasure Coding, you send only 50% more, but then you may need to read it from multiple racks, which means a lot more network hops, so “reading remotely over network” happens all the time anyway.</p>

<p>The same applies to Ozone or S3 which would generally be running on “dense” storage nodes and use EC.</p>

      </article>
    </div>
    
      <div class="categories">
    <span><p>Categories:</p>
    
    
      <a href="
      /categories/#storage">storage</a>
      &nbsp; 
    
      <a href="
      /categories/#analytics">analytics</a>
       
    
  </span>
</div>
    

  
    <div>
      <h3>Related Posts</h3>
      <ul>
      
        <li><a href="/2023/07/28/thanos-with-minio-eseries-or-ontap-s3.html">Thanos with different S3 backends - StorageGRID, ONTAP S3 and MinIO on E-Series</a></li>
      
        <li><a href="/2022/08/09/nomad-beegfs-minio-s3.html">Simple S3 service endpoint for BeeGFS using Hashicorp Nomad and stand-alone MinIO</a></li>
      
        <li><a href="/2021/11/21/alluxio-ontap.html">Alluxio and ONTAP NFS</a></li>
      
        <li><a href="/2021/11/12/alluxio-storagegrid-s3.html">Alluxio and StorageGRID</a></li>
      
        <li><a href="/2022/06/22/e-series-hdfs.html">Apache Hadoop 3 with NetApp E-Series</a></li>
      
      </ul>
    </div>
  

    
  </div><footer class= "footer">
    <p>2025-05-23 19:57 </p>
    <p>Copyright © 2025 scaleoutSean. Content is released under the CC BY license. Design: Alessio Franceschi</p>
</footer>

</body>
</html>
