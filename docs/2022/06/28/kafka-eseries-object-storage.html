<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.27.3 by Michael Rose
  Copyright 2013-2025 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>E-Series as Tier One for multi-tiered Kafka clusters | Acting Technologist</title>
<meta name="description" content="E-Series as Tier 1 in multi-tiered Kafka clusters">


  <meta name="author" content="scaleoutSean">
  
  <meta property="article:author" content="scaleoutSean">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Acting Technologist">
<meta property="og:title" content="E-Series as Tier One for multi-tiered Kafka clusters">
<meta property="og:url" content="https://scaleoutsean.github.io/2022/06/28/kafka-eseries-object-storage.html">


  <meta property="og:description" content="E-Series as Tier 1 in multi-tiered Kafka clusters">





  <meta name="twitter:site" content="@scaleoutSean">
  <meta name="twitter:title" content="E-Series as Tier One for multi-tiered Kafka clusters">
  <meta name="twitter:description" content="E-Series as Tier 1 in multi-tiered Kafka clusters">
  <meta name="twitter:url" content="https://scaleoutsean.github.io/2022/06/28/kafka-eseries-object-storage.html">

  
    <meta name="twitter:card" content="summary">
    
  

  



  <meta property="article:published_time" content="2022-06-28T00:00:00+08:00">






<link rel="canonical" href="https://scaleoutsean.github.io/2022/06/28/kafka-eseries-object-storage.html">







  <meta name="google-site-verification" content="F6q7vIwQ2G0j8tk-KL9rAXOMLXMDMMUkEz4fRs1Nnnc" />


  <meta name="msvalidate.01" content="7cf5b7d96a77410a8ad035f764dc81b3">





<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Acting Technologist Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  window.enable_copy_code_button = true;
</script>

<script src="https://cdn.counter.dev/script.js" data-id="83dc700a-b821-4e57-9425-02b8336a9456" data-utcoffset="0"></script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <link href="https://fonts.googleapis.com/css?family=PT+Sans+Narrow:400,700|PT+Serif:400,700,400italic" rel="stylesheet" type="text/css">


  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
          <a class="site-logo" href="/"><img src="/assets/images/scaleoutsean-acting-technologist.png" alt="Acting Technologist"></a>
        
        <a class="site-title" href="/">
          Acting Technologist
          <span class="site-subtitle">human action through technology</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/archive/"
                
                
              >Archive</a>
            </li><li class="masthead__menu-item">
              <a
                href="/categories/"
                
                
              >Categories</a>
            </li><li class="masthead__menu-item">
              <a
                href="/tags/"
                
                
              >Tags</a>
            </li><li class="masthead__menu-item">
              <a
                href="/projects.html"
                
                
              >Projects</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main">
  


  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="E-Series as Tier One for multi-tiered Kafka clusters">
    <meta itemprop="description" content="E-Series as Tier 1 in multi-tiered Kafka clusters">
    <meta itemprop="datePublished" content="2022-06-28T00:00:00+08:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://scaleoutsean.github.io/2022/06/28/kafka-eseries-object-storage.html" itemprop="url">E-Series as Tier One for multi-tiered Kafka clusters
</a>
          </h1>
          

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2022-06-28T00:00:00+08:00">2022-06-28 00:00</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          18 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
        <ul>
  <li><a href="#multi-tiered-storage-in-kafka-clusters">Multi-tiered storage in Kafka clusters</a></li>
  <li><a href="#how-to-leverage-e-series">How to leverage E-Series</a>
    <ul>
      <li><a href="#performance-vs-data-protection-overhead">Performance vs. data protection overhead</a></li>
      <li><a href="#sequential-performance-vs-latency">Sequential performance vs. latency</a></li>
      <li><a href="#tiering-to-s3">Tiering to S3</a></li>
      <li><a href="#compression">Compression</a>
        <ul>
          <li><a href="#evaluating-data-efficiency">Evaluating data efficiency</a></li>
        </ul>
      </li>
      <li><a href="#snapshots-and-replication">Snapshots and replication</a></li>
      <li><a href="#environmentals">Environmentals</a></li>
    </ul>
  </li>
  <li><a href="#conclusion">Conclusion</a></li>
</ul>

<p>I've blogged about the unnecessary decade long abuse of JBOD and DAS storage in the context of Hadoop, Splunk, Elastic, Vertica and other platforms and applications.</p>

<p>Rather than belabor the points that most people already know by know, I'll keep this one short.</p>

<h2 id="multi-tiered-storage-in-kafka-clusters">Multi-tiered storage in Kafka clusters</h2>

<p>Confluent <a href="https://www.confluent.io/blog/confluent-platform-6-0-delivers-the-most-powerful-event-streaming-platform-to-date/">introduced</a> Tiered Storage in Confluent Platform 6.</p>

<p>Tiering is done similarly to how it's done elsewhere:</p>

<ul>
  <li>Small Tier 1 for hot storage</li>
  <li>Big Tier 2 for warm/cold storage</li>
</ul>

<p><img src="/assets/images/kafka-eseries-object-storage.png" alt="Kafka with Tier 1 on E-Series SAN and Tier 2 on Object Storage" /></p>

<p>With this approach we can:</p>

<ul>
  <li>Use smaller and cheaper servers servers</li>
  <li>Save rack space and energy</li>
  <li>Deploy, maintain and upgrade with ease</li>
  <li>Lower software licensing and maintenance fees</li>
  <li>Gain agility and simplicity</li>
  <li>Greatly improved Kafka Self-Balancing Clusters (when Tier 1 contains just a small amount of data)</li>
  <li>etc (you get the idea)</li>
</ul>

<h2 id="how-to-leverage-e-series">How to leverage E-Series</h2>

<p>Technically we don't "need" to use E-Series (or other SAN) for hot data. It's fine to use broker-internal NVMe in RAID1, for example.</p>

<p>But we've already been through this with Splunk and other applications: once a deployment gets to 10 broker/index/whatever servers, application owner realizes they have 20 NVMe internal disks in RAID1, and one E-Series EF300 with 12 disks in a RAID10 group would have been the same or better value.</p>

<p><em>And</em> they still purchased some external storage for other Kafka applications, databases, management servers and more, effectively spending more in order to get less (e.g. Zookeeper dedicated transaction log volume, Confluent Control Center 300 GB SSD, Kafka Streams 100-300 GB SSD). Plus there's more things to manage.</p>

<p>So, how can we leverage E-Series as Tier 1 storage for Kafka clusters?</p>

<p>The same way we'd do it for Splunk SmartStore, Vertica Eon Mode and other applications that use the tiering pattern:</p>

<ul>
  <li>Configure EF300 (up to 7 GB/s write) or EF600 (up to 15 GB/s write) with a several RAID1 (2 disks) groups or one RAID10 (4-24 disks) or DDP (11-24) volume group for of Kafka brokers
    <ul>
      <li>The larger E-Series model, EF600, can deliver full performance with just one (controller) shelf full of disks (24 NMVe), so you can roughly imagine it delivers &gt; 500 MB/s write performance per populated slot (in DDP configuration)</li>
      <li>The smaller E-Series model (EF300) would deliver approximately half of that</li>
      <li>Size for performance first (DDP or RAID10), pick the number of slots; consider leaving some capacity unused for wear leveling, and then size for usable capacity by choosing the right NVMe disk size (currently 1.92-15.3 TB disks are available)</li>
    </ul>
  </li>
  <li>Create N or N*2 volumes for N broker servers to better spread the workload
    <ul>
      <li>Connect N broker nodes to E-Series using direct attach (for 2-4 broker nodes) or SAN (more than 4 nodes)</li>
      <li>Use iSCSI for up to 25 Gbps, FC or NVMe/FC for 32 Gbps, or Infiniband for 100 or 200 Gbps</li>
    </ul>
  </li>
</ul>

<p>That would take care of hot tier, which can be small (according to Confluent, between 0.1 and 1 TB). Some related best practices can be found <a href="https://docs.netapp.com/us-en/netapp-solutions/data-analytics/confluent-kafka-best-practice-guidelines.html">here</a>. Configure S3 tier for warm/cold data.</p>

<h3 id="performance-vs-data-protection-overhead">Performance vs. data protection overhead</h3>

<p>If you need extra capacity in hot tier (which you may if you frequently pull data going back days and your S3 is slow), other disk slots in E-Series array can be populated with different-sized disks and used for other applications (VMware, <a href="/2022/05/18/vmware-tanzu-netapp-eseries.html">Tanzu</a>/Kubernetes, software-defined S3, databases, etc.). This disk group can be protected with DDP (<a href="https://www.netapp.com/data-storage/what-is-dynamic-disk-pools-technology/">RAID6-like protection schema</a>) to avoid capacity overheads of RAID10 - as long as you still get enough performance from this tier.</p>

<p>I expect an all-DDP (which requires at least 11 slots/disks) configuration should be good enough in most cases, but haven't had a chance to verify this in practice.</p>

<p>If we start with 11 small capacity (e.g. 1.92TB) disks, we can grow DDP by adding as few as 1 disk each time, as many as 13 the first time (11 + 13 = 24), to get more and more performance and capacity while:</p>

<ul>
  <li><em>lowering</em> disk reconstruction time as DDP pool grows (see <a href="/2021/07/06/e-series-ddp-expansion-and-rebalancing.html">this</a>)</li>
  <li><em>lowering</em> the impact of data reconstruction on performance due to disk failure (with DDP it's normally <a href="https://www.netapp.com/pdf.html?item=/media/12421-tr4652.pdf">around 25%</a>), and</li>
  <li><em>decreasing</em> DDP overhead; with DDP, one full EF300/EF600 controller shelf would have protection overhead of 24/22 (9%), compared to 11/9 or 22% in a minimal DDP deployment - both are significantly better than RAID10 or RAID1 installed in a bunch of brokers servers</li>
</ul>

<p>We need to remember that all disks in a DDP should be the same size. Here are some examples of three configurations using RAID10, RAID6 and DDP (two pool sizes, with 2 disks worth of spare capacity in each):</p>

<table>
  <thead>
    <tr>
      <th>#</th>
      <th style="text-align: center">Protection</th>
      <th style="text-align: center">Disks</th>
      <th style="text-align: center">Overhead</th>
      <th style="text-align: center">Overhead %</th>
      <th style="text-align: center">Rel. Seq Perf 80%w</th>
      <th style="text-align: center">Relative Latency</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td style="text-align: center">RAID10</td>
      <td style="text-align: center">8</td>
      <td style="text-align: center">8/4</td>
      <td style="text-align: center">200</td>
      <td style="text-align: center">1.0</td>
      <td style="text-align: center">1.0</td>
    </tr>
    <tr>
      <td>2</td>
      <td style="text-align: center">RAID6</td>
      <td style="text-align: center">10</td>
      <td style="text-align: center">10/8</td>
      <td style="text-align: center">125</td>
      <td style="text-align: center">1.4</td>
      <td style="text-align: center">1.4</td>
    </tr>
    <tr>
      <td>3</td>
      <td style="text-align: center">DDP(11)</td>
      <td style="text-align: center">11</td>
      <td style="text-align: center">11/9</td>
      <td style="text-align: center">122</td>
      <td style="text-align: center">1.6</td>
      <td style="text-align: center">1.3</td>
    </tr>
    <tr>
      <td>4</td>
      <td style="text-align: center">RAID10</td>
      <td style="text-align: center">12</td>
      <td style="text-align: center">12/6</td>
      <td style="text-align: center">200</td>
      <td style="text-align: center">1.5</td>
      <td style="text-align: center">1.5</td>
    </tr>
    <tr>
      <td>5</td>
      <td style="text-align: center">DDP(20)</td>
      <td style="text-align: center">20</td>
      <td style="text-align: center">20/18</td>
      <td style="text-align: center">111</td>
      <td style="text-align: center">1.8</td>
      <td style="text-align: center">1.3</td>
    </tr>
  </tbody>
</table>

<p>Usable (TB) for high DWPD environments (see pages 71 and 72 of <a href="https://www.netapp.com/media/17009-tr4800.pdf">this TR</a>) will be lower than expected, i.e. when sizing for 5 DWPD we should leave approximately 28% of usable disk capacity unprovisioned if we wanted to extend lifetime of flash media</p>

<ul>
  <li>For example, 8 * 1.92TB * 5 DWPD equals 76TB/day or close to 1 GB/s; although the array can deliver more than 1 GB/s, non-stop writing at &gt;1GB/s would result in excessive write workload, potentially increasing failure rates. Normally, though, very few workloads result in average &gt;1 DWPD per disk and Kafka workload is sequential, which means much less impactful on flash media</li>
  <li>DDP uses spare capacity (not spare <em>disks</em>), while RAID6 and RAID10 would need at least one hot or cold spare (not included in overhead figure above)</li>
</ul>

<p>Relative latency (last column in the chart) seems much higher (30-50%), but even the worst case (say, 1ms vs. 1.5ms) probably has limited real life impact.</p>

<h3 id="sequential-performance-vs-latency">Sequential performance vs. latency</h3>

<p>Two last columns in table above show relative performance vs. relative latency. Where do they come from and how to read them?</p>

<ul>
  <li>I sized EF300 with SSDs; each row under the assumption of 64kB sequential I/O, 80% write workload
    <ul>
      <li>Maybe 256kB request size and 90% write would be more appropriate for tiered Kafka, but the results wouldn't be very different</li>
    </ul>
  </li>
  <li>Relative sequential write performance is better for DDP and RAID6 because they don't mirror writes - they stripe brokers' writes across all disks in a pool/group</li>
  <li>This of course results in slightly higher latency, but even at maximum performance for the pool/group, it's still &lt;2ms (not exactly high)
    <ul>
      <li>Let's say a RAID configuration with 10 disks and 80% sequential write workload results in 5 GB/s @ 1.5ms latency; if clients consume 4GB/s rather than 5GB/s, I/O latency could very well be below 1ms</li>
    </ul>
  </li>
</ul>

<p>Let's consider scenarios 2, 3 and 4 from table above. If EF300 with 10-12 disks in any of these protection schemas can deliver enough capacity and performance</p>

<ul>
  <li>RAID6 would give us best sequential performance (scenario 2) with a small 125% overhead</li>
  <li>Scenario #3 would give us slightly lower protection overhead vs. scenario #2 (122 vs 125%), better write performance (1.5x vs 1.4x), and smaller latency</li>
  <li>Scenario #4, 12 disks in RAID10 group, doesn't look great (large capacity overhead, mediocre performance)</li>
  <li>What's not in the chart is "next step" for our cluster: what do we plan to do next year?
    <ul>
      <li>DDP (pool) can be grown by adding 1 SSD at a time - very convenient</li>
      <li>RAID10 can be grown by adding even number of disks (2, 4, 8). It makes us run out of available disk slots sooner</li>
      <li>RAID6 (8 data + 2 parity) should be grown in groups, by adding 10 disks at a time, so not very convenient for Kafka broker hot tier unless you have big clusters and use multiple E-Series arrays. But it may be OK if you used RF2 on Kafka, and multiple E-Series arrays with two RAID6 (8+2) per array. Still, DDP is much less rigid, so I'd prefer to use DDP</li>
      <li>Going beyond one controller shelf (i.e. 24 disks) requires add expansion shelves and is usually used only to add capacity, not performance, so if we plan to grow Kafka capacity quickly and don't want to add additional controller shelves or additional controllers, we should start with 8-11 larger NVMe disks rather than 20 tiny disks. If we plan to grow Kafka performance quickly, then smaller disks are fine - we can go with EF300 and use multiple arrays</li>
    </ul>
  </li>
</ul>

<p>Some anecdotal evidence from inserting random data into a single Kafka 3.2.0 broker (1 topic, 1 partition, RF1, 100ms linger, 128KiB batching):</p>

<ul>
  <li>For some reason, gzip looked very slow (the IOPS chart can be ignored, but it's there to give you an idea of IO request sizes observed, if you divide throughput by IOPS)</li>
</ul>

<p><img src="/assets/images/kafka-eseries-compression-random-data-uncompressed-and-gzip.png" alt="Comparison of throughput with non-random JSON data" /></p>

<ul>
  <li>Snappy and zstd seemed okay (we're looking at duration of each run here; JSON data was random so compression ratios were poor across the board) and LZ4 seemed suspiciously fast. Additional tests should be done.</li>
</ul>

<p><img src="/assets/images/kafka-eseries-compression-random-data-snappy-zstd-lz4.png" alt="Comparison of throughput with non-random JSON data" /></p>

<ul>
  <li>Another run done with <em>non-random</em> JSON contents (fake user profile data, ~1kiB per JSON document) seemed to produce more sensible results. This run read a 10GiB JSON collection off local OS disk and used Kafka producer to send few million of those JSON records to Kafka.</li>
</ul>

<p><img src="/assets/images/kafka-eseries-compression-non-random-data-throughput.png" alt="Comparison of throughput with non-random JSON data" /></p>

<p>In all of these tests Kafka latency was 1.5-7 ms. Kafka broker was configured to flush to disk every few seconds, which avoided burstiness and smoothed out write peaks. Without it writes become bursty - no I/O for 15 seconds, then 500-600 MB/s for 3-4 seconds. I did not measure latency in these "non-smoothed" tests.</p>

<p>With RF2 we'd need 200 MB/s write per broker in the "smoothed-out" scenario, and ~1 GB/s per broker with default Kafka flush settings which indicates that the smaller model, EF300, should be able to handle half a dozen brokers with that workload. (At the same time, broker CPU utilization was &lt;10%, so physically we would probably need just 3-4 servers and 6-8 brokers running in VMs or containers to get more out of the hardware.)</p>

<h3 id="tiering-to-s3">Tiering to S3</h3>

<p>Depending on capacity of, and use case for, Kafka's Tier 1, Object Storage may turn out to be "hot" (or not):</p>

<ul>
  <li>if Kafka consumers need fast response for data that doesn't go beyond 7 days and Tier 1 can hold 14 days of data, Object Storage can be low-performance (HDD) and/or remote because it probably won't be used much</li>
  <li>if Kafka's Tier 1 storage is tiny (0.1 TB per broker, for example), obviously it's unlikely it will be able to hold weeks of data, which means Object Storage will be busy, and should be fast and/or located nearby (e.g. on-prem, and maybe use only SSD/NVMe media for Object Storage, or have an SSD/NVMe tier in S3)</li>
  <li>some Object Storage, such as NetApp StorageGRID, can consist of heterogeneous nodes (e.g. all-flash and NL-SAS nodes) and use ILM policies to adjust not only where data is placed, but also how
    <ul>
      <li>2 Copies on All Flash nodes for all objects within 30 days of creation (gives us faster read performance for recent data)</li>
      <li>Erasure Coding 2+1 with NL-SAS HDD placement for all large objects older than 30 days (lowers object storage software overhead from 200% (RF2) to 150% (EC 2+1), both of these are layered on top of R6-equivalent (StorageGRID requirement) volumes)</li>
    </ul>
  </li>
</ul>

<p>Kafka Tier 1 on E-Series with R10 would result in less usable capacity with faster performance and normally use one hot spare (not displayed).</p>

<p>DDP delivers lower performance for the same number of disks, but overheads of DDP are very limited and can tolerate two concurrent disk failures. DDP reserve is shown in yellow; normally that amounts to two disks worth of capacity for reconstruction in the case one or two disks fail. Other than that, DDP requires no dedicated hot spares so its overhead advantage over RAID6 or RAID10 is even better than the table above suggests.</p>

<p>For classic Kafka we'd use two E-Series arrays and RF2, but with tiered Kafka there's probably no need to use multiple arrays, so I'd consider using a single array (assuming sufficient performance and capacity) either R10 (or multiple 2-disk R1) or DDP, not both.</p>

<p><img src="/assets/images/kafka-eseries-object-r10-and-ddp-storage.png" alt="Kafka with Tier 1 on E-Series R10 or DDP and Tier 2 on Object Storage with RF2 and EC2+1" /></p>

<p>Object Storage (at the bottom) could use multi-replica or Erasure Coding. Some S3 software requires that S3 storage nodes use protected storage (here, RAID6, but it could be RAID5 or DDP or something else), other does not (that, however, results in a longer and more impactful drop in performance when recovering from disk failures).</p>

<h3 id="compression">Compression</h3>

<p>E-Series doesn't compress data; the idea is to make IO path as lean and fast as possible. In the past this meant if the application happened to not compress data that could be nicely compressed (early MongoDB, for example), we left some savings on the table. These days, especially with NOSQL-ish workloads, data can be compressed on the application before being sent to storage. E-Series also doesn't deduplicate, but even if it did, I doubt much could be saved by deduplicating compressed Kafka blobs - even with RF2.</p>

<p>Kafka supports compression (GZip, Snappy, Lz4, zstd) which - after eating up some CPU and memory resources on producers or brokers - has two positive effects on storage sizing:</p>

<ul>
  <li>Lowers network, and storage I/O and capacity requirements (or, lets us get 100% more performance and capacity on the same E-Series array, assuming 50% savings from compression)</li>
  <li>Lowers wear on SSDs (e.g. 3 DWPD &gt; 1.5 DWPD), allowing for less hold-back on usable storage capacity</li>
</ul>

<p>Assuming you don't mind the extra latency and CPU resources, enable compression.</p>

<p>If we tier Kafka data to S3 we won't have much data on E-Series to begin with, but compression can save some egress fees (public S3) and/or decrease latency and increase performance of S3 tier (private or public S3), so I would try to enable compression on Kafka brokers in all scenarios (tiered, non-tiered).</p>

<p>Some anecdotal chart pr0n from tests executed on dual Xeon 6130 bare metal system connected to an older all-flash EF array using iSER:</p>

<ul>
  <li>Kafka 3.2.0</li>
  <li>1 partition, 1 topic, RF1, same client and server</li>
  <li>100ms linger and 128KiB record batches on Kafka producer</li>
  <li>Non-random JSON records emulating user profile data (contact info, self-introduction, location, etc.), approximately 1KiB per record</li>
</ul>

<p>The effect of compression on throughput and latency was minimal; CPU utilization was below 10%.</p>

<p><img src="/assets/images/kafka-eseries-compression-latency-example.png" alt="Effect of compression on throughput and latency" /></p>

<p>With the same linger and batch-size settings for each run, differences in records/s weren't significant.</p>

<p><img src="/assets/images/kafka-eseries-compression-rps-example.png" alt="Comparison of rec/s with and without compression" /></p>

<p>It doesn't make sense that uncompressed was slower than zstd, for example, so I should have done more runs, rebooted between runs, etc.</p>

<h4 id="evaluating-data-efficiency">Evaluating data efficiency</h4>

<p>I executed two simple tests using a 12-core VM (both producer and broker; 1 topic, 1 partition, 1 replica, 500ms linger, 128kiB batch size):</p>

<ul>
  <li>Test 1: 1kB plain text records made of random uppercase alphabetic characters - data reduction with gzip was 18%</li>
  <li>Test 2: 17MB audit log file in JSON format (StorageGRID audit log, 35K JSON documents) ingested using different compression algorithms - data reduction with selected algorithms was much higher:</li>
</ul>

<table>
  <thead>
    <tr>
      <th>Item</th>
      <th style="text-align: right">gzip</th>
      <th style="text-align: right">Snappy</th>
      <th style="text-align: right">zstd</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Records/s</td>
      <td style="text-align: right">52000</td>
      <td style="text-align: right">82000</td>
      <td style="text-align: right">79000</td>
    </tr>
    <tr>
      <td>Ingress (MB/s)</td>
      <td style="text-align: right">24</td>
      <td style="text-align: right">38</td>
      <td style="text-align: right">37</td>
    </tr>
    <tr>
      <td>Avg latency (ms)</td>
      <td style="text-align: right">15</td>
      <td style="text-align: right">33</td>
      <td style="text-align: right">26</td>
    </tr>
    <tr>
      <td>Data reduction (%)</td>
      <td style="text-align: right">80</td>
      <td style="text-align: right">65</td>
      <td style="text-align: right">81</td>
    </tr>
  </tbody>
</table>

<p>These aren't guidelines on whether to use compression (and if yes, which one) but very simple examples to give you an idea that depending on data content and format, CPU speed and other factors, it's best to optimize your settings rather than follow generic recipes.</p>

<p>With random non-repeatable text not even 20% of capacity can be saved despite using a lot of CPU resources, but with structured and repetitive data some algorithms will be better than others - depending on resources and priorities.</p>

<p>I did three runs for each test, but I don't think these results are very accurate (each run took less than 1 second, etc). But for the sake of an argument, if we were to collect StorageGRID audit logs with Kafka, I'd consider zstd because it's faster than gzip (not because it saves 1% more capacity). I'd also do additional tests to examine read latency and CPU utilization, because those metrics weren't captured.</p>

<h3 id="snapshots-and-replication">Snapshots and replication</h3>

<p>E-Series can take volume snapshots, but I don't see why anyone would want to use them on tiered Kafka volumes - it doesn't seem to make any sense, with 99% of data on S3. When <em>all</em> Kafka data is on E-Series, it might make sense to take snapshots before major upgrades or patches.</p>

<p>E-Series arrays are reliable and have redundant components (controller ports, controllers, data/disk protection), but controllers (and disks in them) can't be split across racks.</p>

<ul>
  <li>Use Kafka's RF2 replication for local (rack, row, floor) array redundancy
    <ul>
      <li>RF2 also helps with multiple EF arrays if array sizing consumes &gt;50% of maximum performance and we can't afford the loss of <em>performance</em> (i.e. in the case of controller failure).</li>
      <li>RF2 on single EF series array makes sense if you want to protect Kafka from filesystem corruption or OS downtime of individual broker nodes; this can be done on a single EF300 (remember to size for 2x write workload in that case)</li>
    </ul>
  </li>
  <li>Use Geo-Replication for replication between Kafka clusters</li>
</ul>

<h3 id="environmentals">Environmentals</h3>

<ul>
  <li>"Classic" configuration
    <ul>
      <li>12 2U servers with JBOD or 12 NL-SAS (Total: 24U; 144 NL-SAS HDDs or around 190 TB usable with 4TB disks and RF3)</li>
    </ul>
  </li>
  <li>Modern configuration with Kafka tiering to S3
    <ul>
      <li>6 1U servers with Hot Tier on 1 EF300 (Total: 8U; 12 NVMe SSDs)</li>
      <li>3 or more StorageGRID appliances or other S3 storage
        <ul>
          <li>3 (all flash) SGF6024's take 9U (1U server + 2U storage = 3U per appliance)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><em>Approximate</em> environmentals* for tiered storage scenarios 2, 3 and 4 (10-12 disks in a single EF300) with 3 x SGF6024:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: right">Item</th>
      <th style="text-align: right">Watts (typical)</th>
      <th style="text-align: right">BTU/hr</th>
      <th style="text-align: right">kWh/yr</th>
      <th style="text-align: center">RU</th>
      <th style="text-align: right">Usable (TiB)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right">1 x EF300</td>
      <td style="text-align: right">770W</td>
      <td style="text-align: right">2,600</td>
      <td style="text-align: right">6.75</td>
      <td style="text-align: center">2</td>
      <td style="text-align: right">13.8 (DDP, 11x1.92TB)</td>
    </tr>
    <tr>
      <td style="text-align: right">3 x SGF6024**</td>
      <td style="text-align: right">4,500W</td>
      <td style="text-align: right">17,200</td>
      <td style="text-align: right">41.00</td>
      <td style="text-align: center">9</td>
      <td style="text-align: right">170 (2-copy on S3)</td>
    </tr>
  </tbody>
</table>

<ul>
  <li><code class="language-plaintext highlighter-rouge">*</code>  - these are just approximate and will vary depending on load, disk size, and more; if you need official figures please reach out to NetApp</li>
  <li><code class="language-plaintext highlighter-rouge">**</code> - apart from Rack Units, other environmentals for 3 x SGF6024 (24 x 7.6 TB disks each) do not include power consumption for 3 x 1U StorageGRID server "heads"</li>
  <li>Some customers may require a load balancer to balance Kafka broker connections among object storage appliances, for which we could use VM based StorageGRID or dedicated hardware (NetApp SG100, F5, GLB, or existing NLB). In terms of rack units used, if NetApp SG100 (1U) is used, that's an additional 2 x 1U for an HA pair. Power/BTU specifications for those generally correspond to regular 1U 2-socket Intel-based servers with high core count. Total RU count for this tiered setup then becomes 6 x 1U for Brokers + 3 x 3U for SG6024 + 2 x 1U for SG100(0) NLB = 17U, but can be 15U without NLB or something entirely different if different appliances are used</li>
</ul>

<p>In the case you wonder how come we get only 170 TiB from 3 x SGF6024, each of which has 24 7.6TB disks: this assumes we save two copies of each object ("2-copy policy on S3") and the appliances use DDP-style data protection inside, so overhead of this approach is similar to mirrored RAID 6 (overhead of 2-copy over DDP is similar to overhead of mirrored RAID6), so it's still lower than RF3 with JBOD.</p>

<p>Now, admittedly this scenario uses all-flash object storage appliances, which is more expensive and saves more power, but:</p>

<ul>
  <li>we used 2-copy policy for S3, which isn't very most economical approach we can have. With <em>four</em> SGF6024 nodes we could use Erasure Coding 2+1, rather than 2-copy policy for first layer of S3 data protection to make the cost of capacity cheaper and overheads lower. For example, 4 x SGF6024 with 3.8TB SSDs and EC 2+1 gives us close to 147 TiB usable (after EC 2+1 and DDP-like protection in appliances), so by adding one additional S3 appliance and switching to 50% smaller disks, we've "lost" only 23 TiB of usable capacity compared to 2-copy policy with 3 x SGF6024 while decreasing overhead</li>
  <li>generally speaking, larger S3 clusters - or hybrid S3 clusters (10-20% of S3 appliances with SSDs, 80-90% with NL-SAS HDDs) - make the economics of tiered Kafka much better because overheads are generally low (EC 2+1 over DDP vs RF3 with JBODs can save 30-40%, <a href="/2022/06/22/e-series-hdfs.html#erasure-coding">depending on how wide DDP is</a>), and the cost of this capacity can be spread across many services, not just Kafka but also backup, container registry, archives, and more</li>
</ul>

<h2 id="conclusion">Conclusion</h2>

<p>Various if-else statements in this post make it impossible to use simple rules of thumb without knowing more details about the workload, but that's unavoidable - if the inputs are unknown, it's impossible to provide correct outputs. Fortunately, it's just a lot of sequential(ized) IO writes, and can be sized relatively easily once the requirements are known.</p>

<p>The NetApp solution guide for Kafka sizing has a detailed list of inputs that need to be gathered, and can be viewed <a href="https://docs.netapp.com/us-en/netapp-solutions/data-analytics/confluent-kafka-sizing.html">here</a>.</p>

<p>Kafka prototyping can be done on any storage (even in RAM disks) - we just need to find capacity and performance requirements for a small subset of data, and feed them to Kafka and E-Series sizing tools to verify assumptions and come up with appropriate sizing.</p>

<p>When sizing for capacity, performance, and choosing RAID/DDP configuration, I suggest to consider our next expansion step (say, add 3 more brokers). If Kafka Tier 1 storage will be busy and is likely to grow a lot, then EF600 and RAID10 may be more appropriate than EF300 and DDP, for example.</p>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/e-series" class="page__taxonomy-item p-category" rel="tag">e-series</a><span class="sep">, </span>
    
      <a href="/tags/eseries" class="page__taxonomy-item p-category" rel="tag">eseries</a><span class="sep">, </span>
    
      <a href="/tags/kafka" class="page__taxonomy-item p-category" rel="tag">kafka</a><span class="sep">, </span>
    
      <a href="/tags/netapp" class="page__taxonomy-item p-category" rel="tag">netapp</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/analytics" class="page__taxonomy-item p-category" rel="tag">analytics</a><span class="sep">, </span>
    
      <a href="/categories/storage" class="page__taxonomy-item p-category" rel="tag">storage</a>
    
    </span>
  </p>


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time class="dt-published" datetime="2022-06-28T00:00:00+08:00">2022-06-28 00:00</time></p>

      </footer>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://x.com/intent/tweet?via=scaleoutSean&text=E-Series+as+Tier+One+for+multi-tiered+Kafka+clusters%20https%3A%2F%2Fscaleoutsean.github.io%2F2022%2F06%2F28%2Fkafka-eseries-object-storage.html" class="btn btn--x" aria-label="Share on X" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on X">
    <i class="fab fa-fw fa-x-twitter" aria-hidden="true"></i><span> X</span>
  </a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://scaleoutsean.github.io/2022/06/28/kafka-eseries-object-storage.html" class="btn btn--linkedin" aria-label="Share on LinkedIn" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn">
    <i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span>
  </a>

</section>


      
  <nav class="pagination">
    
      <a href="/2022/06/22/e-series-hdfs.html" class="pagination--pager" title="Apache Hadoop 3 with NetApp E-Series">Previous</a>
    
    
      <a href="/2022/07/05/kafka-solidfire-efficiency.html" class="pagination--pager" title="Storage efficiency with Kafka 3.2 and NetApp SolidFire 12">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
    
<div class="page__related">
  
  <h2 class="page__related-title">You may also enjoy</h2>
  <div class="grid__wrapper">
    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/2025/10/15/mcp-tapout.html" rel="permalink">TAPOUT - MCP server for ONTAP to E-Series migration
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2025-10-15T00:00:00+08:00">2025-10-15 00:00</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          4 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Migrate to NetApp E-Series arrays with MCP
</p>
  </article>
</div>

    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/2025/10/10/post-quantum-crypto-proxy-for-solidfire-eseries-api.html" rel="permalink">Post-Quantum API proxy for E-Series and SolidFire
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2025-10-10T00:00:00+08:00">2025-10-10 00:00</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          9 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Enterprise-grade PQC TLS encryption for E-Series and SolidFire API endpoints
</p>
  </article>
</div>

    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/2025/10/09/storagegrid-s3-cache-branch-buckets.html" rel="permalink">StorageGRID Branch Buckets and Read Cache
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2025-10-09T00:00:00+08:00">2025-10-09 00:00</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          9 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Some ways to take better advantage of the new features in StorageGRID 12
</p>
  </article>
</div>

    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/2025/10/08/epa-3.5.1.html" rel="permalink">E-Series Performance Analyzer v3.5.1
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2025-10-08T00:00:00+08:00">2025-10-08 00:00</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          1 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">E-Series Performance Analyzer v3.5.1 has been released with minor improvements
</p>
  </article>
</div>

    
  </div>
</div>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        

<div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
          <li><a href="https://twitter.com/scaleoutsean" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/scaleoutsean" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>


<div class="page__footer-copyright">&copy; 2020-2025 scaleoutSean.github.io | <a href="https://scaleoutsean.github.io">Acting Technologist</a> | Terms: <a href="https://creativecommons.org/licenses/by/4.0/deed.en" target="_blank">CC BY 4.0</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>






  </body>
</html>
