<!doctype html>
<html lang="en">
  <head>
        <meta charset="UTF-8">
    <meta http-equiv="content-language" content="en">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="googlebot" content="noarchive">
    <meta name="googlebot" content="max-snippet:60">
    <meta name="googlebot" content="max-image-preview:small">
    <meta name="bingbot" content="noarchive">
    <meta name="bingbot" content="max-snippet:60">
    <meta name="bingbot" content="max-image-preview:small">
    <meta name="robots" content="noarchive">
    <meta name="robots" content="max-snippet:60">
    <meta name="google-site-verification" content="F6q7vIwQ2G0j8tk-KL9rAXOMLXMDMMUkEz4fRs1Nnnc" />
    <title>
        
            StorageGRID S3 in Public Cloud workflows without Data-at-Rest | Acting Technologist
      
    </title>
    <meta name="description" content="
     Processing StorageGRID data in Public Cloud without Data-at-Rest
     ">

    <!-- LINK TO ATOM FEED FOR SEO -->
    <link rel="alternate" type="application/atom+xml" href="https://scaleoutsean.github.io/feed.xml" />

    <!-- FAVICON -->
    <link rel="icon" href="/assets/favicon.ico" type="image/x-icon">
    <link rel="shortcut icon" href="/assets/favicon.ico" type="image/x-icon">

    <!-- CSS -->
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="stylesheet" href="/assets/css/syntax.css">

    <!-- Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css">

    <link href="/_pagefind/pagefind-ui.css" rel="stylesheet">
    <script src="/_pagefind/pagefind-ui.js" type="text/javascript"></script>
    <div id="search"></div>
    <script src="https://cdn.counter.dev/script.js" data-id="83dc700a-b821-4e57-9425-02b8336a9456" data-utcoffset="0"></script>
    <script>
        window.addEventListener('DOMContentLoaded', (event) => {
            new PagefindUI({ element: "#search" });
        });
    </script>

    <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>StorageGRID S3 in Public Cloud workflows without Data-at-Rest | Acting Technologist</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="StorageGRID S3 in Public Cloud workflows without Data-at-Rest" />
<meta name="author" content="scaleoutSean" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Processing StorageGRID data in Public Cloud without Data-at-Rest" />
<meta property="og:description" content="Processing StorageGRID data in Public Cloud without Data-at-Rest" />
<link rel="canonical" href="https://scaleoutsean.github.io/2022/01/28/storagegrid-hybrid-cloud-processing-without-data-at-rest.html" />
<meta property="og:url" content="https://scaleoutsean.github.io/2022/01/28/storagegrid-hybrid-cloud-processing-without-data-at-rest.html" />
<meta property="og:site_name" content="Acting Technologist" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-01-28T00:00:00+08:00" />
<script type="application/ld+json">
{"headline":"StorageGRID S3 in Public Cloud workflows without Data-at-Rest","dateModified":"2022-01-28T00:00:00+08:00","datePublished":"2022-01-28T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://scaleoutsean.github.io/2022/01/28/storagegrid-hybrid-cloud-processing-without-data-at-rest.html"},"author":{"@type":"Person","name":"scaleoutSean"},"@type":"BlogPosting","url":"https://scaleoutsean.github.io/2022/01/28/storagegrid-hybrid-cloud-processing-without-data-at-rest.html","description":"Processing StorageGRID data in Public Cloud without Data-at-Rest","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  </head>

<body>

  <div class="container">
    <header id="header">
	<div id = site_title>
		<a href="https://scaleoutsean.github.io/">
			<h2 style="font-size:1.8em;">Acting Technologist</h2>
		</a>
		
		<h3>
			civilizations are created by individuals
		</h3>
		
	</div>

	<div id="subheader">
		
		<nav class="pages">
<a href="/about.html">About</a>

<a href="/archive.html">Archive</a>

<a href="/categories/">Categories</a>

<a href="/projects.html">Projects</a>
</nav>
		
		
		<nav class="social">
			
  
    <a href="https://www.github.com/scaleoutsean" target="_blank" id="github"><i class="fab fa-github" aria-hidden="true"></i></a>
  
   

  
    <a href="https://twitter.com/scaleoutsean" target="_blank" id="twitter"><i class="fab fa-twitter" aria-hidden="true"></i></a>
  
   

  
  
    <a href="/feed.xml" target="_blank" id="rss"><i class="fas fa-rss" aria-hidden="true"></i></a>
   

		</nav>
		
	</div>
</header>


    <div class="post-container">
      <article id = "post">
        <h1 id = "post-title">StorageGRID S3 in Public Cloud workflows without Data-at-Rest</h1>

  
  <div class = "post-info">
    <span>
        <i class="far fa-calendar" aria-hidden="true"></i> <span>28 Jan 2022</span> - <i class="far fa-clock"></i> 


  
  
    9 minute read
  

    </span>
  </div>
  
        <p>This week I heard of a StorageGRID customer who had a query about data processing in the cloud. Probably instinctively, their first idea was to copy data to the cloud.</p>

<p>Sometimes that’s okay, another time it’s unavoidable, etc. But in this case it was completely unnecessary (“Because that’s how we’ve always done it”?).</p>

<p>In order to keep this post more generic, I won’t go into the details about the use case or file (object) format. Instead, let us assume the following requirements:</p>

<ul>
  <li>Data is created or collected on-prem</li>
  <li>Data needs to be processed in the cloud (insufficient resources on-prem, rush jobs, etc.)
    <ul>
      <li>File/object sizes aren’t huge - let’s say 1GB comes in, 1GB goes out (although output is usually, but not always, smaller than the input)</li>
      <li>Each compute job is self-contained</li>
      <li>There’s no merit or need - technical or business - to storing input data in the Public Cloud</li>
    </ul>
  </li>
  <li>Result is copied back to on-premies (&lt; 1GB file/object)</li>
</ul>

<p>Now, rather than focus on just this scenario, let’s generalize and say different requirements would result in different approaches.</p>

<p>This is a random and incomplete collection of approaches, products and services that are known to work or should work with StorageGRID S3.</p>

<!-- TOC -->

<ul>
  <li><a href="#varnish-http-cache">Varnish HTTP Cache</a></li>
  <li><a href="#ram-disk">RAM disk</a></li>
  <li><a href="#alluxio">Alluxio</a></li>
  <li><a href="#netapp-flexcache">NetApp FlexCache</a></li>
  <li><a href="#cloud-sync">Cloud Sync</a></li>
  <li><a href="#related-tools-and-services">Related tools and services</a>
    <ul>
      <li><a href="#spot-ocean-for-apache-spark">Spot Ocean for Apache Spark</a></li>
      <li><a href="#amazon-sns">Amazon SNS</a></li>
      <li><a href="#netapp-dataops-toolkit">NetApp DataOps Toolkit</a></li>
    </ul>
  </li>
  <li><a href="#conclusion">Conclusion</a></li>
</ul>

<!-- /TOC -->

<h2 id="varnish-http-cache">Varnish HTTP Cache</h2>

<p>If you can’t or prefer not to have <a href="https://en.wikipedia.org/wiki/Data_at_rest">Data-at-Rest</a> in the public cloud, you can keep it in memory and never save it in the first place.</p>

<p><a href="http://varnish-cache.org/">Varnish HTTP Cache</a> can act as your cloud-based read cache for StorageGRID buckets located on-premises.</p>

<p>In the case described above, we wouldn’t need to read an object multiple times, but we can still take advantage of Varnish. Before we kick off our job, we simply read the file first, and seconds or minutes later, kick off our compute job. By then the object should be cached.</p>

<p>Job result can be PUT directly to StorageGRID - there’s no need to PUT it through Varnish.</p>

<p>There’s a Technical Report on StorageGRID with Varnish on the NetApp Web site - find it with a search engine.</p>

<h2 id="ram-disk">RAM disk</h2>

<p>This is another way to avoid Data-at-Rest in the cloud.</p>

<p><a href="https://www.kernel.org/doc/html/latest/filesystems/tmpfs.html"><code class="language-plaintext highlighter-rouge">tempfs</code></a> lets you create a RAM based disk. There are similar solutions for Windows.</p>

<p>As our input is around 1GB, all we need to do is create a VM with slightly more RAM, then create a RAM disk and do our processing there. We can write job result to RAM disk and upload in background while the next input file is being downloaded.</p>

<p>This screenshot shows how a data processing job with data in RAM didn’t use any disk IO:</p>

<p><img src="/assets/images/storagegrid-hybrid-cloud-tempfs-processing.png" alt="Processing without disk IO with tempfs" /></p>

<p>This approach can be used in combination with Varnish. (Varnish caches data in RAM so we wouldn’t configure it to use a RAM disk. Instead, we could have worker VMs or containers copy cached files from Varnish server to their local RAM disk so that the Varnish can expire those objects ASAP - if we need to read them just once there’s no need to bloat Varnish cache.)</p>

<p>What if a VM with RAM disk or Varnish crashes? The same thing that you do if a VM <em>without</em> RAM disk or Varnish crashes - you rerun the job. The only extra cost is you need to re-download that file (which costs next to nothing, because to cloud-based clients it’s just a bit of extra ingress traffic).</p>

<p>Software-wise you don’t need anything else but Linux and one of the mainstream S3 clients.</p>

<h2 id="alluxio">Alluxio</h2>

<p>This is like Varnish, but much more suitable for Big Data applications. It can work in a scale-out-manner and supports Hadoop, HBase, Presto, Spark.</p>

<p>If your application is one of these (but also other, generic applications), you could use S3A with StorageGRID, but Alluxio is very feature-rich and will likely do a much better job. For example, the scale-out feature means you could process a TB-sized working set in the RAM of four VMs with 256GiB RAM.</p>

<p>If you’d like to find out more, I wrote several posts about <a href="https://scaleoutsean.github.io/2021/11/12/alluxio-storagegrid-s3.html">Alluxio</a>. Find the other two in Archive.</p>

<p>Alluxio has a Community Edition which you can try out for free.</p>

<h2 id="netapp-flexcache">NetApp FlexCache</h2>

<p>I wrote about FlexCache (including what it’s good for) in one of the Alluxio posts, so you can find that content there or read the ONTAP docs.</p>

<p>Long story short, it’s an excellent NFS and SMB caching feature in ONTAP which all cloud-based ONTAP services support, but StorageGRID S3 isn’t a supported back-end (where data is stored) - FlexCache currently supports ONTAP back-ends (NFS, SMB) so it doesn’t apply in this situation.</p>

<h2 id="cloud-sync">Cloud Sync</h2>

<p>Cloud Sync is a data replication service by NetApp. I recently <a href="https://scaleoutsean.github.io/2022/01/17/using-netapp-cloudsync-api.html">blogged about it</a>.</p>

<p>Now you may be wondering why I mention Cloud Sync if I don’t intend to copy data (i.e. no Data-at-Rest in the cloud). Two reasons:</p>

<ul>
  <li>you may want to copy it anyway</li>
  <li>if you realize you don’t need to copy it, you still can use Cloud Sync - just copy data to a <code class="language-plaintext highlighter-rouge">tempfs</code> disk exported via generic NFS server which will make data disappear on VM shutdown</li>
</ul>

<p>Admittedly it’d be a hassle to set a relationship for every job in every VM, but you could have a VM with RAM-based NFS server and keep this VM online at all times. Job runners would mount this share, process the file, PUT data to StorageGRID S3 and delete the input file to release space.</p>

<p>Cloud Sync has a free trial as well (see the blog posts linked above - there are links and info on how to subscribe and unsubscribe).</p>

<h2 id="related-tools-and-services">Related tools and services</h2>

<h3 id="spot-ocean-for-apache-spark">Spot Ocean for Apache Spark</h3>

<p>Spot has a product called <a href="https://spot.io/products/ocean-apache-spark/">Ocean for Apache Spark</a> (formerly Data Mechanics). You can use Alluxio in Ocean for Apache Spark.</p>

<p>Spot.io provides a free trial for all of its products.</p>

<h3 id="amazon-sns">Amazon SNS</h3>

<p>SNS doesn’t process data, but can be used to drive workflows even if you don’t use AWS for processing.</p>

<p>If StorageGRID grid administrator enables platform services, SNS Notifications may be used to notify us about changes that happen in the bucket.</p>

<p>If a new file lands into the StorageGRID bucket we use in hybrid cloud workflow, we can use SNS to examine object metadata and kick off our job: if the object meets certain criteria (specific metadata value, size, S3 key, etc.) we can initiate read via Varnish proxy or <code class="language-plaintext highlighter-rouge">wget</code>it to RAM disk.</p>

<h3 id="netapp-dataops-toolkit">NetApp DataOps Toolkit</h3>

<p>There are many popular data workflow tools (JFGI) that could be used, so I’ll instead mention a less obvious one: the NetApp DataOps Toolkit which I call DOT (not the official acronym).</p>

<blockquote>
  <p>The NetApp DataOps Toolkit is a Python library that makes it simple for developers, data scientists, DevOps engineers, and data engineers to perform various data management tasks, such as near-instantaneously provisioning, cloning, or snapshotting a data volume or JupyterLab workspace.</p>
</blockquote>

<p>As of recently, DOT supports <a href="https://github.com/NetApp/netapp-dataops-toolkit/blob/83c3b00cc406f964212750fef90dae18e0aeb0ee/netapp_dataops_traditional/netapp_dataops/traditional.py#L101">S3</a>, which lets us do this (and more):</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>netapp_dataops_cli.py pull-from-s3 object <span class="se">\</span>
  <span class="nt">--bucket</span><span class="o">=</span>cloudbatch <span class="se">\</span>
  <span class="nt">--key</span><span class="o">=</span>out/data1.dat <span class="se">\</span>
  <span class="nt">--file</span><span class="o">=</span>/ramdisk/in/data1.dat

Downloading object <span class="s1">'out/data1.dat'</span> from bucket <span class="s1">'cloudbatch'</span> 
and saving as <span class="s1">'/ramdisk/in/data1.dat'</span><span class="nb">.</span>
Download complete.
</code></pre></div></div>

<p>DOT pulls s3/cloudbatch/out/data1.dat and saves it to RAM disk (/ramdisk/in/data1.dat).</p>

<p>Then we can run our compute job and use DOT to PUT the result to s3/cloudbatch/in/data1_result.dat.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">cd</span> /ramdisk
<span class="nv">$ </span>netapp_dataops_cli.py push-to-s3 file <span class="se">\</span>
  <span class="nt">-b</span> cloudbatch <span class="se">\</span>
  <span class="nt">-f</span> <span class="k">in</span>/data1_result.dat

Uploading file <span class="s1">'in/data1_result.dat'</span> to bucket <span class="s1">'cloudbatch'</span> 
and applying key <span class="s1">'in/data1_result.dat'</span><span class="nb">.</span>
Upload complete.
</code></pre></div></div>

<p>Two noteworthy things:</p>

<ul>
  <li>pushing /ramdisk/<strong>in/data1_result.dat</strong> from <strong>/ramdisk</strong> directory PUTs it to s3/cloudbatch/<strong>in/data1_result.dat</strong>. That is, S3 KEY consists of relative path-to-file and full file name (including file extension, if any)</li>
  <li>the DOT integrates with ONTAP and can help you take snapshots, make clones, etc. but for S3 pull/push operations it works with common filesystems and on that topic while running DOT configuration wizard I provided made up details for ONTAP and that didn’t cause me any troubles when running the above, as I only used DOT’s S3-related functions</li>
</ul>

<p>To some of you two RAM disks mounted at /in and /out or different subdirectory names may be more intuitive - feel free to use whatever works for you (/raw and /results or /job_${id}, perhaps).</p>

<p>In the case you are wondering why use the DOT: there may be situations where input files need to be picked in an interactive, on-demand manner (as Data Scientists might do) so not having to leave one’s JupyterLab environment may be helpful.</p>

<p>And we can also use it from the CLI as I did above - without writing our own S3 CLI wrapper. Why would we want to do that? In certain cases - depending on what’s going on on-premises - DOT can help you with other routine but time-consuming steps that may need to happen before that S3 object appears and completely eliminate the need to code the rest of the workflow (if you don’t have existing workflow that can be expanded).</p>

<p>Let’s say your inputs are generated in an on-prem Kubernetes cluster and saved to an ONTAP NFS share in a closed LAN environment. Your workflow might look like this:</p>

<ul>
  <li>ONTAP NFS RWM PVC to StorageGRID’s “cloudbatch” bucket (on LAN)</li>
  <li>SG to Public Cloud (pull from VM/container)</li>
  <li>Public Cloud VM to SG (push from VM/container)</li>
  <li>SG to another ONTAP NFS share for safekeeping (on LAN)</li>
</ul>

<p>You’d need add just one line (to run your compute job) - everything else would be DOT. Since we already have several steps that involve StorageGRID and ONTAP, it can be easier to use DOT for the S3 push/pull steps than - for an example - write this entire workflow in Ansible.</p>

<p>The <a href="https://github.com/NetApp/netapp-dataops-toolkit">DOT</a> is free.</p>

<h2 id="conclusion">Conclusion</h2>

<p>NetApp is the leader in shared storage solutions for the Public Cloud, but that doesn’t mean you should save everything to disk. Once you save that file, you may need to audit access to it, back it up, snapshot it, tier its cold blocks to an object store, etc. If there’s no advantage to it, don’t do it!</p>

<p>There are many ways to avoid Data-at-Rest in the Public Cloud and for suitable use cases there are no trade-offs.</p>

<p>In fact in many such cases Data-at-Rest avoidance is convenient, performs faster, requires less administration, and has other advantages.</p>

      </article>
    </div>
    
      <div class="categories">
    <span><p>Categories:</p>
    
    
      <a href="
      /categories/#storage">storage</a>
      &nbsp; 
    
      <a href="
      /categories/#automation">automation</a>
       
    
  </span>
</div>
    

    
      <div class="related" data-pagefind-ignore>

    <h4>Possibly related - use live search at the top to find other content</h4>
    
    
    
    
    
    
    
        
        
    
        
    
        
          <div class = "related-posts">
          <h5><a href="/2024/03/21/netapp-trident-v2402-arm64.html">• Quickly install NetApp Trident v24.02 on ARM64 Kubernetes</a></h5>
          </div>
          
          
        
    
      
    
        
        
    
        
    
        
          <div class = "related-posts">
          <h5><a href="/2024/03/18/storagegrid-storage-node-filesystem-block-size.html">• Filesystem block size used by NetApp StorageGRID</a></h5>
          </div>
          
          
        
    
      
    
        
        
    
        
    
        
          <div class = "related-posts">
          <h5><a href="/2024/03/18/storagegrid-nlb-http-logs.html">• Analyze HTTP logs from NetApp StorageGRID gateway nodes</a></h5>
          </div>
          
          
        
    
      
    
        
        
    
        
    
        
          <div class = "related-posts">
          <h5><a href="/2024/03/17/monitoring-notifications-eseries-santricity-media-scan-progress.html">• Monitor progress and notify of E-Series media scan events</a></h5>
          </div>
          
          
        
    
      
    
        
        
    
        
    
        
          <div class = "related-posts">
          <h5><a href="/2024/02/29/ubuntu-2404-lts-with-netapp-solidfire.html">• Ubuntu 24.04 LTS with NetApp SolidFire</a></h5>
          </div>
          
          
            
    
    </div>

    

    
  </div><footer class= "footer">
    <p>2024-03-21 16:21 </p>
    <p>Copyright © 2024 scaleoutSean. Content is released under the CC BY license. Design: Alessio Franceschi</p>
</footer>

</body>
</html>
