<!doctype html>
<html lang="en">
  <head>
        <meta charset="UTF-8">
    <meta http-equiv="content-language" content="en">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="googlebot" content="noarchive">
    <meta name="googlebot" content="max-snippet:60">
    <meta name="googlebot" content="max-image-preview:small">
    <meta name="bingbot" content="noarchive">
    <meta name="bingbot" content="max-snippet:60">
    <meta name="bingbot" content="max-image-preview:small">
    <meta name="robots" content="noarchive">
    <meta name="robots" content="max-snippet:60">
    <meta name="google-site-verification" content="F6q7vIwQ2G0j8tk-KL9rAXOMLXMDMMUkEz4fRs1Nnnc" />
    <title>
        
            Milvus with SolidFire and E-Series | Acting Technologist
      
    </title>
    <meta name="description" content="
     Observations from using Milvus with NetApp SolidFire and E-Series
     ">

    <!-- LINK TO ATOM FEED FOR SEO -->
    <link rel="alternate" type="application/atom+xml" href="https://scaleoutsean.github.io/feed.xml" />

    <!-- FAVICON -->
    <link rel="icon" href="/assets/favicon.ico" type="image/x-icon">
    <link rel="shortcut icon" href="/assets/favicon.ico" type="image/x-icon">

    <!-- CSS -->
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="stylesheet" href="/assets/css/syntax.css">

    <!-- Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css">

    <link href="/_pagefind/pagefind-ui.css" rel="stylesheet">
    <script src="/_pagefind/pagefind-ui.js" type="text/javascript"></script>
    <div id="search"></div>
    <script src="https://cdn.counter.dev/script.js" data-id="83dc700a-b821-4e57-9425-02b8336a9456" data-utcoffset="0"></script>
    <script>
        window.addEventListener('DOMContentLoaded', (event) => {
            new PagefindUI({ element: "#search" });
        });
    </script>

    <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Milvus with SolidFire and E-Series | Acting Technologist</title>
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="Milvus with SolidFire and E-Series" />
<meta name="author" content="scaleoutSean" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Observations from using Milvus with NetApp SolidFire and E-Series" />
<meta property="og:description" content="Observations from using Milvus with NetApp SolidFire and E-Series" />
<link rel="canonical" href="https://scaleoutsean.github.io/2022/07/07/milvus-with-solidfire-e-series.html" />
<meta property="og:url" content="https://scaleoutsean.github.io/2022/07/07/milvus-with-solidfire-e-series.html" />
<meta property="og:site_name" content="Acting Technologist" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-07-07T00:00:00+08:00" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://scaleoutsean.github.io/2022/07/07/milvus-with-solidfire-e-series.html"},"author":{"@type":"Person","name":"scaleoutSean"},"description":"Observations from using Milvus with NetApp SolidFire and E-Series","@type":"BlogPosting","url":"https://scaleoutsean.github.io/2022/07/07/milvus-with-solidfire-e-series.html","headline":"Milvus with SolidFire and E-Series","dateModified":"2022-07-07T00:00:00+08:00","datePublished":"2022-07-07T00:00:00+08:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  </head>

<body>

  <div class="container">
    <header id="header">
	<div id = site_title>
		<a href="https://scaleoutsean.github.io/">
			<h2 style="font-size:1.8em;">Acting Technologist</h2>
		</a>
		
		<h3>
			civilizations are created by individuals
		</h3>
		
	</div>

	<div id="subheader">
		
		<nav class="pages">
<a href="/about.html">About</a>

<a href="/archive.html">Archive</a>

<a href="/categories/">Categories</a>

<a href="/projects.html">Projects</a>
</nav>
		
		
		<nav class="social">
			
  
    <a href="https://www.github.com/scaleoutsean" target="_blank" id="github"><i class="fab fa-github" aria-hidden="true"></i></a>
  
   

  
    <a href="https://twitter.com/scaleoutsean" target="_blank" id="twitter"><i class="fab fa-twitter" aria-hidden="true"></i></a>
  
   

  
  
    <a href="/feed.xml" target="_blank" id="rss"><i class="fas fa-rss" aria-hidden="true"></i></a>
   

		</nav>
		
	</div>
</header>


    <div class="post-container">
      <article id = "post">
        <h1 id = "post-title">Milvus with SolidFire and E-Series</h1>

  
  <div class = "post-info">
    <span>
        <i class="far fa-calendar" aria-hidden="true"></i> <span>07 Jul 2022</span> - <i class="far fa-clock"></i> 


  
  
    7 minute read
  

    </span>
  </div>
  
        <ul>
  <li><a href="#wtf-is-milvus">WTF is Milvus</a></li>
  <li><a href="#storage-related-stuff">Storage-related stuff</a>
    <ul>
      <li><a href="#meta-storage-etcd">Meta storage (etcd)</a></li>
      <li><a href="#logs-and-queues">Logs and queues</a></li>
      <li><a href="#object-store">Object store</a></li>
      <li><a href="#storage-efficiency">Storage efficiency</a></li>
      <li><a href="#high-availability-of-block-and-s3-storage-services">High availability of block and S3 storage services</a></li>
    </ul>
  </li>
  <li><a href="#aggregate-performance-on-e-series">Aggregate performance on E-Series</a></li>
</ul>

<h2 id="wtf-is-milvus">WTF is Milvus</h2>

<p>Milvus is a vector database built for scalable similarity search.</p>

<h2 id="storage-related-stuff">Storage-related stuff</h2>

<p>To get Milvus up and running I first RTFM. One of deployment options available that fit my existing environment* was Milvus Standalone - local Milvus that can be started with Docker Compose. (* I officially don’t work this week, so I didn’t want to go out of my way to try it out. I had three SolidFire volumes mounted from my recent <a href="/2022/07/05/kafka-solidfire-efficiency.html">Kafka efficiency testing</a>, so I used those). The volumes:</p>

<ul>
  <li>etcd - as the name suggests, Milvus Standalone uses singleton etcd instance for cluster metadata</li>
  <li>standalone - location for local Milvus data when Milvus is deployed in stand-alone mode</li>
  <li>object store - volume for S3, currently must be Minio-based, where Milvus moves sealed segments when it’s done indexing them</li>
</ul>

<p>Normally things are more complicated - not all-in-one (Standalone), that is.</p>

<p><img src="/assets/images/milvus-bench-01-services-storage.png" alt="Volume performance and IO request sizes" /></p>

<p>Source: Milvus v2.0 (<a href="https://milvus.io/docs/">documentation</a>)</p>

<p>I don’t have the resources to do this easily right now, so for time being I’ll stick with Milvus Standalone. Let’s see about those three volumes used by docker compose file for Milvus Standalone.</p>

<h3 id="meta-storage-etcd">Meta storage (etcd)</h3>

<p>etcd I/O is small and the workload not a novelty since we know it from Kubernetes.</p>

<p>For that we’d just provision a volume (or volumes, for larger clusters) on SSD storage. SolidFire is all-flash, so we’d just set Min IOPS on each such volume to say 5,000 IOPS. E-Series has no QoS settings, so we’d simply create an SSD-backed volume for each instance of etcd.</p>

<h3 id="logs-and-queues">Logs and queues</h3>

<p>Milvus Standalone uses just one volume, which I think keeps just message logs on this persistent volume. This workload could be be similar to Kafka (in fact, Milvus supports Pulsar and Kafka for message storage, but Milvus Standalone uses RocksMQ).</p>

<p>For small-to-medium Milvus, SolidFire should be fine, but for large check out E-Series EF300 or EF600 - this is the same recipe that we would use for <a href="/2022/06/28/kafka-eseries-object-storage.html">S3-tiered Kafka</a>.</p>

<p>Capacity-wise I expect few tens of GB of log space should be enough for Milvus Standalone (even more, but in the case S3 goes down, some time to retry uploads should be allowed), but we need to remember that production clusters are different (there are more containers, some are not even stateful, and stateful volumes may need different sizes), so I’ll take another look when I build a larger Milvus cluster.</p>

<h3 id="object-store">Object store</h3>

<p>Object Store workload is 100% write when there’s no query/search workload, and because uploading data to S3 deals with entire segments, these are large (1MB+) writes. I’m not sure how reads work in terms of request sizes, but I expect smaller reads (index data) combined with full segment downloads, so large and medium read requests. It’d be wasteful to run this off SolidFire; it’s OK for up to perhaps 1 GB/s, but large Minio runs better on E-Series and NetApp StorageGRID does too (and for large Milvus clusters we’d use dedicated StorageGRID appliances <a href="/2022/06/28/kafka-eseries-object-storage.html">that we can also use for Kafka</a>).</p>

<p>My SolidFire “cluster” at home is a small VM, which means I couldn’t properly benchmark Milvus with it, but even this environment provided some insights regarding possible I/O patterns.</p>

<p>In a small “INSERT” test I did, the first volume (ID 613; etcd) was mostly write workload that consisted of small-request sizes, the second volume (ID 614; S3 service) had a similar pattern due to Milvus tiering data to it, while the third volume (ID 615) was was mostly large-size IO.</p>

<p><img src="/assets/images/milvus-bench-02-io-sizes.png" alt="Volume performance and IO request sizes" /></p>

<p>Workloads on S3 (ID 614) and Milvus Standalone (ID 615; chart below) were similar, which wasn’t unexpected because data first lands on Milvus data volume and after indexing, it’s moved to S3.</p>

<p><img src="/assets/images/milvus-bench-03-io-request-write-medium-size.png" alt="Milvus MinIO workload" /></p>

<p>As I said above, normally we wouldn’t use SolidFire for Minio back-end - we want less fancy storage for that - so the S3 workload would be the first to go (to E-Series or StorageGRID) if we wanted to deploy Milvus in production. As mentioned above, Milvus seems to currently only support Minio which will probably change in coming months (I don’t have any “inside info”, I only know what other enterprise who prototype stuff with Minio eventually do).</p>

<p>The rest would then be similar to other databases that write to S3 (when they cool data), and read from S3 (to download, decompress and search).</p>

<h3 id="storage-efficiency">Storage efficiency</h3>

<p>Milvus can be very storage-efficient, so don’t expect much in terms of savings from storage array compression and deduplication.</p>

<p>After using random data to populate Milvus Standalone, observed SolidFire efficiency was only 1.04x (4% savings) from deduplication and compression. This may be better with real-life data and given that Milvus doesn’t need a lot of capacity on local tier it’s not a big deal, but be cautious when counting on storage efficiencies if your available space is very tight (&lt; 1 TB). I may run additional tests with real-life data if need arises.</p>

<p>E-Series has no compression and deduplication so we don’t need to mind this section.</p>

<h3 id="high-availability-of-block-and-s3-storage-services">High availability of block and S3 storage services</h3>

<p>Production clusters would have multiple replicas for etcd, messaging, index, and data. We could place redundant copies on one E-Series array or SolidFire cluster (both have redundant components), but to get even better redundancy we’d deploy two or three storage back-ends across two or three sites. A lower cost version of this could probably use self-deployed Milvus in the public cloud (if the license allows it, I haven’t checked).</p>

<p>E-Series array capacity could be shared between Milvus and StorageGRID SDS, and located all on the same site, or one array per each site. I’d recommend this for medium sites with geo-cluster requirements. Milvus microservices and StorageGRID both rely on software-based replication so neither E-Series nor SolidFire replication would need to be used.</p>

<p>If dedicated StorageGRID appliances were used for S3 service (in addition to E-Series for block storage), we’d need at least three StorageGRID appliances (either per site, for highest availability, or all together (one appliance per site, with limited site redundancy at a lower cost)).</p>

<h2 id="aggregate-performance-on-e-series">Aggregate performance on E-Series</h2>

<p>Inserting 10 million records with RF1 (ni_per 50000, WAL enabled (local disk), and nlist 1024):</p>

<ul>
  <li>rps: 47029.11</li>
  <li>flush_time: 2.51</li>
  <li>index build time: 13.96 (runs 4, 5, 6)</li>
  <li>Test environment: single dual-CPU Xeon 6130 connected via iSER to external volume on EF all-flash array; etcd, Minio and Milvus WAL all on one 100GB RAID1 volume (backed by just 2 SSDs on E-Series)</li>
</ul>

<p>When inserting records (indexing: off, RF1), CPU utilization was very low, and disk I/O was not significant even with WAL and Minio included</p>

<p><img src="/assets/images/milvus-e-series-6130-ef570-insert.png" alt="Inserting records with indexing OFF" /></p>

<p>Three runs as viewed in E-Series SANtricity Web UI (IOs are slightly smoothed compared to dstat output that has 1s granularity):</p>

<p><img src="/assets/images/milvus-e-series-6130-ef570-insert-santricity.png" alt="SANtricity view of inserting records with indexing OFF" /></p>

<p>Same run but with indexing on (index type ivf_sq8) looks a bit different, with indexing no longer deferred there’s a lot more CPU utilization.</p>

<p><img src="/assets/images/milvus-e-series-6130-ef570-insert-index.png" alt="Inserting records with indexing ON" /></p>

<p>Three runs of each type were executed; in hindsight I wonder if the 3rd test was with indexing on (reads were negligible), but I can’t confirm it.</p>

<p><img src="/assets/images/milvus-e-series-6130-ef570-insert-index-santricity.png" alt="SANtricity view of inserting records with indexing OFF (runs 1-3) and ON" /></p>

<p>What we can see from these examples is that inserting from a single node (RF1) didn’t cross 500 MB/s and that’s all-inclusive, with Minio, etcd and WAL all on one device. Although the server was connected to E-Series with iSER, it appears even 25Gbps iSCSI should work fine (or FC, NVMe/FC, etc.).</p>

<p>Even with RF2 the smaller of two current all-flash models (EF300) could likely host half a dozen Milvus nodes, especially if S3 was hosted externally and if workload wasn’t 100% insert (e.g. with 50% insert and 50% search the number of Milvus nodes could probably be doubled or tripled).</p>

      </article>
    </div>
    
      <div class="categories">
    <span><p>Categories:</p>
    
    
      <a href="
      /categories/#analytics">analytics</a>
      &nbsp; 
    
      <a href="
      /categories/#storage">storage</a>
       
    
  </span>
</div>
    

  
    <div>
      <h3>Related Posts</h3>
      <ul>
      
        <li><a href="/2023/07/20/storagegrid-and-elaticsearches.html">StorageGRID and Elasticsearches</a></li>
      
        <li><a href="/2023/08/01/fscrawler-filesystem-analytics-elasticsearch.html">FSCrawler for basic filesystem analytics in Elasticsearch</a></li>
      
        <li><a href="/2024/05/28/netapp-solidfire-volume-qos-histograms.html">Make use of storage QoS histograms on NetApp SolidFire</a></li>
      
        <li><a href="/2024/06/19/trident-policy-sucker-for-solidfire-backends.html">Trident QoS policy sucker for SolidFire backend</a></li>
      
        <li><a href="/2024/06/16/snapshot-schedules-in-solidfire.html">Snapshots and snapshot schedules in NetApp SolidFire</a></li>
      
      </ul>
    </div>
  

    
  </div><footer class= "footer">
    <p>2025-06-08 02:53 </p>
    <p>Copyright © 2025 scaleoutSean. Content is released under the CC BY license. Design: Alessio Franceschi</p>
</footer>

</body>
</html>
