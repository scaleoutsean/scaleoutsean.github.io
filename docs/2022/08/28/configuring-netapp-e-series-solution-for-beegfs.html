<!doctype html>
<html lang="en">
  <head>
        <meta charset="UTF-8">
    <meta http-equiv="content-language" content="en">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="googlebot" content="noarchive">
    <meta name="googlebot" content="max-snippet:60">
    <meta name="googlebot" content="max-image-preview:small">
    <meta name="bingbot" content="noarchive">
    <meta name="bingbot" content="max-snippet:60">
    <meta name="bingbot" content="max-image-preview:small">
    <meta name="robots" content="noarchive">
    <meta name="robots" content="max-snippet:60">
    <meta name="google-site-verification" content="F6q7vIwQ2G0j8tk-KL9rAXOMLXMDMMUkEz4fRs1Nnnc" />
    <title>
        
            Configuration approaches to NetApp E-Series with ThinkParQ BeeGFS | Acting Technologist
      
    </title>
    <meta name="description" content="
     How to mix and match storage and servers
     ">

    <!-- LINK TO ATOM FEED FOR SEO -->
    <link rel="alternate" type="application/atom+xml" href="https://scaleoutsean.github.io/feed.xml" />

    <!-- FAVICON -->
    <link rel="icon" href="/assets/favicon.ico" type="image/x-icon">
    <link rel="shortcut icon" href="/assets/favicon.ico" type="image/x-icon">

    <!-- CSS -->
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="stylesheet" href="/assets/css/syntax.css">

    <!-- Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css">

    <link href="/_pagefind/pagefind-ui.css" rel="stylesheet">
    <script src="/_pagefind/pagefind-ui.js" type="text/javascript"></script>
    <div id="search"></div>
    <script src="https://cdn.counter.dev/script.js" data-id="83dc700a-b821-4e57-9425-02b8336a9456" data-utcoffset="0"></script>
    <script>
        window.addEventListener('DOMContentLoaded', (event) => {
            new PagefindUI({ element: "#search" });
        });
    </script>

    <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Configuration approaches to NetApp E-Series with ThinkParQ BeeGFS | Acting Technologist</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Configuration approaches to NetApp E-Series with ThinkParQ BeeGFS" />
<meta name="author" content="scaleoutSean" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="How to mix and match storage and servers" />
<meta property="og:description" content="How to mix and match storage and servers" />
<link rel="canonical" href="https://scaleoutsean.github.io/2022/08/28/configuring-netapp-e-series-solution-for-beegfs.html" />
<meta property="og:url" content="https://scaleoutsean.github.io/2022/08/28/configuring-netapp-e-series-solution-for-beegfs.html" />
<meta property="og:site_name" content="Acting Technologist" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-08-28T00:00:00+08:00" />
<script type="application/ld+json">
{"headline":"Configuration approaches to NetApp E-Series with ThinkParQ BeeGFS","dateModified":"2022-08-28T00:00:00+08:00","datePublished":"2022-08-28T00:00:00+08:00","url":"https://scaleoutsean.github.io/2022/08/28/configuring-netapp-e-series-solution-for-beegfs.html","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://scaleoutsean.github.io/2022/08/28/configuring-netapp-e-series-solution-for-beegfs.html"},"description":"How to mix and match storage and servers","author":{"@type":"Person","name":"scaleoutSean"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  </head>

<body>

  <div class="container">
    <header id="header">
	<div id = site_title>
		<a href="https://scaleoutsean.github.io/">
			<h2 style="font-size:1.8em;">Acting Technologist</h2>
		</a>
		
		<h3>
			civilizations are created by individuals
		</h3>
		
	</div>

	<div id="subheader">
		
		<nav class="pages">
<a href="/about.html">About</a>

<a href="/archive.html">Archive</a>

<a href="/categories/">Categories</a>

<a href="/projects.html">Projects</a>
</nav>
		
		
		<nav class="social">
			
  
    <a href="https://www.github.com/scaleoutsean" target="_blank" id="github"><i class="fab fa-github" aria-hidden="true"></i></a>
  
   

  
    <a href="https://twitter.com/scaleoutsean" target="_blank" id="twitter"><i class="fab fa-twitter" aria-hidden="true"></i></a>
  
   

  
  
    <a href="/feed.xml" target="_blank" id="rss"><i class="fas fa-rss" aria-hidden="true"></i></a>
   

		</nav>
		
	</div>
</header>


    <div class="post-container">
      <article id = "post">
        <h1 id = "post-title">Configuration approaches to NetApp E-Series with ThinkParQ BeeGFS</h1>

  
  <div class = "post-info">
    <span>
        <i class="far fa-calendar" aria-hidden="true"></i> <span>28 Aug 2022</span> - <i class="far fa-clock"></i> 


  
  
    6 minute read
  

    </span>
  </div>
  
        <p>It may not be easy to digest how BeeGFS with NetApp E-Series is usually configured.</p>

<p>One of the good reasons is that it can be configured in many different ways, but then people just ask for one or two suggestions to look at.</p>

<p>Another is that many BeeGFS clusters out there use data mirroring, which the NetApp E-Series solution for BeeGFS does not - instead, it uses HA server pairs with protected storage on E-Series arrays.</p>

<p>Let’s take a look at some examples.</p>

<h2 id="two-servers-one-disk-array">Two servers, one disk array</h2>

<p>Two servers with one E-Series disk array are used when performance and capacity requirements can be met with a single E-Series array.</p>

<p>In the case where the servers don’t need to max out E-Series array performance, using 1U, uniprocessor servers provides a way to save compute node cost.</p>

<p><img src="/assets/images/beegfs-layout-two-one.png" alt="BeeGFS HA pair with one E-Series array" /></p>

<p>Comments:</p>

<ul>
  <li>It is usually desirable to put data and metadata on separate disk groups/pools</li>
  <li>Create 1 or 2 volumes on each group/pool and present to BeeGFS server pair(s)
    <ul>
      <li>Use RAID 1 or 10 for metadata, usually no more than 2-3% of data capacity</li>
      <li>Use RAID 6 or DDP for data; on each group/pool create 1 or 2 volumes per BeeGFS server</li>
      <li>EF600 has 24 slots so one nice way to divide that for MD and Data is like this:
        <ul>
          <li>2 RAID 6 groups (8+2) = 20 disks; 1 volume per RAID group when two BeeGFS servers are used</li>
          <li>2 RAID 1 groups (1+1) = 4 disks; 1 volume per RAID group when two BeeGFS servers are used</li>
        </ul>
      </li>
      <li>It’s possible to create two metadata volumes on each RAID 1 group, but with two servers one volume per RAID/DDP is enough</li>
    </ul>
  </li>
  <li>If MD doesn’t need more than two disks worth of IOPS, we could also create one RAID 1 group with two volumes on it, but while that would save two disks (and slots in the controller chassis), there isn’t much we can do with 2 available slots <em>unless</em> we change RAID 6 (8+2) to a wider RAID 6 (10+2) or DDP (12/2). Would that make sense?
    <ul>
      <li>If you need more capacity to avoid buying another storage array and are happy with just two disks for MD, that can be a good idea</li>
      <li>8+2 is most performance-effective RAID 6 configuration for EF600, so if you need more capacity you could as well go with DDP (12/2 or even 12/1) if you’re not optimizing for highest data performance</li>
    </ul>
  </li>
</ul>

<h2 id="two-servers-two-disk-arrays">Two servers, two disk arrays</h2>

<p>Sometimes more performance or more capacity is needed. Because we have two E-Series arrays at our disposal, we would probably use 2U, two-way servers to be able to fully utilize their bandwidth.</p>

<p>This is how NetApp BeeGFS Building Block validated for NVIDIA SuperPOD looks like, so you may hear people refer to this approach as “BeeGFS building block”.</p>

<p><img src="/assets/images/beegfs-layout-two-two.png" alt="BeeGFS-E-Series Building Block" /></p>

<p>Comments:</p>

<ul>
  <li>Commonly you can’t go wrong with RAID 6 for data and RAID 1 for MD, but sometimes you may want to use RAID 10 for MD or DDP for data; usually that has to do with capacity or performance requirements, when a small deviation from RAID 6 (8+2) and RAID 1 can help you avoid purchasing another building block
    <ul>
      <li>Example: one RAID1 (1+1) group and one DDP (22/2) pool per each E-Series array. We should create two volumes per each group and pool to have each server active on 50% of the disks</li>
    </ul>
  </li>
</ul>

<h2 id="other-combinations">Other combinations</h2>

<p>By now it’s probably clear that we can pretty much do what we want.</p>

<ul>
  <li>Need 1 million metadata IOPS? Deploy a dedicated HA pair with EF600 for metadata-only workload and deploy separate data “pods” for data-only workload</li>
  <li>Need to maximize capacity and keep the cost-down? Expand EF600 with multiple NL-SAS expansion enclosures (up to 60 18TB disks each) and configure just two wide pools - DDP (30/2) - on each enclosure which takes only 4 disks of overhead per 60 disk enclosure (93.33% usable)</li>
  <li>Don’t want to use any SSDs for BeeGFS data disks? Configure a dedicated Metadata “pod” with EF600 (NVMe SSDs) and have Data pods use E5760 with 100% NL-SAS disks</li>
  <li>BeeGFS clusters with very static data could be built with only NL-SAS (for both data and metadata), but consider using SSD pools (DDP) to cost-down from the optimal RAID 1 while still retaining 100-200K IOPS for metadata before you even think about using NL-SAS for metadata. Especially with EF600 and EF300 where controller shelves can only host SSDs, controller shelf is a great place for all-NVMe metadata SSDs - even if SSDs are in a DDP pool made of 1.92TB SSDs, that’s still much than any RAID 10 configuration for metadata on NL-SAS disks.</li>
</ul>

<p><img src="/assets/images/beegfs-layout-detailed.png" alt="Custom BeeGFS with E-Series" /></p>

<p>Comments:</p>

<ul>
  <li>Because Metadata volumes are on a dedicated E-Series array, each array dedicated to “Data” has 24 disks slots which means we can either:
    <ul>
      <li>Use two DDP (12/2) pools per array, to make use of all disk slots in the controller shelf (or shelves), <strong>or</strong></li>
      <li>Use two RAID 6 (8+2) groups per array and leave 4 disk slots empty (more optimal performance, but we may need another array if more than 20 disks are required for Data capacity). And adding an extra disk array may also need two more servers, so sometimes a small sacrifice in performance can translate into significant savings</li>
    </ul>
  </li>
  <li>When possible (i.e. disk slots are available, etc.) and needed, it may be advantageous to use more smaller disks for metadata disk groups. By using RAID10 (4+4) made of 1.92TB disks it is possible to get up to 4 times more IOPS than with just two 7.68TB disks in RAID1. The capacity is the same and the cost of capacity is slightly higher, but the gain in performance is significant
    <ul>
      <li>BeeGFS clusters that will grow data will ultimately need more metadata capacity and performance, so we need to find optimal balance between using many small disks for IOPS and few large disks for capacity (like in the earlier examples, we don’t want to run out of empty slots and buy another array, but we also don’t want to use very large SSDs for metadata and have insufficient metadata performance)</li>
    </ul>
  </li>
</ul>

<h2 id="networking">Networking</h2>

<p>Although the diagrams are simple, it’s obvious that common approach is switchless - it involves directly attaching servers to E-Series - while server-to-server and client-to-server communications are supposed to use a high-performance, low-latency network such as Infiniband or Ethernet. The same switch pair could be used for server-to-storage communications, but many clusters don’t have it because there’s no gain in performance or even manageability, yet it adds to the cost.</p>

<p>If you need fewer servers and many arrays (example: four servers, 12 arrays), DAS won’t do it because there aren’t enough ports on current E-Series arrays to allow that. In that case a SAN is necessary and a pair of switches (IB or FC) can help. BeeGFS server pairs connected to switches can be connected to multiple E-Series arrays. To use the earlier example of four BeeGFS servers and 12 E-Series arrays: each server pair can be connected to six arrays).</p>

<h2 id="other-information">Other information</h2>

<p>If you plan to deploy BeeGFS with E-Series and get support from NetApp, you should use Ansible to configure E-Series and deploy BeeGFS.</p>

<p>In that case, read <a href="https://docs.netapp.com/us-en/beegfs/beegfs-deploy-overview.html#configuration-profiles-for-beegfs-building-blocks">the documentation</a> to make sure your configuration is supported or submit a feature request (or even better, pull request) for it on Github.</p>

      </article>
    </div>
    
      <div class="categories">
    <span><p>Categories:</p>
    
    
      <a href="
      /categories/#storage">storage</a>
       
    
  </span>
</div>
    

  
    <div>
      <h3>Related Posts</h3>
      <ul>
      
        <li><a href="/2024/04/23/econfig-update.html">EConfig v2</a></li>
      
        <li><a href="/2022/09/18/eseries-ddp-capacity-splitter.html">DDP capacity splitter for E-Series</a></li>
      
        <li><a href="/2022/09/04/brute-force-sizing-netapp-eseries.html">Brute force sizing NetApp E-Series</a></li>
      
        <li><a href="/2023/12/02/containerized-beegfs-with-netapp-eseries.html">Containerized BeeGFS with NetApp E-Series</a></li>
      
        <li><a href="/2022/10/20/beegfs-hsm-irods-robinhood.html">iRODS or Robinhood HSM with BeeGFS</a></li>
      
      </ul>
    </div>
  

    
  </div><footer class= "footer">
    <p>2024-07-03 17:52 </p>
    <p>Copyright © 2024 scaleoutSean. Content is released under the CC BY license. Design: Alessio Franceschi</p>
</footer>

</body>
</html>
