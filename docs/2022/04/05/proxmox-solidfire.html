<!doctype html>
<html lang="en">
  <head>
        <meta charset="UTF-8">
    <meta http-equiv="content-language" content="en">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="googlebot" content="noarchive">
    <meta name="googlebot" content="max-snippet:60">
    <meta name="googlebot" content="max-image-preview:small">
    <meta name="bingbot" content="noarchive">
    <meta name="bingbot" content="max-snippet:60">
    <meta name="bingbot" content="max-image-preview:small">
    <meta name="robots" content="noarchive">
    <meta name="robots" content="max-snippet:60">
    <meta name="google-site-verification" content="F6q7vIwQ2G0j8tk-KL9rAXOMLXMDMMUkEz4fRs1Nnnc" />
    <title>
        
            Consume SolidFire storage from Proxmox hosts | Acting Technologist
      
    </title>
    <meta name="description" content="
     Notes on connecting Proxmox 7.1 iSCSI client to SolidFire 12.3
     ">

    <!-- LINK TO ATOM FEED FOR SEO -->
    <link rel="alternate" type="application/atom+xml" href="https://scaleoutsean.github.io/feed.xml" />

    <!-- FAVICON -->
    <link rel="icon" href="/assets/favicon.ico" type="image/x-icon">
    <link rel="shortcut icon" href="/assets/favicon.ico" type="image/x-icon">

    <!-- CSS -->
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="stylesheet" href="/assets/css/syntax.css">

    <!-- Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css">

    <link href="/_pagefind/pagefind-ui.css" rel="stylesheet">
    <script src="/_pagefind/pagefind-ui.js" type="text/javascript"></script>
    <div id="search"></div>
    <script src="https://cdn.counter.dev/script.js" data-id="83dc700a-b821-4e57-9425-02b8336a9456" data-utcoffset="0"></script>
    <script>
        window.addEventListener('DOMContentLoaded', (event) => {
            new PagefindUI({ element: "#search" });
        });
    </script>

    <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Consume SolidFire storage from Proxmox hosts | Acting Technologist</title>
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="Consume SolidFire storage from Proxmox hosts" />
<meta name="author" content="scaleoutSean" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Notes on connecting Proxmox 7.1 iSCSI client to SolidFire 12.3" />
<meta property="og:description" content="Notes on connecting Proxmox 7.1 iSCSI client to SolidFire 12.3" />
<link rel="canonical" href="https://scaleoutsean.github.io/2022/04/05/proxmox-solidfire.html" />
<meta property="og:url" content="https://scaleoutsean.github.io/2022/04/05/proxmox-solidfire.html" />
<meta property="og:site_name" content="Acting Technologist" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-04-05T00:00:00+08:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"scaleoutSean"},"description":"Notes on connecting Proxmox 7.1 iSCSI client to SolidFire 12.3","@type":"BlogPosting","headline":"Consume SolidFire storage from Proxmox hosts","dateModified":"2022-04-05T00:00:00+08:00","datePublished":"2022-04-05T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://scaleoutsean.github.io/2022/04/05/proxmox-solidfire.html"},"url":"https://scaleoutsean.github.io/2022/04/05/proxmox-solidfire.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  </head>

<body>

  <div class="container">
    <header id="header">
	<div id = site_title>
		<a href="https://scaleoutsean.github.io/">
			<h2 style="font-size:1.8em;">Acting Technologist</h2>
		</a>
		
		<h3>
			civilizations are created by individuals
		</h3>
		
	</div>

	<div id="subheader">
		
		<nav class="pages">
<a href="/about.html">About</a>

<a href="/archive.html">Archive</a>

<a href="/categories/">Categories</a>

<a href="/projects.html">Projects</a>
</nav>
		
		
		<nav class="social">
			
  
    <a href="https://www.github.com/scaleoutsean" target="_blank" id="github"><i class="fab fa-github" aria-hidden="true"></i></a>
  
   

  
    <a href="https://twitter.com/scaleoutsean" target="_blank" id="twitter"><i class="fab fa-twitter" aria-hidden="true"></i></a>
  
   

  
  
    <a href="/feed.xml" target="_blank" id="rss"><i class="fas fa-rss" aria-hidden="true"></i></a>
   

		</nav>
		
	</div>
</header>


    <div class="post-container">
      <article id = "post">
        <h1 id = "post-title">Consume SolidFire storage from Proxmox hosts</h1>

  
  <div class = "post-info">
    <span>
        <i class="far fa-calendar" aria-hidden="true"></i> <span>05 Apr 2022</span> - <i class="far fa-clock"></i> 


  
  
    7 minute read
  

    </span>
  </div>
  
        <ul>
  <li><a href="#configure-proxmox-71-with-solidfire-123">Configure Proxmox 7.1 with SolidFire 12.3</a></li>
  <li><a href="#solidfire-demo-vm-on-proxmox">SolidFire Demo VM on Proxmox</a></li>
</ul>

<p>Some notes for folks who want to use Proxmox 7.1 with SolidFire 12…</p>

<p><img src="/assets/images/proxmox-solidfire-diagram.png" alt="Proxmox with SolidFire" /></p>

<h2 id="configure-proxmox-71-with-solidfire-123">Configure Proxmox 7.1 with SolidFire 12.3</h2>

<p>First, create a tenant account on SolidFire, such as proxmox1 (in the case you have multiple Proxmox hosts).</p>

<p>I’ve created a SoliFire VAG as well (below), because at first I thought to use Proxmox without CHAP, but I later changed my mind… So you don’t need a VAG (unless, perhaps, if you cluster multiple Proxmox hosts).</p>

<p><img src="/assets/images/proxmox-solidfire-account.png" alt="Create tenant account and VAG for Proxmox" /></p>

<p>On your Proxmox machine, install open-iscsi (not sure if it has to be enabled?) and configure iscsid.conf:</p>

<ul>
  <li>In <code class="language-plaintext highlighter-rouge">node.session.auth.chap_algs</code> use MD5 (as SolidFire v12.3 and lower versions do not support other algorithms)</li>
  <li>Provide tenant account credentials for CHAP user and password, in both discovery and access</li>
  <li>I changed iSCSI startup to automatic (iscsid.conf) and enabled/started open-iscsi service - I’m not sure if that’s necessary but it didn’t create problems</li>
</ul>

<p>You’ll probably want to use iSCSI on a dedicated network or VLAN. For my environment I configured a NIC on network used by SolidFire. Pay attention to the MTU setting.</p>

<p>I didn’t use jumbo frames on Proxmox, so I set client MTU to 1500 (SolidFire Demo VM I use uses MTU 9000 on iSCSI network, so ideally the both sides should use the same MTU). iSCSI VLAN 103 was native on the ESXi vSwitch (Proxmox was running nested).</p>

<p><img src="/assets/images/proxmox-solidfire-network.png" alt="Add Proxmox iSCSI interface(s)" /></p>

<p>You may add the SolidFire target via the Web UI or in the CLI.</p>

<p><img src="/assets/images/proxmox-solidfire-add-storage.png" alt="Add iSCSI storage to Proxmox" /></p>

<p>At first I used the Web UI, but later had to change the settings too many times (because I wasn’t <a href="https://pve.proxmox.com/wiki/Storage:_iSCSI">RTFM</a>). Storage configuration can be created and edited in the CLI as well.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># cat /etc/pve/storage.cfg</span>
<span class="nb">dir</span>: <span class="nb">local
	</span>path /var/lib/vz
	content iso,vztmpl,backup

lvmthin: local-lvm
	thinpool data
	vgname pve
	content images,rootdir

iscsi: DR
	portal 192.168.103.34
	target iqn.2010-01.com.solidfire:46z9.solidfire.279
	content none
</code></pre></div></div>

<p>Here:</p>

<ul>
  <li>Device Name: “DR”. SolidFire Cluster Name + Volume Name + Volume ID would have been better, so I later did this again - that’s why the screenshot looks a little different</li>
  <li>Portal IP: SolidFire cluster’s Storage Virtual IP (SVIP)</li>
  <li>SolidFire target is <code class="language-plaintext highlighter-rouge">iqn.2010-01.com.solidfire:46z9</code>
    <ul>
      <li>SolidFire volume created for this entry is called <code class="language-plaintext highlighter-rouge">solidfire</code>, and on SolidFire this Volume’s ID is 279</li>
    </ul>
  </li>
  <li>Content: I picked “none” here, to get a “raw” device which I can format with filesystem of my choosing</li>
</ul>

<p>Presumably you’ll have more than just one volume (which is why you shouldn’t name SolidFire iSCSI devices by cluster name alone). These iSCSI stanzas - and the entire process of provisioning, in fact - should be automated with Python or via the API (SolidFire &amp; Proxmox).</p>

<p>Rescan:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># pvesm scan iscsi 192.168.103.34:3260</span>
iqn.2010-01.com.solidfire:46z9.solidfire.279 192.168.103.34:3260
</code></pre></div></div>

<p>SolidFire disk “DR” (in screenshot it’s “DR-solidfire-279” - I took this again to make it better but in CLI examples below I continue using the original name before the screenshot was updated - <code class="language-plaintext highlighter-rouge">DR</code>) on Proxmox:</p>

<p><img src="/assets/images/proxmox-solidfire-storage.png" alt="DR disk on Proxmox" /></p>

<p>Find Proxmox Volume ID for this device:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># pvesm list DR</span>
Volid                                           Format  Type            Size VMID
DR:0.0.0.scsi-36f47acc10000000034367a3900000117 raw     images    2147483648
</code></pre></div></div>

<p>From the Proxmox Wiki:</p>

<blockquote>
  <p>So it is usually best to export one big LUN, and setup LVM on top of that LUN.</p>
</blockquote>

<p>Let’s create LVM group on that volume, as they suggest.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># pvesm add lvm sf01 --vgname sf279 --base "DR:0.0.0.scsi-36f47acc10000000034367a3900000117"</span>
  Physical volume <span class="s2">"/dev/disk/by-id/scsi-36f47acc10000000034367a3900000117"</span> successfully created.
  Volume group <span class="s2">"sf279"</span> successfully created
</code></pre></div></div>

<p>That’s it!</p>

<p><img src="/assets/images/proxmox-solidfire-lvm-on-solidfire-volume.png" alt="Proxmox LVM on SolidFire iSCSI device" /></p>

<p>This gives us the ability to take snapshots and add additional SolidFire volumes to this LVM pool, but if you do that remember to use the same QoS settings (or a QoS policy) on all volumes that are part of the same volume group.</p>

<p>Another thing to note is Proxmox API provides a way to set various workload-based bandwidth limits so in Proxmox you can set a lower limit than SolidFire gives you to restrict sequential bandwidth for certain operations (say, Proxmox-side cloning) and let the rest be used by other storage operations. Refer to their documentation for these details.</p>

<p>I haven’t tried to resize this test volume because generally speaking a better way to grow LVM is to add additional same-sized devices. SolidFire comes with deduplication, compression and thin provisioning, so there’s no harm in having “unused” space in volumes used by LVM as long as “discard” (rethinning) is in place.</p>

<p>SolidFire supports Group Snapshots so although you can’t take SolidFire snapshots from Proxmox, you can take LVM snapshots, and from time to time you may want to take a SolidFire group snapshot as well (just in case, as they say), which can be done with a SolidFire group snapshot schedule. We could consider the old snapshot trick that folks use with VMFS filesystems on ESXi:</p>

<ul>
  <li>Create a schedule that takes a daily LVM snapshot on Proxmox (e.g. 4:00pm)
    <ul>
      <li>When taking daily LVM snapshot on Proxmox, delete any old Proxmox daily snapshot(s)</li>
    </ul>
  </li>
  <li>Create a Solidfire schedule for daily group snapshot of all LVM volumes (e.g. 4:05pm)</li>
  <li>This gives you just one daily LVM snapshot from Proxmox side, while SolidFire snapshots contain LVM with one Proxmox snapshot which can be filesystem- or application-consistent (depending on how you took it). You’d first restore the SolidFire snapshot or clone it, then import (LVM) disks and reconstitute LVM if required, and finally restore the LVM snapshot taken from Proxmox, if you thought it provided better consistency than crash-consistent SolidFire snapshots</li>
</ul>

<p>Proxmox has its backup mechanisms which you can use, but you can also use some of the Proxmox features (such as “suspend”) and then take a snapshot on SolidFire.</p>

<p>Guest OS that support TRIM could use “discard” mount option for SolidFire-backed volumes, so that they can be rethinned.</p>

<p>As far as storage efficiency is concerned, SolidFire compresses and deduplicates very well, which is why good old LVM with ext4 or XFS works very well here. If we used ZFS and enabled compression, the likely result would be a lower efficiency because cross-volume savings would evaporate, whereas SolidFire gives you global deduplication across the entire cluster. Maybe there are some workloads where it makes sense to use btrfs or ZFS, but I’d go with LVM as my default and choose others only when it clearly makes sense.</p>

<p>There’s certainly more to say about how Proxmox and SolidFire could be used together, but this will do for now.</p>

<h2 id="solidfire-demo-vm-on-proxmox">SolidFire Demo VM on Proxmox</h2>

<p>What follows is my usual experiment - try to run SolidFire Demo VM and see if I can get it to work on different virtualization platforms. Officially only vSphere is supported, but I’ve managed to get the Demo VM 12.3 to on Virtualbox, so I thought to give it a try.</p>

<p>I briefly tried to run the SolidFire Demo VM (OVA for vSphere 7).</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># tar xvf solidfire-demo-node-12.3.0958.ova</span>
<span class="c"># qm importovf 300 solidfire-elementos-demo-magnesium-patch3-12.3.0.958.ova.ovf o2 --format qcow2</span>
</code></pre></div></div>

<p>This resulted in a promising-looking VM.</p>

<p><img src="/assets/images/proxmox-solidfire-demo-vm-import.png" alt="SolidFire Demo VM as Proxmox VM" /></p>

<p>But the result wasn’t so promising.</p>

<p><img src="/assets/images/proxmox-solidfire-demo-vm-start-fail.png" alt="SolidFire Demo VM fails to start" /></p>

<p>As you may have noticed, I had this Demo VM (which is big-ish) on NFSv3 (because I don’t have enough capacity to store it elsewhere). And that NFS “server” happens to be a tiny ARM device… So I can’t say if this is an import problem, or simply the fact that NFS server is too underpowered.</p>

<p>The VM is able to start, but not complete, boot process. Again, it could be the slow storage, but it could also be someting in post-initrd boot process.</p>

<p><img src="/assets/images/proxmox-solidfire-demo-vm-start.png" alt="SolidFire Demo VM boot screen" /></p>

<p>Possible workarounds include a change of disk controller. I’ve tried several from this list to same result.</p>

<p><img src="/assets/images/proxmox-solidfire-demo-vm-storage-controller-options.png" alt="Proxmox 7.1 SCSI controller choices" /></p>

<p>As you can see from <a href="https://scaleoutsean.github.io/2021/04/20/solidfire-12.3.html">my notes on SolidFire Demo VM with Virtualbox</a>, I managed to get it to work by detaching all of the disks from SCSI controller, adding a new SATA (AHCI, to be exact) controller, and finally attaching the disks to AHCI controller in the exact same order (although, apart from boot disk, the order maybe doesn’t matter).</p>

<p>To do the same on Proxmox I detached all SCSI disks so that they become Unused, then attached them as SATA devices. I had to hit ESC to get to boot menu where I picked the largest (60GB) disk which is boot disk, but just like before it timed out after a while.</p>

<p><img src="/assets/images/proxmox-solidfire-demo-vm-sata.png" alt="SolidFire Demo VM with SATA disks" /></p>

<p>Unfortunately this failed as well. I suppose the reason is the same why SolidFire Demo VM couldn’t work on Virtualbox 5 - no support for device paths (MD + Data disks) by-disk-uuid. I haven’t verified this, but I found a post on the Proxmox forum that says /dev/disk/by-uuid paths aren’t provided to guests.</p>

<p>I suppose the next thing to try would be SolidFire Demo VM on the free ESXi (6.7U3 and 7.0) running nested on Proxmox. (<strong>UPDATE:</strong> this was reported to work.)</p>

      </article>
    </div>
    
      <div class="categories">
    <span><p>Categories:</p>
    
    
      <a href="
      /categories/#storage">storage</a>
      &nbsp; 
    
      <a href="
      /categories/#containers">containers</a>
      &nbsp; 
    
      <a href="
      /categories/#virtualization">virtualization</a>
       
    
  </span>
</div>
    

  
    <div>
      <h3>Related Posts</h3>
      <ul>
      
        <li><a href="/2024/02/11/add-solidfire-storage-from-kvm.html">Add NetApp SolidFire iSCSI storage to KVM</a></li>
      
        <li><a href="/2024/02/07/migrate-netapp-hci-from-vmware.html">How to change NetApp HCI from VMware to alternatives</a></li>
      
        <li><a href="/2025/05/22/windows-2025-netapp-solidfire.html">Windows Server 2025 with NetApp SolidFire</a></li>
      
        <li><a href="/2024/07/31/openshift-ocp-416-iscsi-boot-solidfire.html">Boot OpenShift RCOS from NetApp SolidFire iSCSI target</a></li>
      
        <li><a href="/2024/04/01/windows-server-2025-with-solidfire-part-three-hyper-v.html">Windows Server 2025 with NetApp SolidFire 12 iSCSI Part Three</a></li>
      
      </ul>
    </div>
  

    
  </div><footer class= "footer">
    <p>2025-05-22 13:14 </p>
    <p>Copyright © 2025 scaleoutSean. Content is released under the CC BY license. Design: Alessio Franceschi</p>
</footer>

</body>
</html>
