<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.27.3 by Michael Rose
  Copyright 2013-2025 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Scaling out Nomad batch jobs with BeeGFS and NetApp E-Series | Acting Technologist</title>
<meta name="description" content="Disaggregate and scale out your storage-intensive media jobs with Nomad, BeeGFS and E-Series">


  <meta name="author" content="scaleoutSean">
  
  <meta property="article:author" content="scaleoutSean">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Acting Technologist">
<meta property="og:title" content="Scaling out Nomad batch jobs with BeeGFS and NetApp E-Series">
<meta property="og:url" content="https://scaleoutsean.github.io/2022/04/24/nomad-batch-job-scale-out-parallel-filesystem-beegfs-e-netapp-series.html">


  <meta property="og:description" content="Disaggregate and scale out your storage-intensive media jobs with Nomad, BeeGFS and E-Series">





  <meta name="twitter:site" content="@scaleoutSean">
  <meta name="twitter:title" content="Scaling out Nomad batch jobs with BeeGFS and NetApp E-Series">
  <meta name="twitter:description" content="Disaggregate and scale out your storage-intensive media jobs with Nomad, BeeGFS and E-Series">
  <meta name="twitter:url" content="https://scaleoutsean.github.io/2022/04/24/nomad-batch-job-scale-out-parallel-filesystem-beegfs-e-netapp-series.html">

  
    <meta name="twitter:card" content="summary">
    
  

  



  <meta property="article:published_time" content="2022-04-24T00:00:00+08:00">






<link rel="canonical" href="https://scaleoutsean.github.io/2022/04/24/nomad-batch-job-scale-out-parallel-filesystem-beegfs-e-netapp-series.html">







  <meta name="google-site-verification" content="F6q7vIwQ2G0j8tk-KL9rAXOMLXMDMMUkEz4fRs1Nnnc" />


  <meta name="msvalidate.01" content="7cf5b7d96a77410a8ad035f764dc81b3">





<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Acting Technologist Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  window.enable_copy_code_button = true;
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <link href="https://fonts.googleapis.com/css?family=PT+Sans+Narrow:400,700|PT+Serif:400,700,400italic" rel="stylesheet" type="text/css">


  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          Acting Technologist
          <span class="site-subtitle">human action through technology</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/archive/"
                
                
              >Archive</a>
            </li><li class="masthead__menu-item">
              <a
                href="/categories/"
                
                
              >Categories</a>
            </li><li class="masthead__menu-item">
              <a
                href="/tags/"
                
                
              >Tags</a>
            </li><li class="masthead__menu-item">
              <a
                href="/projects/"
                
                
              >Projects</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">

  

  <div class="author__content">
    <h3 class="author__name p-name" itemprop="name">
      <a class="u-url" rel="me" href="https://scaleoutsean.github.io/" itemprop="url">scaleoutSean</a>
    </h3>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Scaling out Nomad batch jobs with BeeGFS and NetApp E-Series">
    <meta itemprop="description" content="  Introduction  Setup  Scaling-out IO-intensive batch jobs  Shared file system vs. file server vs. internal disk  Scale-out batch jobs with Nomad I/O and BeeGFS          Getting data in and out of the cluster      Scaling out batch jobs        File format conversion          Simple approach for small clusters      Medium and large scale        Getting the files out (or in)  Summary  Demo">
    <meta itemprop="datePublished" content="2022-04-24T00:00:00+08:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://scaleoutsean.github.io/2022/04/24/nomad-batch-job-scale-out-parallel-filesystem-beegfs-e-netapp-series.html" itemprop="url">Scaling out Nomad batch jobs with BeeGFS and NetApp E-Series
</a>
          </h1>
          

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2022-04-24T00:00:00+08:00">2022-04-24 00:00</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          12 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
        <ul>
  <li><a href="#introduction">Introduction</a></li>
  <li><a href="#setup">Setup</a></li>
  <li><a href="#scaling-out-io-intensive-batch-jobs">Scaling-out IO-intensive batch jobs</a></li>
  <li><a href="#shared-file-system-vs-file-server-vs-internal-disk">Shared file system vs. file server vs. internal disk</a></li>
  <li><a href="#scale-out-batch-jobs-with-nomad-io-and-beegfs">Scale-out batch jobs with Nomad I/O and BeeGFS</a>
    <ul>
      <li><a href="#getting-data-in-and-out-of-the-cluster">Getting data in and out of the cluster</a></li>
      <li><a href="#scaling-out-batch-jobs">Scaling out batch jobs</a></li>
    </ul>
  </li>
  <li><a href="#file-format-conversion">File format conversion</a>
    <ul>
      <li><a href="#simple-approach-for-small-clusters">Simple approach for small clusters</a></li>
      <li><a href="#medium-and-large-scale">Medium and large scale</a></li>
    </ul>
  </li>
  <li><a href="#getting-the-files-out-or-in">Getting the files out (or in)</a></li>
  <li><a href="#summary">Summary</a></li>
  <li><a href="#demo">Demo</a></li>
</ul>

<h2 id="introduction">Introduction</h2>

<h2 id="setup">Setup</h2>

<p>These simple example is about running such jobs at a high speed. Normally we'd have multiple clients, but here I have just one.</p>

<ul>
  <li>BeeGFS cluster (VMs)
    <ul>
      <li>b1 - management node</li>
      <li>b2 - metadata server</li>
      <li>b3 - storage node 1</li>
      <li>b5 - client</li>
      <li>b6 - client</li>
    </ul>
  </li>
  <li>Nomad servers (VMs)
    <ul>
      <li>b5 - Nomad server, Nomad client</li>
      <li>b6 - Nomad client</li>
    </ul>
  </li>
  <li>Parallel filesystem
    <ul>
      <li>BeeGFS mounted at /mnt/beegfs on BeeGFS client (b5, b6)</li>
    </ul>
  </li>
  <li>Block storage
    <ul>
      <li>NetApp E-Series, used by BeeGFS storage node 1 (b3)</li>
    </ul>
  </li>
</ul>

<p>On Nomad client with BeeGFS, set Host Volume to a BeeGFS mount point or its subdirectory (e.g. /mnt/beegfs/nomad). This beegfs-ctl output shows the client nodes in the cluster.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>beegfs-ctl <span class="nt">--listnodes</span> <span class="nt">--nodetype</span><span class="o">=</span>client
74EA-62643165-b5 <span class="o">[</span>ID: 73]
36C-6264331C-b6 <span class="o">[</span>ID: 75]
</code></pre></div></div>

<p>Normally we'd have redundancy and availability for BeeGFS Management (VMware or other), Metadata (physical) and Storage (physical) servers. Nomad servers use Raft for HA.</p>

<p><img src="/assets/images/nomad-batch-scaleout-beegfs-eseries.png" alt="HA Nomad cluster with BeeGFS and E-Series" /></p>

<p>Both Nomad and BeeGFS clients are disposable. Failed jobs can be retried from any client with a similar profile.</p>

<h2 id="scaling-out-io-intensive-batch-jobs">Scaling-out IO-intensive batch jobs</h2>

<p>Scale-out of the compute and networking resources used to run jobs depends on the job-running servers, and here we can use fewer 2U dual-socket servers or more 1U uniprocessor servers. That's out of scope for this post, as any combination will do.</p>

<p>As far as storage performance is concerned, we usually scale out in "pod" increments - one pod being something like BeeGFS servers and one E-Series array.</p>

<p>As we add storage servers and arrays, total aggregate performance of shared filesystem increases due to file striping across multiple servers and/or arrays, while per-client performance can easily saturate each individual BeeGFS client.</p>

<p>As an example, here's how we'd scale from 10 to 30 or 50 Nomad/BeeGFS clients that have concurrent IO of 1 GB/s per client:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"># Client (QTY)</th>
      <th style="text-align: center"># Servers (QTY)</th>
      <th style="text-align: center">Mixed R/W (GB/s)</th>
      <th style="text-align: center">E-Series (QTY)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">10-20</td>
      <td style="text-align: center">2</td>
      <td style="text-align: center">20</td>
      <td style="text-align: center">1</td>
    </tr>
    <tr>
      <td style="text-align: center">20-40</td>
      <td style="text-align: center">4</td>
      <td style="text-align: center">40</td>
      <td style="text-align: center">2</td>
    </tr>
    <tr>
      <td style="text-align: center">40-60</td>
      <td style="text-align: center">6</td>
      <td style="text-align: center">60</td>
      <td style="text-align: center">3</td>
    </tr>
  </tbody>
</table>

<p>Let's say we process images or videos for DL/ML or stitch many images into video files. That can be very compute-intensive, so BeeGFS client-to-server ratio would likely be higher than in this table, because not every client would always use 1 GB/s. But for video streaming we may need fewer clients.</p>

<h2 id="shared-file-system-vs-file-server-vs-internal-disk">Shared file system vs. file server vs. internal disk</h2>

<p>Both b5 and b6 have the same view of the filesystem and can read and write to it.</p>

<p><img src="/assets/images/nomad-batch-scaleout-beegfs-eseries-01-topology.png" alt="Nomad cluster with two BeeGFS clients" /></p>

<p>The same volume is exposed to Nomad from the BeeGFS client b6:</p>

<p><img src="/assets/images/nomad-batch-scaleout-beegfs-eseries-04-b5.png" alt="Host Volume from b5" /></p>

<p>And from b6:</p>

<p><img src="/assets/images/nomad-batch-scaleout-beegfs-eseries-03-b6.png" alt="Host Volume from b6" /></p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@b5<span class="nv">$ </span><span class="nb">dir</span> <span class="nt">-lat</span> /mnt/beegfs/nomad/transcode/
<span class="nt">-rw-rw-r--</span> 1 vagrant vagrant 11968151 Apr 23 07:37 beegfs-csi-k8s.mp4
<span class="nt">-rw-r--r--</span> 1 vagrant vagrant 23169766 Apr 22 14:41 velero-solidfire-storagegrid-s3.mp4

@b6<span class="nv">$ </span><span class="nb">dir</span> <span class="nt">-lat</span> /mnt/beegfs/nomad/transcode/
<span class="nt">-rw-rw-r--</span> 1 vagrant vagrant 11968151 Apr 23 07:37 beegfs-csi-k8s.mp4
<span class="nt">-rw-r--r--</span> 1 vagrant vagrant 23169766 Apr 22 14:41 velero-solidfire-storagegrid-s3.mp4
</code></pre></div></div>

<p>This means we can upload data from any client, dispatch compute jobs to any client, and read the result from any client. It can be all the same client, or we can use dedicated clients or groups of BeeGFS clients for particular jobs:</p>

<ul>
  <li>upload data through dedicated multi-homed gateway-style clients at 50 MB/s</li>
  <li>run 3D video simulation on 100 GPU-enabled clients consuming 10 GB/s</li>
  <li>view high resolution output from a developer workstation reading at 3 GB/s</li>
</ul>

<p>There are other approaches that may be worse or better, depending on the requirements and environment.</p>

<p>Sometimes it is better to use local disks. This can come in multiple variants: each client could use own disks, or a parallel filesystem (even BeeGFS) could be created across many clients in RAID0-like fashion. Sometimes this works well, other times it performs poorly and causes unplanned downtime (RAID0-like). Some approaches add storage mirroring (increases the cost by a factor of 2) or erasure coding (increases the cost by 30-50%) but that doesn't entirely fix the problem.</p>

<p>There's also a DAS approach combined with Object Stores, where an Object Store is used to persist data, and anything local is temporary. I blogged about this approach in <a href="/2022/01/28/storagegrid-hybrid-cloud-processing-without-data-at-rest.html">this post</a> which talks about leveraging S3 storage for workloads in the public multi-cloud.</p>

<p>Other times it makes more sense to use network sharing such as NFS and SMB. To paraphrase that politician, you'll know it when you need it. We could pick a BeeGFS client and export/share BeeGFS data through NFS server or Samba, but if you need extensive management features that Linux doesn't have, that may be insufficient.</p>

<p>There's no "best approach" across all use cases and situations.</p>

<p>Many IO-intensive workloads can benefit from using a parallel file system and there's nothing that says that if you use BeeGFS with E-Series you cannot also use some other approach in the same cluster. As an example, we can use BeeGFS for processing and sharing of hot Big Data, have a storage backend for <a href="/2022/04/09/beegfs-csi-introduction.html#options-in-a-mixed-environment">some block workloads</a>, and use S3 for infrequent access to unstructured data.</p>

<h2 id="scale-out-batch-jobs-with-nomad-io-and-beegfs">Scale-out batch jobs with Nomad I/O and BeeGFS</h2>

<p>I wrote about using Nomad batch jobs with Nomad/BeeGFS clients <a href="/2022/04/05/nomad-beegfs-eseries.html">here</a>, emphasizing the ability to access same data from at high performance. In this post I'll only talk about scaling-out jobs mentioned in that post, both in terms of compute, but also in terms of getting data in and out in parallel.</p>

<h3 id="getting-data-in-and-out-of-the-cluster">Getting data in and out of the cluster</h3>

<p>There are many ways to do that, such as running S3, NFS, or SMB on selected BeeGFS clients (mentioned earlier), concurrent Nomad client connectivity to NFS/SMB/BeeGFS, fetching data from S3, replicating it in and out with <a href="/2022/01/18/using-netapp-cloudsync-api.html">CloudSync</a>, rsync, and more.</p>

<p>But, to stay on the previous topic (media conversion), I intend to use SRT - one of popular media transfer protocols.</p>

<p>There's nothing that prevents us from using several at the same time, but since I've covered NFS and S3 in other posts, I'm not going to write about those (or FTP).</p>

<p>Or, to put it bluntly, Nomad and BeeGFS client don't care what you run as long as you have the resources: it's all just jobs - one-off download/upload batch job, NGINX-proxied Web portal, SRT media streaming or file transfer service, or something else.</p>

<h3 id="scaling-out-batch-jobs">Scaling out batch jobs</h3>

<p>What's different compared to that previous Nomad/BeeGFS post is that here we use parametrized batch jobs advantageous to this use case.</p>

<p>That is, previous post was about simple batch jobs which are usually used for a different thing every time (one time we may patch the OS, another time backup a database). For repetitive jobs in which only parameters change, it makes sense to start a "batch job service" and then let it sit there and wait for repetitive jobs, so that it can evaluate, allocate and dispatch them faster.</p>

<p>Another interesting feature offered by Nomad and BeeGFS is the ability to run workloads across different availability zones. We can use Nomad to tag nodes with location (such as the 3rd and 4th floor of DC1, or DC1 and DC2), while BeeGFS can use remote clients or - for selected or all filesystems - buddy mirroring to make storage available locally at each site.</p>

<p>In my example I have each client located on a different floor of DC1:</p>

<p><img src="/assets/images/nomad-batch-scaleout-beegfs-eseries-01-topology.png" alt="Nomad, BeeGFS, E-Series" /></p>

<h2 id="file-format-conversion">File format conversion</h2>

<p>As mentioned earlier, we run parametric batch jobs so that we can dispatch many repetitive jobs to clients.</p>

<h3 id="simple-approach-for-small-clusters">Simple approach for small clusters</h3>

<p>Some simple Nomad demos out there use Object Stores: data is downloaded to the client, converted, and uploaded back to Object Store. How is that not better, but different?</p>

<p>It must be said those are simple demos focusing on front-end features and do not represent production-worthy configuration for medium or large scale deployments.</p>

<h3 id="medium-and-large-scale">Medium and large scale</h3>

<p>For medium and large deployments I'd probably have 1-2 BeeGFS clients that can get out to Object Storage, while the rest of BeeGFS clients would be used for workloads. There's no need to waste CPU and GPU resources downloading and uploading files. Compute nodes should run conversion or otherwise be shut down.</p>

<p>The other difference is if a client with ephemeral disk fails, we need to download the file again which is sometimes fine, but other times expensive. BeeGFS with E-Series uses protected storage (RAID6, for example) and BeeGFS storage service is highly available, so failures can be handled with ease - failed download or upload job would be retired elsewhere.</p>

<p>Our transcode job is configured to run in a batch-style task group that balances jobs across clients on two floors.</p>

<p><img src="/assets/images/nomad-batch-scaleout-beegfs-eseries-02-transcode.png" alt="Parametric task group on Nomad with BeeGFS" /></p>

<p>Individual jobs are then submitted to this task group (called transcode) from the CLI, via the API, from the Nomad Web interface. Let's say I needed a high resolution video for social media and a lower resolution preview for administrative approval.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ nomad job dispatch -detach \ 
    -meta profile="large" \
    -meta input="/mnt/beegfs/nomad/transcode/beegfs-csi-k8s.mp4" transcode
$ nomad job dispatch -detach \
    -meta profile="small" \
    -meta input="/mnt/beegfs/nomad/transcode/beegfs-csi-k8s.mp4" transcode
</code></pre></div></div>

<p>Some may wonder how do we find the right file name. You could get it from SSH shell on a client, but normally you'd get it from a workflow application somewhere in your environment. The same application could dispatch these jobs for you without you doing it manually, as it's common in media workflows.</p>

<p>Individual jobs are sent and processed on one of the available nodes from this gruop. Although, due to the fact that in my small environment node b5 was both Client and Server and I could have only 2-3 conversion jobs running at the same time, most jobs would go to the other client (b6).</p>

<p><img src="/assets/images/nomad-batch-scaleout-beegfs-eseries-05-transcode-jobs.png" alt="Parametric job runs on Nomad with BeeGFS" /></p>

<p>I've added a twist to this process. Rather than just convert the file, which I already did in the non-scale-out demo, I added SRT streaming upon job completion.</p>

<p>What that gives me is the ability to review a job in real-time. In a situation where hardware or network resources are limited this could be effective for lower quality content.</p>

<p>In any case, here's how that looked in practice:</p>

<ul>
  <li>Run a job, check where it was dispatched (e.g. b6, which is 192.168.1.196)</li>
  <li>Open Network Stream from the player on the client to be ready</li>
  <li>As soon as media conversion is done, SRT begins to stream content which may be viewed in video player (if it's running and connected)</li>
</ul>

<p><img src="/assets/images/nomad-batch-scaleout-beegfs-eseries-09-srt-streaming.png" alt="Parametric media conversion and SRT steam with Nomad and BeeGFS" /></p>

<p>In comparison, that simple approach that uses S3 (mentioned earlier) first downloads input file, then renders the file locally, and then uploads the converted files to S3. To view a converted file, we'd need to wait until it's uploaded and then play it from the S3 bucket which may involve additional wait and incur egress charges. Of course, professional media workflows that us S3 have additional tricks in their sleeve. My point is that basic or naive DIY approaches can become complicated or expensive at scale.</p>

<p>One thing I like about this workflow is if you don't have a player that's has that SRT stream open, no bandwidth will be consumed.</p>

<p>There are many ways to make this better, one of which is we could open video player and connect to SRT stream automatically, depending on job parameters (preview=True).</p>

<p>What can be further improved, though, is preview, download and upload of large media files. For that I built anther demo.</p>

<h2 id="getting-the-files-out-or-in">Getting the files out (or in)</h2>

<p>I use SRT to transfer files independently of conversion. This part may feel a bit fake, but this is a PoC so it's simple. Obviously, real life use cases have authentication and authorization and such transfer may be exposed only on private network, while more traditional Web based services or S3 may be used in general upload and download workflows.</p>

<p>But even this basic process showcases advantages of Nomad with BeeGFS and E-Series:</p>

<ul>
  <li>Pick a file name you want to download</li>
  <li>Submit file transfer batch job to Nomad</li>
  <li>On the remote client, start file download</li>
</ul>

<p>This could be done better and offer a choice of download protocols (HTTPS, SRT, etc.).</p>

<p>But this could be used by an astronomer trying to get a high resolution video for local viewing.</p>

<p>Similarly to transcoding, we submit a job that takes some parameters such as the path and file name (example: /mnt/beegfs/nomad/transcode/out-16a44ad493d264909b1ffea8b5445e03-large.mp4). I could have used more descriptive file names in transcoding, but then I'd have to devise a naming convention for input files so I just kept this checksum-based output file names.</p>

<p><img src="/assets/images/nomad-batch-scaleout-beegfs-eseries-10-srt-file-transfer-submit.png" alt="SRT file transfer submission" /></p>

<p>After job has been submitted we check where to (<code class="language-plaintext highlighter-rouge">b6</code>) and run a client-side script (such as "./download.sh b6 out-16a44ad493d264909b1ffea8b5445e03-large.mp4"). This can be automated, but I haven't done it.</p>

<p><img src="/assets/images/nomad-batch-scaleout-beegfs-eseries-07-srt-file-send.png" alt="Running SRT file transfer batch job" /></p>

<p>This screenshot below is repetitive and from another job, but I had to re-run this and capture it with client-side (shell) output.</p>

<p><img src="/assets/images/nomad-batch-scaleout-beegfs-eseries-08-srt-file-receive-done.png" alt="When seconds matter, SRT takes less than one second" /></p>

<p>Abbreviated shell output for easier viewing:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="nv">$ </span>bash srt-get.sh
Source connected <span class="o">(</span><span class="nb">caller</span><span class="o">)</span>, <span class="nb">id</span> <span class="o">[]</span>
Writing output to <span class="o">[</span>/home/sean/Temp/srt/./out-16a44ad493d264909b1ffea8b5445e03-large.mp4]
Connection closed, reading buffer remains
Download COMPLETE.

<span class="nv">$ </span><span class="nb">du</span> <span class="nt">-sh</span> /home/sean/Temp/srt/./out-16a44ad493d264909b1ffea8b5445e03-large.mp4 
42M	/home/sean/Temp/srt/./out-16a44ad493d264909b1ffea8b5445e03-large.mp4
</code></pre></div></div>

<p>My client was on the same 1GigE L2 network as BeeGFS client, but data was transferred from a BeeGFS client VM running on another physical host to my notebook and it took 1 second to download that 42 MB file. Later I tried larger files and got about 87 MB/s which is slower than S3, but I wonder if SRT may be better over larger distances and lossy networks.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">----system----</span> <span class="nt">-net</span>/total- <span class="nt">-dsk</span>/total-
     <span class="nb">time</span>     | recv  send| <span class="nb">read  </span>writ
24-04 06:23:18|3705B 4161B|   0     0 
24-04 06:23:19| 490B  646B|   0     0 
24-04 06:23:20|  42M   44M|   0     0 
24-04 06:23:21|2114B 4100B|   0    56k
</code></pre></div></div>

<p>The same could be done for file uploads <em>to</em> BeeGFS. And it can be done at scale - the combined ingress and egress speed can be many gigabytes per second.</p>

<p><img src="/assets/images/nomad-batch-scaleout-beegfs-eseries-11-batch-job-farm.png" alt="Nomad scale-out batch job farm with BeeGFS and E-Series" /></p>

<h2 id="summary">Summary</h2>

<p>As I concluded in the non-scaleout post on the same topic, there are highly specialized job schedulers that are unbeatable for certain workloads or verticals, but there are also plenty of DIY workflows built in-house on outdated technology from 20 years ago.</p>

<p>They're slow, limited, hard to maintain and can't cater to new workloads. Often they also use storage or workflows that are too slow or perform data copying that could be avoided with BeeGFS and E-Series.</p>

<p>Such worklaods can be moved to Nomad with BeeGFS and E-Series in days, and expanded to host not just batch jobs, but also rescue struggling containerization and virtualization efforts.</p>

<h2 id="demo">Demo</h2>

<ul>
  <li><a href="https://rumble.com/v12vy7f-scaling-out-io-intensive-parametrized-jobs-with-nomad-and-beegfs.html">Scaling-out IO-intensive batch jobs with HashiCorp Nomad, ThinkParQ BeeGFS, and NetApp E-Series</a> - 4m03s</li>
</ul>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/beegfs" class="page__taxonomy-item p-category" rel="tag">beegfs</a><span class="sep">, </span>
    
      <a href="/tags/e-series" class="page__taxonomy-item p-category" rel="tag">e-series</a><span class="sep">, </span>
    
      <a href="/tags/eseries" class="page__taxonomy-item p-category" rel="tag">eseries</a><span class="sep">, </span>
    
      <a href="/tags/netapp" class="page__taxonomy-item p-category" rel="tag">netapp</a><span class="sep">, </span>
    
      <a href="/tags/nomad" class="page__taxonomy-item p-category" rel="tag">nomad</a><span class="sep">, </span>
    
      <a href="/tags/workflow" class="page__taxonomy-item p-category" rel="tag">workflow</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/storage" class="page__taxonomy-item p-category" rel="tag">storage</a>
    
    </span>
  </p>


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time class="dt-published" datetime="2022-04-24T00:00:00+08:00">2022-04-24 00:00</time></p>

      </footer>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://x.com/intent/tweet?via=scaleoutSean&text=Scaling+out+Nomad+batch+jobs+with+BeeGFS+and+NetApp+E-Series%20https%3A%2F%2Fscaleoutsean.github.io%2F2022%2F04%2F24%2Fnomad-batch-job-scale-out-parallel-filesystem-beegfs-e-netapp-series.html" class="btn btn--x" aria-label="Share on X" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on X">
    <i class="fab fa-fw fa-x-twitter" aria-hidden="true"></i><span> X</span>
  </a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://scaleoutsean.github.io/2022/04/24/nomad-batch-job-scale-out-parallel-filesystem-beegfs-e-netapp-series.html" class="btn btn--linkedin" aria-label="Share on LinkedIn" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn">
    <i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span>
  </a>

</section>


      
  <nav class="pagination">
    
      <a href="/2022/04/13/backup-restore-beegfs-csi-pv-with-kanister-kasten.html" class="pagination--pager" title="Backup and restore NetApp BeeGFS CSI PVs with Kanister">Previous</a>
    
    
      <a href="/2022/04/28/solidfire-operator-kubernetes.html" class="pagination--pager" title="SolidFire Operator for Kubernetes">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
    
<div class="page__related">
  
  <h2 class="page__related-title">You may also enjoy</h2>
  <div class="grid__wrapper">
    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/2025/08/11/epa-4-beta.html" rel="permalink">E-Series Performance Analyzer (EPA) v4 beta
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2025-08-11T00:00:00+08:00">2025-08-11 00:00</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          9 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">
  Introduction
  EPA 4
  EPA Collector-specific
  Details related to storage requirements
  What's next

</p>
  </article>
</div>

    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/2025/08/11/bing-index-issue.html" rel="permalink">Got binged in July 2025
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2025-08-11T00:00:00+08:00">2025-08-11 00:00</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          3 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">I don't track users (see Privacy page), don't have ads and don't care about "likes" or "followers", but generally I want my content to be in search engine in...</p>
  </article>
</div>

    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/2025/07/30/solidfire-windows-admin-center-extension.html" rel="permalink">SolidFire Extension for Windows Admin Center 2025
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2025-07-30T00:00:00+08:00">2025-07-30 00:00</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          19 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">
  Introduction
  Walk-through
  And now…
  Conclusion
  Appendix A: demo
    
      Animated GIF (no playback control)
      Video demo with voice-over
    ...</p>
  </article>
</div>

    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/2025/07/26/solidfire-windows-admin-center-gateway.html" rel="permalink">SolidFire Gateway for Windows Admin Center 2025
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2025-07-26T00:00:00+08:00">2025-07-26 00:00</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          5 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">
  Introduction
  What it is
  What it does
  IIS Setup
  Why this matters
  What about it?
  Conclusion

</p>
  </article>
</div>

    
  </div>
</div>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        

<div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
          <li><a href="https://twitter.com/scaleoutsean" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/scaleoutsean" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>


<div class="page__footer-copyright">&copy; 2020-2025 scaleoutSean.github.io | <a href="https://scaleoutsean.github.io">Acting Technologist</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>






  </body>
</html>
