<!doctype html>
<html lang="en">
  <head>
        <meta charset="UTF-8">
    <meta http-equiv="content-language" content="en">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="googlebot" content="noarchive">
    <meta name="googlebot" content="max-snippet:60">
    <meta name="googlebot" content="max-image-preview:small">
    <meta name="bingbot" content="noarchive">
    <meta name="bingbot" content="max-snippet:60">
    <meta name="bingbot" content="max-image-preview:small">
    <meta name="robots" content="noarchive">
    <meta name="robots" content="max-snippet:60">
    <meta name="google-site-verification" content="F6q7vIwQ2G0j8tk-KL9rAXOMLXMDMMUkEz4fRs1Nnnc" />
    <title>
        
            Scaling out Nomad batch jobs with BeeGFS and NetApp E-Series | Acting Technologist
      
    </title>
    <meta name="description" content="
     Disaggregate and scale out your storage-intensive media jobs with Nomad, BeeGFS and E-Series
     ">

    <!-- LINK TO ATOM FEED FOR SEO -->
    <link rel="alternate" type="application/atom+xml" href="https://scaleoutsean.github.io/feed.xml" />

    <!-- FAVICON -->
    <link rel="icon" href="/assets/favicon.ico" type="image/x-icon">
    <link rel="shortcut icon" href="/assets/favicon.ico" type="image/x-icon">

    <!-- CSS -->
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="stylesheet" href="/assets/css/syntax.css">

    <!-- Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css">

    <link href="/_pagefind/pagefind-ui.css" rel="stylesheet">
    <script src="/_pagefind/pagefind-ui.js" type="text/javascript"></script>
    <div id="search"></div>
    <script src="https://cdn.counter.dev/script.js" data-id="83dc700a-b821-4e57-9425-02b8336a9456" data-utcoffset="0"></script>
    <script>
        window.addEventListener('DOMContentLoaded', (event) => {
            new PagefindUI({ element: "#search" });
        });
    </script>

    <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Scaling out Nomad batch jobs with BeeGFS and NetApp E-Series | Acting Technologist</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Scaling out Nomad batch jobs with BeeGFS and NetApp E-Series" />
<meta name="author" content="scaleoutSean" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Disaggregate and scale out your storage-intensive media jobs with Nomad, BeeGFS and E-Series" />
<meta property="og:description" content="Disaggregate and scale out your storage-intensive media jobs with Nomad, BeeGFS and E-Series" />
<link rel="canonical" href="https://scaleoutsean.github.io/2022/04/24/nomad-batch-job-scale-out-parallel-filesystem-beegfs-e-netapp-series.html" />
<meta property="og:url" content="https://scaleoutsean.github.io/2022/04/24/nomad-batch-job-scale-out-parallel-filesystem-beegfs-e-netapp-series.html" />
<meta property="og:site_name" content="Acting Technologist" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-04-24T00:00:00+08:00" />
<script type="application/ld+json">
{"headline":"Scaling out Nomad batch jobs with BeeGFS and NetApp E-Series","dateModified":"2022-04-24T00:00:00+08:00","datePublished":"2022-04-24T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://scaleoutsean.github.io/2022/04/24/nomad-batch-job-scale-out-parallel-filesystem-beegfs-e-netapp-series.html"},"author":{"@type":"Person","name":"scaleoutSean"},"@type":"BlogPosting","url":"https://scaleoutsean.github.io/2022/04/24/nomad-batch-job-scale-out-parallel-filesystem-beegfs-e-netapp-series.html","description":"Disaggregate and scale out your storage-intensive media jobs with Nomad, BeeGFS and E-Series","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  </head>

<body>

  <div class="container">
    <header id="header">
	<div id = site_title>
		<a href="https://scaleoutsean.github.io/">
			<h2 style="font-size:1.8em;">Acting Technologist</h2>
		</a>
		
		<h3>
			civilizations are created by individuals
		</h3>
		
	</div>

	<div id="subheader">
		
		<nav class="pages">
<a href="/about.html">About</a>

<a href="/archive.html">Archive</a>

<a href="/categories/">Categories</a>

<a href="/projects.html">Projects</a>
</nav>
		
		
		<nav class="social">
			
  
    <a href="https://www.github.com/scaleoutsean" target="_blank" id="github"><i class="fab fa-github" aria-hidden="true"></i></a>
  
   

  
    <a href="https://twitter.com/scaleoutsean" target="_blank" id="twitter"><i class="fab fa-twitter" aria-hidden="true"></i></a>
  
   

  
  
    <a href="/feed.xml" target="_blank" id="rss"><i class="fas fa-rss" aria-hidden="true"></i></a>
   

		</nav>
		
	</div>
</header>


    <div class="post-container">
      <article id = "post">
        <h1 id = "post-title">Scaling out Nomad batch jobs with BeeGFS and NetApp E-Series</h1>

  
  <div class = "post-info">
    <span>
        <i class="far fa-calendar" aria-hidden="true"></i> <span>24 Apr 2022</span> - <i class="far fa-clock"></i> 


  
  
    13 minute read
  

    </span>
  </div>
  
        <ul>
  <li><a href="#introduction">Introduction</a></li>
  <li><a href="#setup">Setup</a></li>
  <li><a href="#scaling-out-io-intensive-batch-jobs">Scaling-out IO-intensive batch jobs</a></li>
  <li><a href="#shared-file-system-vs-file-server-vs-internal-disk">Shared file system vs. file server vs. internal disk</a></li>
  <li><a href="#scale-out-batch-jobs-with-nomad-io-and-beegfs">Scale-out batch jobs with Nomad I/O and BeeGFS</a>
    <ul>
      <li><a href="#getting-data-in-and-out-of-the-cluster">Getting data in and out of the cluster</a></li>
      <li><a href="#scaling-out-batch-jobs">Scaling out batch jobs</a></li>
    </ul>
  </li>
  <li><a href="#file-format-conversion">File format conversion</a>
    <ul>
      <li><a href="#simple-approach-for-small-clusters">Simple approach for small clusters</a></li>
      <li><a href="#medium-and-large-scale">Medium and large scale</a></li>
    </ul>
  </li>
  <li><a href="#getting-the-files-out-or-in">Getting the files out (or in)</a></li>
  <li><a href="#summary">Summary</a></li>
  <li><a href="#demo">Demo</a></li>
</ul>

<h2 id="introduction">Introduction</h2>

<h2 id="setup">Setup</h2>

<p>These simple example is about running such jobs at a high speed. Normally we’d have multiple clients, but here I have just one.</p>

<ul>
  <li>BeeGFS cluster (VMs)
    <ul>
      <li>b1 - management node</li>
      <li>b2 - metadata server</li>
      <li>b3 - storage node 1</li>
      <li>b5 - client</li>
      <li>b6 - client</li>
    </ul>
  </li>
  <li>Nomad servers (VMs)
    <ul>
      <li>b5 - Nomad server, Nomad client</li>
      <li>b6 - Nomad client</li>
    </ul>
  </li>
  <li>Parallel filesystem
    <ul>
      <li>BeeGFS mounted at /mnt/beegfs on BeeGFS client (b5, b6)</li>
    </ul>
  </li>
  <li>Block storage
    <ul>
      <li>NetApp E-Series, used by BeeGFS storage node 1 (b3)</li>
    </ul>
  </li>
</ul>

<p>On Nomad client with BeeGFS, set Host Volume to a BeeGFS mount point or its subdirectory (e.g. /mnt/beegfs/nomad). This beegfs-ctl output shows the client nodes in the cluster.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>beegfs-ctl <span class="nt">--listnodes</span> <span class="nt">--nodetype</span><span class="o">=</span>client
74EA-62643165-b5 <span class="o">[</span>ID: 73]
36C-6264331C-b6 <span class="o">[</span>ID: 75]
</code></pre></div></div>

<p>Normally we’d have redundancy and availability for BeeGFS Management (VMware or other), Metadata (physical) and Storage (physical) servers. Nomad servers use Raft for HA.</p>

<p><img src="/assets/images/nomad-batch-scaleout-beegfs-eseries.png" alt="HA Nomad cluster with BeeGFS and E-Series" /></p>

<p>Both Nomad and BeeGFS clients are disposable. Failed jobs can be retried from any client with a similar profile.</p>

<h2 id="scaling-out-io-intensive-batch-jobs">Scaling-out IO-intensive batch jobs</h2>

<p>Scale-out of the compute and networking resources used to run jobs depends on the job-running servers, and here we can use fewer 2U dual-socket servers or more 1U uniprocessor servers. That’s out of scope for this post, as any combination will do.</p>

<p>As far as storage performance is concerned, we usually scale out in “pod” increments - one pod being something like BeeGFS servers and one E-Series array.</p>

<p>As we add storage servers and arrays, total aggregate performance of shared filesystem increases due to file striping across multiple servers and/or arrays, while per-client performance can easily saturate each individual BeeGFS client.</p>

<p>As an example, here’s how we’d scale from 10 to 30 or 50 Nomad/BeeGFS clients that have concurrent IO of 1 GB/s per client:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"># Client (QTY)</th>
      <th style="text-align: center"># Servers (QTY)</th>
      <th style="text-align: center">Mixed R/W (GB/s)</th>
      <th style="text-align: center">E-Series (QTY)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">10-20</td>
      <td style="text-align: center">2</td>
      <td style="text-align: center">20</td>
      <td style="text-align: center">1</td>
    </tr>
    <tr>
      <td style="text-align: center">20-40</td>
      <td style="text-align: center">4</td>
      <td style="text-align: center">40</td>
      <td style="text-align: center">2</td>
    </tr>
    <tr>
      <td style="text-align: center">40-60</td>
      <td style="text-align: center">6</td>
      <td style="text-align: center">60</td>
      <td style="text-align: center">3</td>
    </tr>
  </tbody>
</table>

<p>Let’s say we process images or videos for DL/ML or stitch many images into video files. That can be very compute-intensive, so BeeGFS client-to-server ratio would likely be higher than in this table, because not every client would always use 1 GB/s. But for video streaming we may need fewer clients.</p>

<h2 id="shared-file-system-vs-file-server-vs-internal-disk">Shared file system vs. file server vs. internal disk</h2>

<p>Both b5 and b6 have the same view of the filesystem and can read and write to it.</p>

<p><img src="/assets/images/nomad-batch-scaleout-beegfs-eseries-01-topology.png" alt="Nomad cluster with two BeeGFS clients" /></p>

<p>The same volume is exposed to Nomad from the BeeGFS client b6:</p>

<p><img src="/assets/images/nomad-batch-scaleout-beegfs-eseries-04-b5.png" alt="Host Volume from b5" /></p>

<p>And from b6:</p>

<p><img src="/assets/images/nomad-batch-scaleout-beegfs-eseries-03-b6.png" alt="Host Volume from b6" /></p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@b5<span class="nv">$ </span><span class="nb">dir</span> <span class="nt">-lat</span> /mnt/beegfs/nomad/transcode/
<span class="nt">-rw-rw-r--</span> 1 vagrant vagrant 11968151 Apr 23 07:37 beegfs-csi-k8s.mp4
<span class="nt">-rw-r--r--</span> 1 vagrant vagrant 23169766 Apr 22 14:41 velero-solidfire-storagegrid-s3.mp4

@b6<span class="nv">$ </span><span class="nb">dir</span> <span class="nt">-lat</span> /mnt/beegfs/nomad/transcode/
<span class="nt">-rw-rw-r--</span> 1 vagrant vagrant 11968151 Apr 23 07:37 beegfs-csi-k8s.mp4
<span class="nt">-rw-r--r--</span> 1 vagrant vagrant 23169766 Apr 22 14:41 velero-solidfire-storagegrid-s3.mp4
</code></pre></div></div>

<p>This means we can upload data from any client, dispatch compute jobs to any client, and read the result from any client. It can be all the same client, or we can use dedicated clients or groups of BeeGFS clients for particular jobs:</p>

<ul>
  <li>upload data through dedicated multi-homed gateway-style clients at 50 MB/s</li>
  <li>run 3D video simulation on 100 GPU-enabled clients consuming 10 GB/s</li>
  <li>view high resolution output from a developer workstation reading at 3 GB/s</li>
</ul>

<p>There are other approaches that may be worse or better, depending on the requirements and environment.</p>

<p>Sometimes it is better to use local disks. This can come in multiple variants: each client could use own disks, or a parallel filesystem (even BeeGFS) could be created across many clients in RAID0-like fashion. Sometimes this works well, other times it performs poorly and causes unplanned downtime (RAID0-like). Some approaches add storage mirroring (increases the cost by a factor of 2) or erasure coding (increases the cost by 30-50%) but that doesn’t entirely fix the problem.</p>

<p>There’s also a DAS approach combined with Object Stores, where an Object Store is used to persist data, and anything local is temporary. I blogged about this approach in <a href="/2022/01/28/storagegrid-hybrid-cloud-processing-without-data-at-rest.html">this post</a> which talks about leveraging S3 storage for workloads in the public multi-cloud.</p>

<p>Other times it makes more sense to use network sharing such as NFS and SMB. To paraphrase that politician, you’ll know it when you need it. We could pick a BeeGFS client and export/share BeeGFS data through NFS server or Samba, but if you need extensive management features that Linux doesn’t have, that may be insufficient.</p>

<p>There’s no “best approach” across all use cases and situations.</p>

<p>Many IO-intensive workloads can benefit from using a parallel file system and there’s nothing that says that if you use BeeGFS with E-Series you cannot also use some other approach in the same cluster. As an example, we can use BeeGFS for processing and sharing of hot Big Data, have a storage backend for <a href="/2022/04/09/beegfs-csi-introduction.html#options-in-a-mixed-environment">some block workloads</a>, and use S3 for infrequent access to unstructured data.</p>

<h2 id="scale-out-batch-jobs-with-nomad-io-and-beegfs">Scale-out batch jobs with Nomad I/O and BeeGFS</h2>

<p>I wrote about using Nomad batch jobs with Nomad/BeeGFS clients <a href="/2022/04/05/nomad-beegfs-eseries.html">here</a>, emphasizing the ability to access same data from at high performance. In this post I’ll only talk about scaling-out jobs mentioned in that post, both in terms of compute, but also in terms of getting data in and out in parallel.</p>

<h3 id="getting-data-in-and-out-of-the-cluster">Getting data in and out of the cluster</h3>

<p>There are many ways to do that, such as running S3, NFS, or SMB on selected BeeGFS clients (mentioned earlier), concurrent Nomad client connectivity to NFS/SMB/BeeGFS, fetching data from S3, replicating it in and out with <a href="/2022/01/18/using-netapp-cloudsync-api.html">CloudSync</a>, rsync, and more.</p>

<p>But, to stay on the previous topic (media conversion), I intend to use SRT - one of popular media transfer protocols.</p>

<p>There’s nothing that prevents us from using several at the same time, but since I’ve covered NFS and S3 in other posts, I’m not going to write about those (or FTP).</p>

<p>Or, to put it bluntly, Nomad and BeeGFS client don’t care what you run as long as you have the resources: it’s all just jobs - one-off download/upload batch job, NGINX-proxied Web portal, SRT media streaming or file transfer service, or something else.</p>

<h3 id="scaling-out-batch-jobs">Scaling out batch jobs</h3>

<p>What’s different compared to that previous Nomad/BeeGFS post is that here we use parametrized batch jobs advantageous to this use case.</p>

<p>That is, previous post was about simple batch jobs which are usually used for a different thing every time (one time we may patch the OS, another time backup a database). For repetitive jobs in which only parameters change, it makes sense to start a “batch job service” and then let it sit there and wait for repetitive jobs, so that it can evaluate, allocate and dispatch them faster.</p>

<p>Another interesting feature offered by Nomad and BeeGFS is the ability to run workloads across different availability zones. We can use Nomad to tag nodes with location (such as the 3rd and 4th floor of DC1, or DC1 and DC2), while BeeGFS can use remote clients or - for selected or all filesystems - buddy mirroring to make storage available locally at each site.</p>

<p>In my example I have each client located on a different floor of DC1:</p>

<p><img src="/assets/images/nomad-batch-scaleout-beegfs-eseries-01-topology.png" alt="Nomad, BeeGFS, E-Series" /></p>

<h2 id="file-format-conversion">File format conversion</h2>

<p>As mentioned earlier, we run parametric batch jobs so that we can dispatch many repetitive jobs to clients.</p>

<h3 id="simple-approach-for-small-clusters">Simple approach for small clusters</h3>

<p>Some simple Nomad demos out there use Object Stores: data is downloaded to the client, converted, and uploaded back to Object Store. How is that not better, but different?</p>

<p>It must be said those are simple demos focusing on front-end features and do not represent production-worthy configuration for medium or large scale deployments.</p>

<h3 id="medium-and-large-scale">Medium and large scale</h3>

<p>For medium and large deployments I’d probably have 1-2 BeeGFS clients that can get out to Object Storage, while the rest of BeeGFS clients would be used for workloads. There’s no need to waste CPU and GPU resources downloading and uploading files. Compute nodes should run conversion or otherwise be shut down.</p>

<p>The other difference is if a client with ephemeral disk fails, we need to download the file again which is sometimes fine, but other times expensive. BeeGFS with E-Series uses protected storage (RAID6, for example) and BeeGFS storage service is highly available, so failures can be handled with ease - failed download or upload job would be retired elsewhere.</p>

<p>Our transcode job is configured to run in a batch-style task group that balances jobs across clients on two floors.</p>

<p><img src="/assets/images/nomad-batch-scaleout-beegfs-eseries-02-transcode.png" alt="Parametric task group on Nomad with BeeGFS" /></p>

<p>Individual jobs are then submitted to this task group (called transcode) from the CLI, via the API, from the Nomad Web interface. Let’s say I needed a high resolution video for social media and a lower resolution preview for administrative approval.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ nomad job dispatch -detach \ 
    -meta profile="large" \
    -meta input="/mnt/beegfs/nomad/transcode/beegfs-csi-k8s.mp4" transcode
$ nomad job dispatch -detach \
    -meta profile="small" \
    -meta input="/mnt/beegfs/nomad/transcode/beegfs-csi-k8s.mp4" transcode
</code></pre></div></div>

<p>Some may wonder how do we find the right file name. You could get it from SSH shell on a client, but normally you’d get it from a workflow application somewhere in your environment. The same application could dispatch these jobs for you without you doing it manually, as it’s common in media workflows.</p>

<p>Individual jobs are sent and processed on one of the available nodes from this gruop. Although, due to the fact that in my small environment node b5 was both Client and Server and I could have only 2-3 conversion jobs running at the same time, most jobs would go to the other client (b6).</p>

<p><img src="/assets/images/nomad-batch-scaleout-beegfs-eseries-05-transcode-jobs.png" alt="Parametric job runs on Nomad with BeeGFS" /></p>

<p>I’ve added a twist to this process. Rather than just convert the file, which I already did in the non-scale-out demo, I added SRT streaming upon job completion.</p>

<p>What that gives me is the ability to review a job in real-time. In a situation where hardware or network resources are limited this could be effective for lower quality content.</p>

<p>In any case, here’s how that looked in practice:</p>

<ul>
  <li>Run a job, check where it was dispatched (e.g. b6, which is 192.168.1.196)</li>
  <li>Open Network Stream from the player on the client to be ready</li>
  <li>As soon as media conversion is done, SRT begins to stream content which may be viewed in video player (if it’s running and connected)</li>
</ul>

<p><img src="/assets/images/nomad-batch-scaleout-beegfs-eseries-09-srt-streaming.png" alt="Parametric media conversion and SRT steam with Nomad and BeeGFS" /></p>

<p>In comparison, that simple approach that uses S3 (mentioned earlier) first downloads input file, then renders the file locally, and then uploads the converted files to S3. To view a converted file, we’d need to wait until it’s uploaded and then play it from the S3 bucket which may involve additional wait and incur egress charges. Of course, professional media workflows that us S3 have additional tricks in their sleeve. My point is that basic or naive DIY approaches can become complicated or expensive at scale.</p>

<p>One thing I like about this workflow is if you don’t have a player that’s has that SRT stream open, no bandwidth will be consumed.</p>

<p>There are many ways to make this better, one of which is we could open video player and connect to SRT stream automatically, depending on job parameters (preview=True).</p>

<p>What can be further improved, though, is preview, download and upload of large media files. For that I built anther demo.</p>

<h2 id="getting-the-files-out-or-in">Getting the files out (or in)</h2>

<p>I use SRT to transfer files independently of conversion. This part may feel a bit fake, but this is a PoC so it’s simple. Obviously, real life use cases have authentication and authorization and such transfer may be exposed only on private network, while more traditional Web based services or S3 may be used in general upload and download workflows.</p>

<p>But even this basic process showcases advantages of Nomad with BeeGFS and E-Series:</p>

<ul>
  <li>Pick a file name you want to download</li>
  <li>Submit file transfer batch job to Nomad</li>
  <li>On the remote client, start file download</li>
</ul>

<p>This could be done better and offer a choice of download protocols (HTTPS, SRT, etc.).</p>

<p>But this could be used by an astronomer trying to get a high resolution video for local viewing.</p>

<p>Similarly to transcoding, we submit a job that takes some parameters such as the path and file name (example: /mnt/beegfs/nomad/transcode/out-16a44ad493d264909b1ffea8b5445e03-large.mp4). I could have used more descriptive file names in transcoding, but then I’d have to devise a naming convention for input files so I just kept this checksum-based output file names.</p>

<p><img src="/assets/images/nomad-batch-scaleout-beegfs-eseries-10-srt-file-transfer-submit.png" alt="SRT file transfer submission" /></p>

<p>After job has been submitted we check where to (<code class="language-plaintext highlighter-rouge">b6</code>) and run a client-side script (such as “./download.sh b6 out-16a44ad493d264909b1ffea8b5445e03-large.mp4”). This can be automated, but I haven’t done it.</p>

<p><img src="/assets/images/nomad-batch-scaleout-beegfs-eseries-07-srt-file-send.png" alt="Running SRT file transfer batch job" /></p>

<p>This screenshot below is repetitive and from another job, but I had to re-run this and capture it with client-side (shell) output.</p>

<p><img src="/assets/images/nomad-batch-scaleout-beegfs-eseries-08-srt-file-receive-done.png" alt="When seconds matter, SRT takes less than one second" /></p>

<p>Abbreviated shell output for easier viewing:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="nv">$ </span>bash srt-get.sh
Source connected <span class="o">(</span><span class="nb">caller</span><span class="o">)</span>, <span class="nb">id</span> <span class="o">[]</span>
Writing output to <span class="o">[</span>/home/sean/Temp/srt/./out-16a44ad493d264909b1ffea8b5445e03-large.mp4]
Connection closed, reading buffer remains
Download COMPLETE.

<span class="nv">$ </span><span class="nb">du</span> <span class="nt">-sh</span> /home/sean/Temp/srt/./out-16a44ad493d264909b1ffea8b5445e03-large.mp4 
42M	/home/sean/Temp/srt/./out-16a44ad493d264909b1ffea8b5445e03-large.mp4
</code></pre></div></div>

<p>My client was on the same 1GigE L2 network as BeeGFS client, but data was transferred from a BeeGFS client VM running on another physical host to my notebook and it took 1 second to download that 42 MB file. Later I tried larger files and got about 87 MB/s which is slower than S3, but I wonder if SRT may be better over larger distances and lossy networks.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">----system----</span> <span class="nt">-net</span>/total- <span class="nt">-dsk</span>/total-
     <span class="nb">time</span>     | recv  send| <span class="nb">read  </span>writ
24-04 06:23:18|3705B 4161B|   0     0 
24-04 06:23:19| 490B  646B|   0     0 
24-04 06:23:20|  42M   44M|   0     0 
24-04 06:23:21|2114B 4100B|   0    56k
</code></pre></div></div>

<p>The same could be done for file uploads <em>to</em> BeeGFS. And it can be done at scale - the combined ingress and egress speed can be many gigabytes per second.</p>

<p><img src="/assets/images/nomad-batch-scaleout-beegfs-eseries-11-batch-job-farm.png" alt="Nomad scale-out batch job farm with BeeGFS and E-Series" /></p>

<h2 id="summary">Summary</h2>

<p>As I concluded in the non-scaleout post on the same topic, there are highly specialized job schedulers that are unbeatable for certain workloads or verticals, but there are also plenty of DIY workflows built in-house on outdated technology from 20 years ago.</p>

<p>They’re slow, limited, hard to maintain and can’t cater to new workloads. Often they also use storage or workflows that are too slow or perform data copying that could be avoided with BeeGFS and E-Series.</p>

<p>Such worklaods can be moved to Nomad with BeeGFS and E-Series in days, and expanded to host not just batch jobs, but also rescue struggling containerization and virtualization efforts.</p>

<h2 id="demo">Demo</h2>

<ul>
  <li><a href="https://rumble.com/v12vy7f-scaling-out-io-intensive-parametrized-jobs-with-nomad-and-beegfs.html">Scaling-out IO-intensive batch jobs with HashiCorp Nomad, ThinkParQ BeeGFS, and NetApp E-Series</a> - 4m03s</li>
</ul>

      </article>
    </div>
    
      <div class="categories">
    <span><p>Categories:</p>
    
    
      <a href="
      /categories/#storage">storage</a>
       
    
  </span>
</div>
    

    
      <div class="related" data-pagefind-ignore>

    <h4>Possibly related - use live search at the top to find other content</h4>
    
    
    
    
    
    
    
        
        
    
        
    
        
          <div class = "related-posts">
          <h5><a href="/2024/04/26/swagger-files-netapp-eseries-arrays.html">• Swagger files for NetApp E-Series</a></h5>
          </div>
          
          
        
    
      
    
        
        
    
        
    
        
          <div class = "related-posts">
          <h5><a href="/2024/04/25/openapi-swagger-for-netapp-solidfire.html">• OpenAPI and SolidFire</a></h5>
          </div>
          
          
        
    
      
    
        
        
    
        
    
        
          <div class = "related-posts">
          <h5><a href="/2024/04/24/netapp-solidfire-monitor-backup-influx-grafana-11.html">• Metrics for NetApp SolidFire backup-to-S3 in InfluxDB and Grafana</a></h5>
          </div>
          
          
        
    
      
    
        
        
    
        
    
        
          <div class = "related-posts">
          <h5><a href="/2024/04/23/grafana-11-netapp-solidfire-sfc.html">• NetApp SolidFire Collector with Grafana 11</a></h5>
          </div>
          
          
        
    
      
    
        
        
    
        
    
        
          <div class = "related-posts">
          <h5><a href="/2024/04/23/econfig-update.html">• EConfig v2</a></h5>
          </div>
          
          
            
    
    </div>

    

    
  </div><footer class= "footer">
    <p>2024-04-26 23:54 </p>
    <p>Copyright © 2024 scaleoutSean. Content is released under the CC BY license. Design: Alessio Franceschi</p>
</footer>

</body>
</html>
