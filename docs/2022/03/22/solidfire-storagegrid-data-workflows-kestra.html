<!doctype html>
<html lang="en">
  <head>
        <meta charset="UTF-8">
    <meta http-equiv="content-language" content="en">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="googlebot" content="noarchive">
    <meta name="googlebot" content="max-snippet:60">
    <meta name="googlebot" content="max-image-preview:small">
    <meta name="bingbot" content="noarchive">
    <meta name="bingbot" content="max-snippet:60">
    <meta name="bingbot" content="max-image-preview:small">
    <meta name="robots" content="noarchive">
    <meta name="robots" content="max-snippet:60">
    <meta name="google-site-verification" content="F6q7vIwQ2G0j8tk-KL9rAXOMLXMDMMUkEz4fRs1Nnnc" />
    <title>
        
            Simplify data workflows with Kestra | Acting Technologist
      
    </title>
    <meta name="description" content="
     Offload workflows and scheduling in automation to simplify script-driven workflows and easily build more complex workflows
     ">

    <!-- LINK TO ATOM FEED FOR SEO -->
    <link rel="alternate" type="application/atom+xml" href="https://scaleoutsean.github.io/feed.xml" />

    <!-- FAVICON -->
    <link rel="icon" href="/assets/favicon.ico" type="image/x-icon">
    <link rel="shortcut icon" href="/assets/favicon.ico" type="image/x-icon">

    <!-- CSS -->
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="stylesheet" href="/assets/css/syntax.css">

    <!-- Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css">

    <link href="/_pagefind/pagefind-ui.css" rel="stylesheet">
    <script src="/_pagefind/pagefind-ui.js" type="text/javascript"></script>
    <div id="search"></div>
    <script src="https://cdn.counter.dev/script.js" data-id="83dc700a-b821-4e57-9425-02b8336a9456" data-utcoffset="0"></script>
    <script>
        window.addEventListener('DOMContentLoaded', (event) => {
            new PagefindUI({ element: "#search" });
        });
    </script>

    <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Simplify data workflows with Kestra | Acting Technologist</title>
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="Simplify data workflows with Kestra" />
<meta name="author" content="scaleoutSean" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Offload workflows and scheduling in automation to simplify script-driven workflows and easily build more complex workflows" />
<meta property="og:description" content="Offload workflows and scheduling in automation to simplify script-driven workflows and easily build more complex workflows" />
<link rel="canonical" href="https://scaleoutsean.github.io/2022/03/22/solidfire-storagegrid-data-workflows-kestra.html" />
<meta property="og:url" content="https://scaleoutsean.github.io/2022/03/22/solidfire-storagegrid-data-workflows-kestra.html" />
<meta property="og:site_name" content="Acting Technologist" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-03-22T00:00:00+08:00" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://scaleoutsean.github.io/2022/03/22/solidfire-storagegrid-data-workflows-kestra.html"},"author":{"@type":"Person","name":"scaleoutSean"},"description":"Offload workflows and scheduling in automation to simplify script-driven workflows and easily build more complex workflows","@type":"BlogPosting","url":"https://scaleoutsean.github.io/2022/03/22/solidfire-storagegrid-data-workflows-kestra.html","headline":"Simplify data workflows with Kestra","dateModified":"2022-03-22T00:00:00+08:00","datePublished":"2022-03-22T00:00:00+08:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  </head>

<body>

  <div class="container">
    <header id="header">
	<div id = site_title>
		<a href="https://scaleoutsean.github.io/">
			<h2 style="font-size:1.8em;">Acting Technologist</h2>
		</a>
		
		<h3>
			civilizations are created by individuals
		</h3>
		
	</div>

	<div id="subheader">
		
		<nav class="pages">
<a href="/about.html">About</a>

<a href="/archive.html">Archive</a>

<a href="/categories/">Categories</a>

<a href="/projects.html">Projects</a>
</nav>
		
		
		<nav class="social">
			
  
    <a href="https://www.github.com/scaleoutsean" target="_blank" id="github"><i class="fab fa-github" aria-hidden="true"></i></a>
  
   

  
    <a href="https://twitter.com/scaleoutsean" target="_blank" id="twitter"><i class="fab fa-twitter" aria-hidden="true"></i></a>
  
   

  
  
    <a href="/feed.xml" target="_blank" id="rss"><i class="fas fa-rss" aria-hidden="true"></i></a>
   

		</nav>
		
	</div>
</header>


    <div class="post-container">
      <article id = "post">
        <h1 id = "post-title">Simplify data workflows with Kestra</h1>

  
  <div class = "post-info">
    <span>
        <i class="far fa-calendar" aria-hidden="true"></i> <span>22 Mar 2022</span> - <i class="far fa-clock"></i> 


  
  
    11 minute read
  

    </span>
  </div>
  
        <h2 id="introduction">Introduction</h2>

<p>Sometimes I blog about storage-specific automation and usually I focus on SolidFire because it… just damn works.</p>

<p>I’ve also written several scripts - for backup and whatnot - that showcase how SolidFire makes it easy to automate somewhat complex actions. The SolidFire architecture and developer-friendly API make that possible.</p>

<p>What I haven’t had time to do yet was to investigate inserting these “steps” of workflows into real workflows. For an example, in SolidBackup there are several steps:</p>

<ul>
  <li>Configuration management - maintain and manage your list of volumes</li>
  <li>Cloning management - run jobs (possibly hundreds) - that copy production volumes into clones (SolidFire array-side)</li>
  <li>Backup management - mount those clones from backup containers (Docker, Kubernetes) or VMs and copy (backup) data to S3</li>
  <li>Monitoring, reporting, alerting, auditing - have some way to see what’s going on without using <code class="language-plaintext highlighter-rouge">grep</code></li>
</ul>

<p>I solved the first by having a configuration file in (say) Git. The second and third are scripts and the third requires redirecting script logs to a back-end that would provide monitoring, reporting, alerting, and auditing. So there’s some amount of DIY work that needs to be done, especially because if script logging isn’t well structured (and it likely won’t be, if more than one tool is used in the process).</p>

<p>Of course, I knew that early on, but I also knew there’s no logging without scripts that do the work, so I wrote the scripts first with the idea that those may help others get started and write the rest to suit their circumstances (everyone has a different solution and requirements for that last step).</p>

<p>For backup, recently I’ve examined the possibility of taking care of cloning independently (Step 2) and using <a href="/2022/03/15/velero-18-with-restic-and-trident-2201.html#using-velero-and-restic-to-backup-regular-solidfire-volumes">Velero V1.8 for backup management</a> (Step 3) even for non-containerized volumes (as long as they are formatted with ext[3,4] or xfs).</p>

<p>I’ve also explored using <a href="/2022/02/14/middle-class-rbac-solidfire-ansible.html">Ansible</a> for workflows that require RBAC feature for segregated access to SolidFire resources.</p>

<p>For generic workflows, a less prescriptive approach would be to use a generic workflow management tool.</p>

<p>For Deep Learning and Big Data related workflows, NetApp has the Data Ops Tool (I call it “DOT”) that currently doesn’t support SolidFire API. It certainly could but SolidFire usually isn’t used in Deep Learning. So rather than trying Airflow and Jupyter/DOT, I wanted to try something more “generic”.</p>

<p>So I picked Kestra for this PoC. You can find more about it on <a href="https://kestra.io/blogs/2022-02-22-leroy-merlin-usage-kestra.html">their Web site</a>. The point is, it’s a data-focused workflow tool so it should be good for my purpose.</p>

<p>I tried two things:</p>

<ul>
  <li>SolidFire’s native Backup to S3 (use the SolidFire API from a container)</li>
  <li>StorageGRID Audit Log Analysis (implement several steps to have the entire pipeline in one place)</li>
</ul>

<h2 id="backup-to-s3">Backup to S3</h2>

<p>Kestra lets you build “flows”, i.e. pipelines, or data workflows. I first built one called “backup”. It’s a single task workflow, it just calls an API method that <a href="/2021/06/22/solidfire-backup-and-cloning-with-per-storage-node-queues.html">copies SolidFire volumes to S3 buckets</a>.</p>

<p><img src="/assets/images/workflow-automation-with-backup-01.png" alt="Backup workflow for SolidFire" /></p>

<p>The way I built this “flow” is to ask the user for several inputs and kick of a Backup-to-S3 job based on these inputs. Now, this is kind of fake. Normally, if we provided a Volume ID we wouldn’t need to also provide a SolidFire volume name because you could get it via the API. But I wanted to test parametric execution where not everything is hard-coded.</p>

<p>Also, rather than backup volumes one by one, we would probably have a fairly static list of 20 or 100 volumes and backup four or more in parallel (see the link above, related to parallel backup to S3).</p>

<p>Anyway, the user provides some inputs that make the script work. The script itself is based on the steps provided <a href="/2021/04/21/solidfire-backup-to-s3.html#automating-solidfire-backup-to-s3">here</a>. It could all be hard-coded, too, especially if you have just one cluster and the set of Volume IDs rarely changes you could edit the list of hardcoded volumes in Kestra’s Web based editor which (see the demo) provides versioning and diff-view features.</p>

<p><img src="/assets/images/workflow-automation-with-backup-03.png" alt="Kestra flow inputs" /></p>

<p>Once inputs are provided, workflow can execute. This step below kicks it off and in Executions we can find the flow’s log. In the screenshot below all runs result in a “Warning” (yellow button) because the script returned more than just 0 and Kestra couldn’t determine if that was a warning or not (it wasn’t, so the warnings are unnecessary, but you’d have to check the flow the first time to figure this out). This is merely an aesthetic/usability issue and can be fixed by making the script run more quietly.</p>

<p><img src="/assets/images/workflow-automation-with-backup-02.png" alt="Kestra executions" /></p>

<p>Backups usually run at night, so we want to schedule those flows. That can be done. I also specified the optional backfill schedule to run one before its scheduled time.</p>

<p><img src="/assets/images/workflow-automation-with-backup-04.png" alt="Kestra schedules" /></p>

<p>I had expected backfill time to be in browser-local TZ, but that didn’t work. After some experimenting Kestra started two backup jobs at once (one because I executed the workflow, another was backfill scheduled to $NOW), which is when I realized that backfill time is in UTC TZ.</p>

<p><img src="/assets/images/workflow-automation-with-backup-05.png" alt="SolidFire runs scheduled Kestra job and backfill job" /></p>

<p>After a flow execution completes its logs can be reviewed. The bad part about this is passwords and credentials end up in the logs. Unless you are the only user operating this instance, you’d want to find a way to fix that. I haven’t found a way to not log those sensitive details (in Community Edition I used).</p>

<p><img src="/assets/images/workflow-automation-with-backup-06.png" alt="Kestra flow log" /></p>

<p>So that’s basically it. There are several advanced options - I could provide a list of volumes and run N loops (jobs), etc. - but it’s only a matter of spending more time to refine and optimize flows.</p>

<p>I should add that (as expected) it is possible to chain tasks/steps, as it is to write everything in one giant step. The former is better because it’s not all-or-nothing. This screenshot shows how we can extract variables such as async handle (of the backup job) and job key from response returned by the SolidFire API, which is one of ways how we dynamically pass inputs from one task to another.</p>

<p><img src="/assets/images/workflow-automation-with-backup-07.png" alt="Kestra flow outputs" /></p>

<p>This could be used in the next step, to poll jobs as they run, and possibly retry those that fail. I hit some Kestra bugs here so I gave up on those tasks.</p>

<h2 id="generic-data-workflow">Generic data workflow</h2>

<p>Because I’ve written too many backup-related posts, I thought to try a more practical workflow, such as StorageGRID audit log converter (<a href="/2021/10/20/sgac-storagegrid-audit-log-converter-v0.2.1.html">SGAC</a>). Why?</p>

<p>It’s more complex - one needs to get the audit log file from an NFS share, maybe split it in 16 or 32 chunks, then convert each, and finally pass JSON files to some other program for analysis. This may involve multiple systems, applications, and protocols.</p>

<p>Enter my fake SGAC flow for Kestra:</p>

<p><img src="/assets/images/workflow-automation-with-sgac-01.png" alt="Made up workflow for SGAC" /></p>

<p>There are more steps than necessary (one of them is completely repetitive), but I was just playing with it and didn’t want to delete tasks. What I do is build a container that runs SGAC (download SGAC Python script, install Python modules), then run it against uploaded StorageGRID audit log file (manual step in my flow), let the container convert that log (or log chunks) to JSON, and push output(s) to Elasticsearch.</p>

<p>This looks much nicer as far as demonstrations go - you can see the flow going from one task to another, etc. Convert is the task where conversion to JSON happens, so it takes most time. Elasticsearch load fails because of what I suspect is a bug or documentation problem (and Kestra has few other issues that aren’t hard to spot, it seems to me) and I couldn’t make Elasticsearch upload to work even in a single task flow.</p>

<p><img src="/assets/images/workflow-automation-with-sgac-02.png" alt="SGAC flow in Kestra" /></p>

<p>But bugs can be fixed and documentation improved - we care if the converted audit log file was produced as expected. It was (at the very bottom).</p>

<p><img src="/assets/images/workflow-automation-with-sgac-03.png" alt="SGAC JSON file" /></p>

<p>We can find and download it from Outputs tab for this job, which was a workaround I used to get the output and upload it manually to Elasticsearch to see if it was okay. (I think it’s listed twice because <code class="language-plaintext highlighter-rouge">files.outFile</code> may be a deprecated variable to access file outputs while <code class="language-plaintext highlighter-rouge">outputFiles</code> is the new one, but I’m not sure).</p>

<p><img src="/assets/images/workflow-automation-with-sgac-04.png" alt="SGAC convert task output" /></p>

<p>Kestra on Kubernetes can drive many containers to process StorageGRID logs and that almost always beats a single VM running a DIY convert-and-upload script. So even if you don’t see much benefit from moving a script like SGAC to Kestra, I see value in Kestra for this use case because it “kubernetizes” SGAC and makes it easy to scale it out, retry jobs, and more. If you have a busy StorageGRID v11.0-11.5 and need to convert 50-100 2GB logs every day, Kestra can probably help you.</p>

<p>Is this example related to SolidFire? Not directly, but there’s no reason why we couldn’t run this workload on SolidFire:</p>

<ul>
  <li>We just need some ephemeral storage to run container images. Data itself (StorageGRID logs, in this case) doesn’t need to be stored at rest on SolidFire</li>
  <li>Docker images used to convert StorageGRID logs into JSON can keep these files in <a href="/2022/01/28/storagegrid-hybrid-cloud-processing-without-data-at-rest.html">RAM</a> - there’s no need to save anything (even temporary data files) on SolidFire volumes; converted JSON files can be uploaded to a StorageGRID bucket or Elasticsearch or other location as last step of the flow</li>
</ul>

<h2 id="kestra-or-the-dot-for-solidfire-workflows">Kestra or the DOT for SolidFire workflows</h2>

<p>Can Kestra act as a Data Ops Toolkit for SolidFire? While it may be easier to automate SolidFire from Kestra than expand the Data Ops Toolkit with SolidFire-specific functionality, I think Kestra is not focused on Deep Learning (feel free to compare it with Airflow, Argo or whatever seems of interest).</p>

<p>Although you could Kestra’s “forms” (in Execute step) to get outcomes similar to Jupyter with the DOT, personally I’d use it more for scheduled jobs that span different applications (check Kestra’s documentation to find more on that) than for interactive use.</p>

<p>What about use cases for ONTAP and StorageGRID? There isn’t much overlap, but - for the sake of an example - the DOT can download data from StorageGRID to ONTAP for processing, and upload back the results. In this case, if it’s application specific <em>and</em> Kestra has a plugin for it, I’d use it. If there’s no plugin or if the workflow involves ONTAP clones, I’d use the DOT. And finally, we can use the DOT from <em>within</em> Kestra, which further increases our options.</p>

<p>Some ideas for Kestra with SolidFire:</p>

<ul>
  <li>SolidFire’s native Backup to S3 - this needs managed parallelism, logging, monitoring, scheduling; I wouldn’t use it for Velero or other backup that runs externally and has its own application-specific, integrated scheduling</li>
  <li>Multiple tasks that use the SolidFire API - migrate scripts from VMs to Kestra to have them managed and executed in one place</li>
  <li>Database cloning between different arrays - good for DevTest, although I’d like to see Kestra provide native PowerShell support to make it easier to use in Windows environments</li>
</ul>

<h2 id="conclusion">Conclusion</h2>

<p>As far as SolidFire-related automation is concerned, Kestra’s Bash wrapper seems to have some bugs. I struggled with it when I tried to use solidire-cli from it, but even scripts that had curl with <code class="language-plaintext highlighter-rouge">-H</code> option couldn’t be validated and saved. So I ended up using Kestra’s Python wrapper and SolidFire Python SDK.</p>

<p>This post shared two basic examples that illustrate the value in offloading automation plumbing to a specialized engine such as Kestra: you get scheduling, logging, monitoring and more without spending much time on that.</p>

<p>Kestra can run from Docker and Kubernetes, and can build sophisticated workflows that save time and improve reliability (at least when I compare it to my own scripts).</p>

<p><img src="/assets/images/workflow-automation-kestra.png" alt="Complex Kestra workflow" /></p>

<p>But for now you have to be ready to deal with its bugs (or the not-there-yet documentation - either way, I struggled too much for my liking).</p>

<p>Once it has a way to mask or exclude sensitive log entries and the documentation and code are better, I’d consider using it for data-related workflows.</p>

<p>Kestra Enterprise Edition has features such as RBAC which solve some of the shortcomings I mention, if you need better access control, namespace segregation, etc.</p>

<h2 id="demo">Demo</h2>

<ul>
  <li><a href="https://rumble.com/vyjomt-kestra-demo-with-solidfire-backup-and-sgac-log-conversion.html">Kestra demo with SolidFire Backup and SGAC log conversion</a> - 3m55s</li>
</ul>

      </article>
    </div>
    
      <div class="categories">
    <span><p>Categories:</p>
    
    
      <a href="
      /categories/#automation">automation</a>
       
    
  </span>
</div>
    

  
    <div>
      <h3>Related Posts</h3>
      <ul>
      
        <li><a href="/2022/08/11/leverage-beegfs-nextflow.html">Leverage BeeGFS in Nextflow to avoid unnecessary data movement</a></li>
      
        <li><a href="/2022/06/14/batch-copy-files-beegfs.html">Copy files to or from BeeGFS before or after scheduled jobs</a></li>
      
        <li><a href="/2022/04/24/nomad-batch-job-scale-out-parallel-filesystem-beegfs-e-netapp-series.html">Scaling out Nomad batch jobs with BeeGFS and NetApp E-Series</a></li>
      
        <li><a href="/2022/04/05/nomad-beegfs-eseries.html">Nomad batch jobs with BeeGFS and E-Series</a></li>
      
        <li><a href="/2022/03/29/manage-solidfire-jupyter-powershell.html">Manage SolidFire from Jupyter Python or .NET notebooks</a></li>
      
      </ul>
    </div>
  

    
  </div><footer class= "footer">
    <p>2025-07-23 23:04 </p>
    <p>Copyright © 2025 scaleoutSean. Content is released under the CC BY license. Design: Alessio Franceschi</p>
</footer>

</body>
</html>
