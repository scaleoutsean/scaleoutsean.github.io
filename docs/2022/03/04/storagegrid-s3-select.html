<!doctype html>
<html lang="en">
  <head>
        <meta charset="UTF-8">
    <meta http-equiv="content-language" content="en">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="googlebot" content="noarchive">
    <meta name="googlebot" content="max-snippet:60">
    <meta name="googlebot" content="max-image-preview:small">
    <meta name="bingbot" content="noarchive">
    <meta name="bingbot" content="max-snippet:60">
    <meta name="bingbot" content="max-image-preview:small">
    <meta name="robots" content="noarchive">
    <meta name="robots" content="max-snippet:60">
    <meta name="google-site-verification" content="F6q7vIwQ2G0j8tk-KL9rAXOMLXMDMMUkEz4fRs1Nnnc" />
    <title>
        
            S3 Select and other new features of NetApp StorageGRID 11.6 | Acting Technologist
      
    </title>
    <meta name="description" content="
     About S3 Select and other new features in StorageGRID 11.6
     ">

    <!-- LINK TO ATOM FEED FOR SEO -->
    <link rel="alternate" type="application/atom+xml" href="https://scaleoutsean.github.io/feed.xml" />

    <!-- FAVICON -->
    <link rel="icon" href="/assets/favicon.ico" type="image/x-icon">
    <link rel="shortcut icon" href="/assets/favicon.ico" type="image/x-icon">

    <!-- CSS -->
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="stylesheet" href="/assets/css/syntax.css">

    <!-- Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css">

    <link href="/_pagefind/pagefind-ui.css" rel="stylesheet">
    <script src="/_pagefind/pagefind-ui.js" type="text/javascript"></script>
    <div id="search"></div>
    <script src="https://cdn.counter.dev/script.js" data-id="83dc700a-b821-4e57-9425-02b8336a9456" data-utcoffset="0"></script>
    <script>
        window.addEventListener('DOMContentLoaded', (event) => {
            new PagefindUI({ element: "#search" });
        });
    </script>

    <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>S3 Select and other new features of NetApp StorageGRID 11.6 | Acting Technologist</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="S3 Select and other new features of NetApp StorageGRID 11.6" />
<meta name="author" content="scaleoutSean" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="About S3 Select and other new features in StorageGRID 11.6" />
<meta property="og:description" content="About S3 Select and other new features in StorageGRID 11.6" />
<link rel="canonical" href="https://scaleoutsean.github.io/2022/03/04/storagegrid-s3-select.html" />
<meta property="og:url" content="https://scaleoutsean.github.io/2022/03/04/storagegrid-s3-select.html" />
<meta property="og:site_name" content="Acting Technologist" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-03-04T00:00:00+08:00" />
<script type="application/ld+json">
{"headline":"S3 Select and other new features of NetApp StorageGRID 11.6","dateModified":"2022-03-04T00:00:00+08:00","datePublished":"2022-03-04T00:00:00+08:00","author":{"@type":"Person","name":"scaleoutSean"},"@type":"BlogPosting","url":"https://scaleoutsean.github.io/2022/03/04/storagegrid-s3-select.html","mainEntityOfPage":{"@type":"WebPage","@id":"https://scaleoutsean.github.io/2022/03/04/storagegrid-s3-select.html"},"description":"About S3 Select and other new features in StorageGRID 11.6","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  </head>

<body>

  <div class="container">
    <header id="header">
	<div id = site_title>
		<a href="https://scaleoutsean.github.io/">
			<h2 style="font-size:1.8em;">Acting Technologist</h2>
		</a>
		
		<h3>
			civilizations are created by individuals
		</h3>
		
	</div>

	<div id="subheader">
		
		<nav class="pages">
<a href="/about.html">About</a>

<a href="/archive.html">Archive</a>

<a href="/categories/">Categories</a>

<a href="/projects.html">Projects</a>
</nav>
		
		
		<nav class="social">
			
  
    <a href="https://www.github.com/scaleoutsean" target="_blank" id="github"><i class="fab fa-github" aria-hidden="true"></i></a>
  
   

  
    <a href="https://twitter.com/scaleoutsean" target="_blank" id="twitter"><i class="fab fa-twitter" aria-hidden="true"></i></a>
  
   

  
  
    <a href="/feed.xml" target="_blank" id="rss"><i class="fas fa-rss" aria-hidden="true"></i></a>
   

		</nav>
		
	</div>
</header>


    <div class="post-container">
      <article id = "post">
        <h1 id = "post-title">S3 Select and other new features of NetApp StorageGRID 11.6</h1>

  
  <div class = "post-info">
    <span>
        <i class="far fa-calendar" aria-hidden="true"></i> <span>04 Mar 2022</span> - <i class="far fa-clock"></i> 


  
  
    12 minute read
  

    </span>
  </div>
  
        <!-- TOC -->

<ul>
  <li><a href="#whats-new-in-storagegrid-116">What’s new in StorageGRID 11.6</a></li>
  <li><a href="#s3-select">S3 Select</a>
    <ul>
      <li><a href="#example-1-basic">Example 1: Basic</a></li>
      <li><a href="#example-2-network-efficient-log-grep">Example 2: Network-efficient log grep</a></li>
      <li><a href="#example-3-brute-force-sizer-for-solidfire">Example 3: Brute Force Sizer for SolidFire</a></li>
      <li><a href="#what-you-need-to-know-when-using-storagegrid-s3-select-feature">What you need to know when using StorageGRID S3 Select feature</a></li>
    </ul>
  </li>
  <li><a href="#storagegrid-log-forwarding">StorageGRID log forwarding</a></li>
  <li><a href="#demos">Demos</a></li>
</ul>

<!-- /TOC -->

<h2 id="whats-new-in-storagegrid-116">What’s new in StorageGRID 11.6</h2>

<p>StorageGRID 11.6 just came out and it has several very nice new features. You can find them in <a href="https://docs.netapp.com/us-en/storagegrid-116/">the documentation</a> (finally, it’s on Github Pages!) or <a href="https://community.netapp.com/t5/Tech-ONTAP-Blogs/What-s-new-in-the-upcoming-StorageGRID-11-6-release/ba-p/432709">read the announcement</a>.</p>

<p>I’ll focus on two of the new features, S3 Select and log forwarding.</p>

<h2 id="s3-select">S3 Select</h2>

<p>AWS <a href="https://aws.amazon.com/s3/features/#Data_processing">S3 Select</a> has been around for a while and many AWS S3 users know about it.</p>

<p>Here’s how AWS put it:</p>

<blockquote>
  <p>Amazon S3 has a built-in feature and complementary services that query data without needing to copy and load it into a separate analytics platform or data warehouse. This means you can run big data analytics directly on your data stored in Amazon S3. S3 Select is an S3 feature designed to increase query performance by up to 400%, and reduce querying costs as much as 80%. It works by retrieving a subset of an object’s data (using simple SQL expressions) instead of the entire object, which can be up to 5 terabytes in size.</p>
</blockquote>

<p>In other words S3 Select turns S3 into a read-only database. Instead of downloading an entire table (object without S3) or set of tables, you query it with SQL-like syntax.</p>

<p>Another comparison would be instead of importing data into a NOSQL DB and having to maintain a separate DB cluster, simpler queries can be done directly off S3. Of course, S3 Select can’t and won’t fully replace HBase and such, but just moving 10% of data off HDFS to S3 can translate into nice savings for a PB sized deployment. Not to mention that you can run such queries against StorageGRID from any location in your organization or even the public cloud. Heck, you can even run S3 analytics from your existing Hadoop cluster (S3A) until you retire it!</p>

<p>StorageGRID 11.6 introduces a subset of AWS S3 Select features. You can see the details related to SQL grammar and more in the documentation.</p>

<p>What is that good for? Look at the AWS S3 Select use cases and examples and see what others are doing with the feature.</p>

<h3 id="example-1-basic">Example 1: Basic</h3>

<p>Let’s consider this basic example (<a href="https://aws.amazon.com/blogs/storage/querying-data-without-servers-or-databases-using-amazon-s3-select/">source</a>):</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">resp</span> <span class="o">=</span> <span class="n">s3</span><span class="p">.</span><span class="n">select_object_content</span><span class="p">(</span>
    <span class="n">Bucket</span><span class="o">=</span><span class="s">'s3select-demo'</span><span class="p">,</span>
    <span class="n">Key</span><span class="o">=</span><span class="s">'sample_data.csv'</span><span class="p">,</span>
    <span class="n">ExpressionType</span><span class="o">=</span><span class="s">'SQL'</span><span class="p">,</span>
    <span class="n">Expression</span><span class="o">=</span><span class="s">"SELECT * FROM s3object s where s.</span><span class="se">\"</span><span class="s">Name</span><span class="se">\"</span><span class="s"> = 'Jane'"</span><span class="p">,</span>
    <span class="n">InputSerialization</span> <span class="o">=</span> <span class="p">{</span><span class="s">'CSV'</span><span class="p">:</span> <span class="p">{</span><span class="s">"FileHeaderInfo"</span><span class="p">:</span> <span class="s">"Use"</span><span class="p">},</span> <span class="s">'CompressionType'</span><span class="p">:</span> <span class="s">'NONE'</span><span class="p">},</span>
    <span class="n">OutputSerialization</span> <span class="o">=</span> <span class="p">{</span><span class="s">'CSV'</span><span class="p">:</span> <span class="p">{}},</span>
<span class="p">)</span>
</code></pre></div></div>

<p>Without this approach maybe you’d open this stuff in Excel and create some queries there. Now you can have this in a JavaScript-powered dashboard.</p>

<p>Or you’d copy (from NAS) or get (from S3) <em>the entire</em> file/object, and parse it with Python or shell script to get only 2-3 rows of results.</p>

<p>Or you’d stuff such CSV files into a DB which can turn out to be a very expensive and slow container for such data. And then you’d have to backup and maintain the database and application rather than pay cents per GB for S3-compatible storage that requires almost no management and usually doesn’t have to be backed-up or replicated while still providing solid performance and availability for workloads like this.</p>

<p>With S3 Select, you can do all the reads on the remote S3 storage and just retrieve the result, often saving huge amounts of network bandwidth and time.</p>

<p>And this isn’t just the time to get the result out, but also time to make data usable: whereas with other approaches you may need to download objects, copy them over WAN and maybe even multiple hosts. Here, once data is on StorageGRID, you can securely query it from anywhere - the HQs, Branch 1, Branch 2, your K8s in the Public Cloud….</p>

<p>Some verticals and use cases where S3 Select could be useful:</p>

<ul>
  <li>Verticals: manufacturing, finance, life sciences and more</li>
  <li>Use cases: analytics, data warehousing, hybrid cloud analytics, SIEM, etc.</li>
</ul>

<h3 id="example-2-network-efficient-log-grep">Example 2: Network-efficient log grep</h3>

<p>I know, I know… There are packaged, feature-rich applications for this problem and I’ve used them too. But let’s say your requirements are basic: you upload logs to S3 where you retain them for simple troubleshooting. Maybe you’re into simple data exploration and your organization won’t stand up or otherwise maintain a variety of applications that could make this easier.</p>

<p>Or maybe it’s logs from machines in your factory. Maybe you’re interested in data that matches a certain time span.</p>

<p>With S3 Select it’s easy to make queries - both from scripts and browsers - to select such data from a large set of objects. See the first video demo (at the bottom) on how I query a StorageGRID audit log to find all access between two time stamps.</p>

<h3 id="example-3-brute-force-sizer-for-solidfire">Example 3: Brute Force Sizer for SolidFire</h3>

<p>I pre-recorded Examples 1 &amp; 2 while StorageGRID was still in beta, but today I happened to have some extra time <em>and</em> I also had an idea (both ingridients are required for a good demo). The idea came from yesterday’s experience when I spent 2 hours trying to:</p>

<ul>
  <li>figure out how various terms and units in our monitoring and sizing tools map to each other</li>
  <li>find an optimal sizing for a SolidFire hardware refresh</li>
</ul>

<p>It occurred to me that - because SolidFire appliance models are currently three and a cluster can consist of up to 40 nodes - the number of all possible configurations should be about right for this use case.</p>

<p>First I wrote a script to create all possible combinations, while respecting some criteria:</p>

<ul>
  <li>no cluster should have less than 4 nodes</li>
  <li>no node should have more capacity than 33% of total cluster capacity</li>
  <li>no cluster should have more than 40 nodes</li>
</ul>

<p>Storage efficiency of 3x was hardcoded in the script because I wasn’t too concerned about it (I wanted to size based on usable).</p>

<p>By the time I was done, I had something like this:</p>

<p><img src="/assets/images/solidfire-brute-force-sizing-prepare-00.png" alt="BFSS configuration generator" /></p>

<p>Or, if that is not easy to read:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">cat </span>bruteForceSizingSolidFire.txt
ComboID,TotalNodes,SmallNodes,MediumNodes,LargeNodes,TotalCapTB,UsableCapTB,UsableCapHATB,EffectiveCapTB,EffectiveCapHATB
1,4,0,0,4,168.96,84.48,63.36,253.44,190.08
2,5,0,0,5,211.2,105.6,84.48,316.8,253.44
3,6,0,0,6,253.44,126.72,105.6,380.16,316.8
4,7,0,0,7,295.68,147.84,126.72,443.52,380.16
...
12299,40,38,2,0,443.52,221.76,211.2,665.28,633.6
12300,39,39,0,0,411.84,205.92,200.64,617.76,601.92
12301,40,39,0,1,454.08,227.04,205.92,681.12,617.76
12302,40,39,1,0,432.96,216.48,205.92,649.44,617.76
12303,40,40,0,0,422.4,211.2,205.92,633.6,617.76

</code></pre></div></div>

<p>Field names show that we calculate total capacity, usable capacity, effective capacity, and also cover some “N-1” scenarios in the case the largest sized node in the cluster fails. The entire “universe” of possible valid combinations was 12303 and this “flat file database” size was around 600kB - just about right for my purpose!</p>

<p>I uploaded that file to a bucket on StorageGRID 11.6.</p>

<p>Next I needed a sizing tool.</p>

<p>I wrote the above in Powershell and had half a mind to do the same for the sizing tool but it was getting late so I recycled Amazon’s example script but made it a bit more “real life” - not all variables were hardcoded.</p>

<p>Brute Force Sizer for SolidFire is similar to previous examples except that it takes three arguments each of which has a default value and obviously SQL queries are different.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>./bfsizing.py <span class="nt">-h</span>
usage: bfsizing.py <span class="o">[</span><span class="nt">-h</span><span class="o">]</span> <span class="o">[</span><span class="nt">-min</span> MINIMUM] <span class="o">[</span><span class="nt">-max</span> MAXIMUM] <span class="o">[</span><span class="nt">-n</span> NODES]

optional arguments:
  <span class="nt">-h</span>, <span class="nt">--help</span>            show this <span class="nb">help </span>message and <span class="nb">exit</span>
  <span class="nt">-min</span> MINIMUM, <span class="nt">--minimum</span> MINIMUM
                        Minimum usable capacity <span class="k">in </span>TB <span class="o">(</span>default: 10<span class="o">)</span>
  <span class="nt">-max</span> MAXIMUM, <span class="nt">--maximum</span> MAXIMUM
                        Maximum usable capacity <span class="k">in </span>TB <span class="o">(</span>default: 50<span class="o">)</span>
  <span class="nt">-n</span> NODES, <span class="nt">--nodes</span> NODES
                        Maximum number of SolidFire nodes <span class="o">(</span>default: 10<span class="o">)</span>
</code></pre></div></div>

<p>All right! Let’s fire up this sucker…</p>

<p>I’m looking for 25 nodes or less and targeting usable capacity between 120-130 TB:</p>

<p><img src="/assets/images/solidfire-brute-force-sizing-with-s3-select-00.png" alt="BFSS for 120-130TB usable" /></p>

<p>I didn’t put any effort into formatting query output - after all it’s just a PoC script and it’s not difficult to figure it out by reading CSV.</p>

<p>Second result below (configuration ID #78), for example, proposes 7 nodes (2 medium, 5 large) with 126.72 TB usable.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">time</span> ./bfsizing.py <span class="nt">-n</span> 25 <span class="nt">--minimum</span> 120 <span class="nt">--maximum</span> 130
Query: SELECT <span class="k">*</span> FROM s3object s where s.UsableCapTB <span class="o">&gt;</span> 120.0 AND s.UsableCapTB &lt;<span class="o">=</span> 130.0 AND s.TotalNodes &lt;<span class="o">=</span> 25
3,6,0,0,6,253.44,126.72,105.6,380.16,316.8
78,7,0,2,5,253.44,126.72,105.6,380.16,316.8
...
10534,21,20,0,1,253.44,126.72,105.6,380.16,316.8
10574,22,20,2,0,253.44,126.72,116.16,380.16,348.48
10784,22,21,1,0,242.88,121.44,110.88,364.32,332.64
10993,23,22,1,0,253.44,126.72,116.16,380.16,348.48
11164,23,23,0,0,242.88,121.44,116.16,364.32,348.48
11335,24,24,0,0,253.44,126.72,121.44,380.16,364.32

Stats details bytesScanned:
618203
Stats details bytesProcessed:
618203
Stats details bytesReturned:
4415

real    0m1.794s
user    0m0.319s
sys     0m0.069s
</code></pre></div></div>

<p>This run took just 1.7 seconds. Normally I’d have to click around a Web based sizer for 5 minutes to get 5 good alternatives.</p>

<p>But my real desired number of nodes was 9-24 because I wanted to split them in three SolidFire <a href="/2021/07/06/solidfire-protection-domains-data-path.html">Protection Domains</a>. So I scrolled towards the end (the CSV file happened to be generated in ascending number of small nodes). Although I could have provided different output sorting options in the script, I didn’t think I needed that.</p>

<p><img src="/assets/images/solidfire-brute-force-sizing-with-s3-select-01.png" alt="BFSS for Protection Domains with 120 to 130 TB" /></p>

<p>While this is a toy project that would be better with few additional arguments and options (sort by (variable), set both min and max number of nodes, set storage efficiency, maybe even relative cost factor) it’s already useful enough for some sizing scenarios.</p>

<p>With 24 nodes and one failed Protection Domain (i.e. 16 nodes from the other two Protection Domain still up and running) we’d get 1.6 million IOPS (16 *100K, as all current SolidFire nodes have the same performance rating).</p>

<p>At this point I was happy with this “micro application” - I could get multiple valid sizing combinations in less than 2 seconds. Compare that to having to run a Web sizing tool for the fifth or sixth time… Not fun!</p>

<p>And remember - nothing prevents you from copying that CSV result and pasting it into a spreadsheet for easy analysis.</p>

<p><img src="/assets/images/solidfire-brute-force-sizing-with-s3-select-02.png" alt="BFSS in spreadsheet" /></p>

<p>If I wanted a lower cost and if 1 million IOPS total was enough, I could have examined configs with a lower node count (maybe 18 nodes rather than 24).</p>

<p>If we used the sizer often we could download the CSV file from the Web and use it from Excel, but not all data is static and your CSV file could involve 1,200,000 rows rather than 12,000. Also, this approach eliminates the need for continous data distribution and downloads. Even the sizer script itself could be downloaded from the same S3 bucket.</p>

<p>I haven’t done enough checking for correctness, so even if I used this I’d still verify BFSS’s results in an official sizing tool.</p>

<p>A video demo of Brute Force Sizer for SolidFire is also available at the bottom of this page.</p>

<h3 id="what-you-need-to-know-when-using-storagegrid-s3-select-feature">What you need to know when using StorageGRID S3 Select feature</h3>

<p>This is from the StorageGRID 11.6 <a href="https://docs.netapp.com/us-en/storagegrid-116/">documentation</a> so you can find about it in more detail there. I also added some comments (you can easily tell them apart, I hope).</p>

<ul>
  <li>SelectObject calls aren’t impacted by StorageGRID consistency controls</li>
  <li>The tenant account has to have the S3 Select permission. This will probably confuse some users who don’t know how StorageGRID is administered and attempt to use S3 Select without it being enabled for their tenant account</li>
  <li>Users need s3:GetObject permission for the object they want to query</li>
  <li>Supported formats may be different from AWS S3: in version 11.6 that’s CSV, either raw or compressed as GZIP or BZIP2</li>
  <li>SQL expression has a maximum length of 256 KB (for me this is more than enough - I can barely compose a 100 byte query)</li>
  <li>Any record in the input or results has a maximum length of 1 MiB (not too bad - if you result is &gt; 1 MiB maybe you could run multiple queries or simply download the objects and compute locally)</li>
</ul>

<p>If you want to query S3 data but not with SQL select, you can do that with Spark, Presto, Hadoop and other applications that implement their own client-side data access libraries over S3 API. See <a href="/2022/01/28/storagegrid-hybrid-cloud-processing-without-data-at-rest.html">this</a> for some examples.</p>

<h2 id="storagegrid-log-forwarding">StorageGRID log forwarding</h2>

<p>Version 11.6 can forward both system logs and audit logs to an <a href="https://docs.netapp.com/us-en/storagegrid-116/monitor/configuring-syslog-server.html">external syslog server</a>.</p>

<p>There are several additional ways to forward logs (to StorageGRID Admin nodes, etc.), so this feature now probably covers over 95% of ways people need in order to better handle StorageGRID logs.</p>

<p>I think this feature was very nicely implemented.</p>

<p>What it does not do for StorageGRID <strong>audit</strong> log, though, is change the <strong>format</strong> of the StorageGRID audit log. This means you still need to parse and transform the log if you want to easily search and analyze it.</p>

<p>I (re)wrote an <a href="/2021/10/20/sgac-storagegrid-audit-log-converter-v0.2.1.html">audit log converter for StorageGRID</a> which StorageGRID users with version 11.0 - 11.5 can use to ingest StorageGRID logs into Elastic or elsewhere. It converts the log into JSON file.</p>

<p>We can use Filebeat to send processed logs in JSON format to a forwarder such as Logstash and from there to Elastic or other sink.</p>

<p>For 11.6 we’d want to transform the log to JSON on the fly, so we’d have to unpack the entries on-the fly, possibly by collapsing the two steps into one and do everything in Logstash. SGAC gets a few downloads every month but I haven’t gotten any feedback for it so I’m not sure if I’ll update SGAC for StorageGRID 11.6 and when.</p>

<p>Incidentally, the second example in the shortest (listed first) demo video below runs S3 Select against StorageGRID audit log file, if you’re interested in S3 Select in the context of log search.</p>

<p>In version 11.6, StorageGRID <strong>service</strong> logs can also be forwarded but I haven’t tried to parse them yet.</p>

<h2 id="demos">Demos</h2>

<p>The first and shortest video shows examples with two CSV files (basic and “remote grep”). In each case S3-selecting a small subset of object data results in big egress savings. The second is Brute Force Sizer for SolidFire and the third is an official demo.</p>

<ul>
  <li><a href="https://rumble.com/vwax3p-s3-select-in-storagegrid-11.6.html">S3 Select in StorageGRID 11.6</a> - 1m22s</li>
  <li><a href="https://rumble.com/vwfaul-brute-force-sizing-for-solidfire-using-s3-select-with-storagegrid.html">Brute Force Sizer for SolidFire with S3 Select</a> - 2m16s</li>
  <li>Longer, “professional” <a href="https://www.netapp.tv/player/28903/stream?assetType=movies">S3 Select demo</a> with S3 Select in Jupyter at NetApp TV - 4m21s</li>
</ul>

      </article>
    </div>
    
      <div class="categories">
    <span><p>Categories:</p>
    
    
      <a href="
      /categories/#storage">storage</a>
      &nbsp; 
    
      <a href="
      /categories/#analytics">analytics</a>
       
    
  </span>
</div>
    

  

    
  </div><footer class= "footer">
    <p>2024-10-17 18:15 </p>
    <p>Copyright © 2024 scaleoutSean. Content is released under the CC BY license. Design: Alessio Franceschi</p>
</footer>

</body>
</html>
