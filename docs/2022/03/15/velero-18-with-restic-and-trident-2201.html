<!doctype html>
<html lang="en">
  <head>
        <meta charset="UTF-8">
    <meta http-equiv="content-language" content="en">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="googlebot" content="noarchive">
    <meta name="googlebot" content="max-snippet:60">
    <meta name="googlebot" content="max-image-preview:small">
    <meta name="bingbot" content="noarchive">
    <meta name="bingbot" content="max-snippet:60">
    <meta name="bingbot" content="max-image-preview:small">
    <meta name="robots" content="noarchive">
    <meta name="robots" content="max-snippet:60">
    <meta name="google-site-verification" content="F6q7vIwQ2G0j8tk-KL9rAXOMLXMDMMUkEz4fRs1Nnnc" />
    <title>
        
            Velero V1.8 with Restic, SolidFire 12.3 and StorageGRID 11.5 | Acting Technologist
      
    </title>
    <meta name="description" content="
     Update of Restic-driven backup with Velero 1.8, SolidFire 12.3 and StorageGRID 11
     ">

    <!-- LINK TO ATOM FEED FOR SEO -->
    <link rel="alternate" type="application/atom+xml" href="https://scaleoutsean.github.io/feed.xml" />

    <!-- FAVICON -->
    <link rel="icon" href="/assets/favicon.ico" type="image/x-icon">
    <link rel="shortcut icon" href="/assets/favicon.ico" type="image/x-icon">

    <!-- CSS -->
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="stylesheet" href="/assets/css/syntax.css">

    <!-- Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css">

    <link href="/_pagefind/pagefind-ui.css" rel="stylesheet">
    <script src="/_pagefind/pagefind-ui.js" type="text/javascript"></script>
    <div id="search"></div>
    <script src="https://cdn.counter.dev/script.js" data-id="83dc700a-b821-4e57-9425-02b8336a9456" data-utcoffset="0"></script>
    <script>
        window.addEventListener('DOMContentLoaded', (event) => {
            new PagefindUI({ element: "#search" });
        });
    </script>

    <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Velero V1.8 with Restic, SolidFire 12.3 and StorageGRID 11.5 | Acting Technologist</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Velero V1.8 with Restic, SolidFire 12.3 and StorageGRID 11.5" />
<meta name="author" content="scaleoutSean" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Update of Restic-driven backup with Velero 1.8, SolidFire 12.3 and StorageGRID 11" />
<meta property="og:description" content="Update of Restic-driven backup with Velero 1.8, SolidFire 12.3 and StorageGRID 11" />
<link rel="canonical" href="https://scaleoutsean.github.io/2022/03/15/velero-18-with-restic-and-trident-2201.html" />
<meta property="og:url" content="https://scaleoutsean.github.io/2022/03/15/velero-18-with-restic-and-trident-2201.html" />
<meta property="og:site_name" content="Acting Technologist" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-03-15T00:00:00+08:00" />
<script type="application/ld+json">
{"headline":"Velero V1.8 with Restic, SolidFire 12.3 and StorageGRID 11.5","dateModified":"2022-03-15T00:00:00+08:00","datePublished":"2022-03-15T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://scaleoutsean.github.io/2022/03/15/velero-18-with-restic-and-trident-2201.html"},"author":{"@type":"Person","name":"scaleoutSean"},"@type":"BlogPosting","url":"https://scaleoutsean.github.io/2022/03/15/velero-18-with-restic-and-trident-2201.html","description":"Update of Restic-driven backup with Velero 1.8, SolidFire 12.3 and StorageGRID 11","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  </head>

<body>

  <div class="container">
    <header id="header">
	<div id = site_title>
		<a href="https://scaleoutsean.github.io/">
			<h2 style="font-size:1.8em;">Acting Technologist</h2>
		</a>
		
		<h3>
			civilizations are created by individuals
		</h3>
		
	</div>

	<div id="subheader">
		
		<nav class="pages">
<a href="/about.html">About</a>

<a href="/archive.html">Archive</a>

<a href="/categories/">Categories</a>

<a href="/projects.html">Projects</a>
</nav>
		
		
		<nav class="social">
			
  
    <a href="https://www.github.com/scaleoutsean" target="_blank" id="github"><i class="fab fa-github" aria-hidden="true"></i></a>
  
   

  
    <a href="https://twitter.com/scaleoutsean" target="_blank" id="twitter"><i class="fab fa-twitter" aria-hidden="true"></i></a>
  
   

  
  
    <a href="/feed.xml" target="_blank" id="rss"><i class="fas fa-rss" aria-hidden="true"></i></a>
   

		</nav>
		
	</div>
</header>


    <div class="post-container">
      <article id = "post">
        <h1 id = "post-title">Velero V1.8 with Restic, SolidFire 12.3 and StorageGRID 11.5</h1>

  
  <div class = "post-info">
    <span>
        <i class="far fa-calendar" aria-hidden="true"></i> <span>15 Mar 2022</span> - <i class="far fa-clock"></i> 


  
  
    14 minute read
  

    </span>
  </div>
  
        <p><strong>NOTICE:</strong> all credentials and tokens on this page are samples, not leaked.</p>

<ul>
  <li><a href="#introduction">Introduction</a></li>
  <li><a href="#backup-and-restore-kubernetes-application-data-and-metadata-to-s3">Backup and restore Kubernetes application data and metadata to S3</a></li>
  <li><a href="#is-this-good-enough-or-">Is this “good enough” or …</a></li>
  <li><a href="#using-velero-and-restic-to-backup-regular-solidfire-volumes">Using Velero and Restic to backup regular SolidFire volumes</a></li>
  <li><a href="#summary">Summary</a></li>
</ul>

<h2 id="introduction">Introduction</h2>

<p>I wrote about Velero &amp; Trident CSI (also about <a href="/2021/02/02/use-velero-with-netapp-storagegrid.html">Velero and StorageGRID</a>) before, but since Velero V1.8 came out last month I’ve been wanting to write about it again.</p>

<p>My setup consists of one of the Kubernetes clusters used in previous post <a href="/2022/03/06/elastic-elk-stack-on-netapp.html">Elasticsearch 8 with NetApp storage</a>), a three-node vanilla Kubernetes v1.23.4 running on ARM64 hardware. I looked forward to using <a href="/2022/03/02/openstack-solidfire-part-2.html">OpenStack-based Kubernetes cluster with Cinder CSI</a> for this (specifically because Trident CSI isn’t required for Restic-based backups), but due to a power loss incident yesterday part of my home lab was corrupted. So now I have to mention again that NetApp Trident currently works with x86_64 systems and other architectures can work if you <a href="/2021/02/24/netapp-trident-on-arm64.html">build it from source</a> by yourself.</p>

<p>In any case, you need a Kubernetes cluster and Trident CSI isn’t mandatory in this case because this post is about the non-CSI approach. You can also try the <a href="/2021/02/08/use-velero-with-netapp-solidfire-and-trident-csi.html">CSI approach</a> if you’re interested in that.</p>

<h2 id="backup-and-restore-kubernetes-application-data-and-metadata-to-s3">Backup and restore Kubernetes application data and metadata to S3</h2>

<ul>
  <li>Prepare a bucket on StorageGRID and store your bucket credentials into a credentials file (or otherwise pass them to velero install command). My bucket is solidfire-velero and my credentials file is sg.velero:</li>
</ul>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>default]
<span class="nv">aws_access_key_id</span><span class="o">=</span>AAAAAAAAAAAAAAAAAAA
<span class="nv">aws_secret_access_key</span><span class="o">=</span>BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB
</code></pre></div></div>

<ul>
  <li>
    <p>Deploy Trident CSI and create a storage class to be used by Kubernetes pods. If Trident CSI is used, obviously you can use any back-end, whether it’s SolidFire or any of several supported ONTAP-based products and services. E-Series users could try BeeGFS CSI.</p>
  </li>
  <li>
    <p><a href="https://velero.io/docs/v1.8/basic-install/">Install Velero V1.8</a> by telling it how you want it to work, where to find S3 credentials, what bucket you want to use and some S3-specific settings. Again, I’m not using volume snapshots here. That’s why I <code class="language-plaintext highlighter-rouge">--use-restic</code> and do not <code class="language-plaintext highlighter-rouge">--use-volume-snapshots</code>.</p>
  </li>
</ul>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>velero <span class="nb">install</span> <span class="se">\</span>
  <span class="nt">--provider</span> aws <span class="se">\</span>
  <span class="nt">--secret-file</span> ./sg.velero <span class="se">\</span>
  <span class="nt">--plugins</span> velero/velero-plugin-for-aws:v1.3.1 <span class="se">\</span>
  <span class="nt">--bucket</span> solidfire-velero <span class="se">\</span>
  <span class="nt">--use-restic</span> <span class="se">\</span>
  <span class="nt">--restic-pod-cpu-request</span><span class="o">=</span>1000m <span class="se">\</span>
  <span class="nt">--restic-pod-cpu-limit</span><span class="o">=</span>5000m <span class="se">\</span>
  <span class="nt">--restic-pod-mem-request</span><span class="o">=</span>512Mi <span class="se">\</span>
  <span class="nt">--restic-pod-mem-limit</span><span class="o">=</span>1024Mi <span class="se">\</span>
  <span class="nt">--use-volume-snapshots</span><span class="o">=</span><span class="nb">false</span> <span class="se">\</span>
  <span class="nt">--backup-location-config</span> <span class="nv">region</span><span class="o">=</span>us-east-1,s3ForcePathStyle<span class="o">=</span><span class="s2">"true"</span>,s3Url<span class="o">=</span>https://s3.com.org
</code></pre></div></div>

<p>Next, create a backup and try to restore it.</p>

<p>Here there aren’t any NetApp-specific steps. I deployed Elasticsearch 8.0.1 (see the linked Elasticsearch on NetApp storage post for more on that), but this time into its own namespace. That resulted in a Stateful Set with one Elasticsearch pod (I had only one) in the namespace elastic-system. It also automatically created a PVC that landed on a PV on SolidFire.</p>

<p>I annotated the Elasticsearch pod to have it included in backup.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl describe pod/elasticsearch-master-0 <span class="nt">-n</span> elastic-system | <span class="nb">grep </span>Annotations
Annotations:  backup.velero.io/backup-volumes: elasticsearch-master-elasticsearch-master-0
</code></pre></div></div>

<p>Then I created a backup. Annotations make it easier to exclude PV’s and other resources that we do not want to have backed up.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>velero backup create elasticsearch-demo <span class="nt">--include-namespaces</span> elastic-system
Backup request <span class="s2">"elasticsearch-demo"</span> submitted successfully.
Run <span class="sb">`</span>velero backup describe elasticsearch-demo<span class="sb">`</span> or <span class="sb">`</span>velero backup logs elasticsearch-demo<span class="sb">`</span> <span class="k">for </span>more details.
</code></pre></div></div>

<p>Logs indicate the pod and PV data were backed up:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>velero backup logs elasticsearch-demo
<span class="nb">time</span><span class="o">=</span><span class="s2">"2022-03-15T06:24:26Z"</span> <span class="nv">level</span><span class="o">=</span>info <span class="nv">msg</span><span class="o">=</span><span class="s2">"Setting up backup temp file"</span> <span class="nv">backup</span><span class="o">=</span>velero/elasticsearch-demo <span class="nv">logSource</span><span class="o">=</span><span class="s2">"pkg/controller/backup_controller.go:556"</span>
...
<span class="nb">time</span><span class="o">=</span><span class="s2">"2022-03-15T06:24:27Z"</span> <span class="nv">level</span><span class="o">=</span>info <span class="nv">msg</span><span class="o">=</span><span class="s2">"Backing up all pod volumes using restic: false"</span> <span class="nv">backup</span><span class="o">=</span>velero/elasticsearch-demo <span class="nv">logSource</span><span class="o">=</span><span class="s2">"pkg/backup/backup.go:228"</span>
...
<span class="nb">time</span><span class="o">=</span><span class="s2">"2022-03-15T06:24:35Z"</span> <span class="nv">level</span><span class="o">=</span>info <span class="nv">msg</span><span class="o">=</span><span class="s2">"Backed up 33 items out of an estimated total of 33 (estimate will change throughout the backup)"</span> <span class="nv">backup</span><span class="o">=</span>velero/elasticsearch-demo <span class="nv">logSource</span><span class="o">=</span><span class="s2">"pkg/backup/backup.go:399"</span> <span class="nv">name</span><span class="o">=</span>elasticsearch-master-headless-5swbj <span class="nv">namespace</span><span class="o">=</span>elastic-system <span class="nv">progress</span><span class="o">=</span> <span class="nv">resource</span><span class="o">=</span>endpointslices.discovery.k8s.io
<span class="nb">time</span><span class="o">=</span><span class="s2">"2022-03-15T06:24:36Z"</span> <span class="nv">level</span><span class="o">=</span>info <span class="nv">msg</span><span class="o">=</span><span class="s2">"Backed up a total of 33 items"</span> <span class="nv">backup</span><span class="o">=</span>velero/elasticsearch-demo <span class="nv">logSource</span><span class="o">=</span><span class="s2">"pkg/backup/backup.go:424"</span> <span class="nv">progress</span><span class="o">=</span>
</code></pre></div></div>

<p>Backups:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>velero backup get
NAME                 STATUS      ERRORS   WARNINGS   CREATED                         EXPIRES   STORAGE LOCATION   SELECTOR
elasticsearch-demo   Completed   0        1          2022-03-15 06:24:26 +0000 UTC   29d       default            &lt;none&gt;
</code></pre></div></div>

<p>Some interesting parts from the job details (abbreviated output):</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>velero backup describe elasticsearch-demo <span class="nt">--details</span>
Name:         elasticsearch-demo
Namespace:    velero
Labels:       velero.io/storage-location<span class="o">=</span>default
Annotations:  velero.io/source-cluster-k8s-gitversion<span class="o">=</span>v1.23.4
              velero.io/source-cluster-k8s-major-version<span class="o">=</span>1
              velero.io/source-cluster-k8s-minor-version<span class="o">=</span>23

Phase:  Completed

Errors:    0
Warnings:  1

Namespaces:
  Included:  elastic-system
  Excluded:  &lt;none&gt;

Resources:
  Included:        <span class="k">*</span>
  Excluded:        &lt;none&gt;
  Cluster-scoped:  auto

Label selector:  &lt;none&gt;

Storage Location:  default

Velero-Native Snapshot PVs:  auto

&lt;SNIP&gt;

Backup Format Version:  1.1.0

Started:    2022-03-15 06:24:26 +0000 UTC
Completed:  2022-03-15 06:24:36 +0000 UTC

Expiration:  2022-04-14 06:24:26 +0000 UTC

Total items to be backed up:  33
Items backed up:              33

Resource List:
  
  &lt;SNIP&gt;

  v1/Namespace:
    - elastic-system
  v1/PersistentVolume:
    - pvc-88918eb7-c4e4-4838-bc42-936a93689be6
  v1/PersistentVolumeClaim:
    - elastic-system/elasticsearch-master-elasticsearch-master-0
  v1/Pod:
    - elastic-system/elasticsearch-master-0
  &lt;SNIP&gt;

Velero-Native Snapshots: &lt;none included&gt;
</code></pre></div></div>

<p>As you can see, PVC and PV were backed up without snapshots.</p>

<p>The bucket solidfire-velero on StorageGRID got two “subdirectories” one “restic” which stores Restic backup data, and another “backups” where various metadata and K8s application backup data goes. List of objects in this second (K8s data and metadata) tree, solidfire-velero/backups/elasticsearch-demo:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>2022-03-15 14:24:37 CST]    29B elasticsearch-demo-csi-volumesnapshotcontents.json.gz
<span class="o">[</span>2022-03-15 14:24:38 CST]    29B elasticsearch-demo-csi-volumesnapshots.json.gz
<span class="o">[</span>2022-03-15 14:24:36 CST] 4.4KiB elasticsearch-demo-logs.gz
<span class="o">[</span>2022-03-15 14:24:38 CST]    29B elasticsearch-demo-podvolumebackups.json.gz
<span class="o">[</span>2022-03-15 14:24:38 CST]   502B elasticsearch-demo-resource-list.json.gz
<span class="o">[</span>2022-03-15 14:24:38 CST]    29B elasticsearch-demo-volumesnapshots.json.gz
<span class="o">[</span>2022-03-15 14:24:37 CST] 138KiB elasticsearch-demo.tar.gz
<span class="o">[</span>2022-03-15 14:24:36 CST] 2.0KiB velero-backup.json
</code></pre></div></div>

<p><img src="/assets/images/velero-restic-solidfire-storagegrid-bucket-listing.png" alt="Velero backup in StorageGRID S3 bucket" /></p>

<p>Now we can wipe Elasticsearch (the entire namespace, which would also destroy the PVC and PV in it) and try to restore everything from this backup. I also had a scheduled backup in place so I used a specific backup instance:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>velero restore create <span class="nt">--from-backup</span> elastic-daily-20220315010024
</code></pre></div></div>

<p>This recreated the namespace, PVC and PV on SolidFire. The new PVC was named the same, and Trident recreated PV that was destroyed before. We have new volume that’s visible in tridentctl and kubectl output.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>./tridentctl <span class="nt">-n</span> trident get backend 
+--------------------------+----------------+--------------------------------------+--------+---------+
|           NAME           | STORAGE DRIVER |                 UUID                 | STATE  | VOLUMES |
+--------------------------+----------------+--------------------------------------+--------+---------+
| solidfire_192.168.103.34 | solidfire-san  | 246879c3-3738-4893-8174-138e75d0610b | online |       1 |
+--------------------------+----------------+--------------------------------------+--------+---------+

<span class="nv">$ </span>kubectl get pvc <span class="nt">-n</span> elastic-system
NAME                                          STATUS   VOLUME                                     CAPACITY    ACCESS MODES   STORAGECLASS   AGE
elasticsearch-master-elasticsearch-master-0   Bound    pvc-88918eb7-c4e4-4838-bc42-936a93689be6   1953125Ki   RWO            iscsi-bronze   73m
</code></pre></div></div>

<h2 id="is-this-good-enough-or-">Is this “good enough” or …</h2>

<p>It depends.</p>

<p>First, a simplified approach like the one from this post may not work well for databases and database-like applications that span multiple volumes. If Elasticsearch Stateful Set spanned several pods, extra steps would be required to minimize RPO (such as freeze and unfreeze hooks).</p>

<p>Second, in some cases Velero would benefit from CSI snapshots, especially if we could take several snapshots at the same time (which SolidFire can do with Group Snapshots and ONTAP with Consistency Groups). Example:</p>

<p><img src="/assets/images/solidfire-group-snapshot.png" alt="SolidFire Group Snapshot" /></p>

<p>Third, about backup destinations. I used StorageGRID for this demo and have tried Velero with both StorageGRID and MinIO. The Velero documentation has convenient examples with Kubernetes-based MinIO, but backing up Kubernetes apps to MinIO running on the same Kubernetes cluster doesn’t sound very reassuring to me. I know MinIO can be installed elsewhere, but if one wants a reliable object store to keep their data for months or years, they may as well buy StorageGRID appliances and get worry-free object storage.</p>

<p>In some cases we’d probably be better off with the CSI approach (to the extent that it works in Velero - currently generic CSI support is still in beta; I haven’t tested CSI backup in V1.8 yet) and in other cases you may want to simply buy something that works, such as <a href="https://docs.netapp.com/us-en/astra-control-center/">NetApp Astra Control Center</a> and get everything you need to protect your business-critical K8s data and applications.</p>

<p>Last week I blogged about <a href="/2022/03/11/vmware-photon-iscsi-solidfire.html">VMware Photon OS with iSCSI and SolidFire</a> because Photon OS with iSCSI client would give us the ability to run Trident CSI on Photon OS and backup PV data with Velero CSI. vSphere CSI supports Velero, but that doesn’t use Trident CSI (it seems vSphere CSI Plugin is the only supported CSI provisioner, which can work with SolidFire (creates PVs as VMDKs on VMFS) and if you wanted to use Velero CSI with Trident you could, but without VMware support). So this remains another area of investigation into native Trident CSI integrations.</p>

<h2 id="using-velero-and-restic-to-backup-regular-solidfire-volumes">Using Velero and Restic to backup regular SolidFire volumes</h2>

<p>Last year I created a PoC project for SolidFire backup to S3 called “solidbackup” that uses SolidFire clones to make copies from SolidFire volume snapshots, and takes advantage of Restic or other backup utility (it’s customizable) to backup that clone-from-snapshot data to S3. It runs from Linux VM and can’t handle non-Linux filesystems.</p>

<p>One of the reasons I wanted to try Velero again was to understand if Velero scheduled backups could replace the bulk of solidbackup. While solidbackup targets non-Kubernetes workloads on Linux (KVM, generic Linux, Trident Docker volumes), it could be improved:</p>

<ul>
  <li>Create (or update) Solidfire clone volumes as usual (<a href="/2021/05/08/revisiting-solidbackup.html#create-configuration-and-keep-src--dst-clone-volumes-in-sync">this step</a> using solidsync script from solidbackup). solidsync could also run from Kubernetes.</li>
  <li>Then, instead of running backup from a dedicated VM (the next step in that post above), reassign clone volume ownership to a Trident account of a small Kubernetes cluster with Velero Restic (this involves just one cmdlet (Set-SFVolume -VolumeID 1 -AccountID 2) in PowerShell)</li>
  <li>Now that clone volume is owned by the Trident user, import it to Kubernetes</li>
</ul>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>tridentctl import volume &lt;backendName&gt; &lt;volumeName&gt; <span class="nt">--no-manage</span>
</code></pre></div></div>

<p>As an example, I have an Elasticsearch data volume from a Docker container from that recent post on Elasticsearch 8. Let’s pretend a took a cold snapshot or “smart” snapshot from which I can clone that data volume and name it “sb-test”. I need to reassign its tenant account ownership from “elk” (which is what Trident Docker Volume Plugin used) to “arm64” (which is what Trident CSI is using in this Velero environment).</p>

<p><img src="/assets/images/velero-restic-solidfire-solidbackup.png" alt="Clone a non-K8s volume for backup by Velero" /></p>

<p>This costs me almost nothing capacity-wise.</p>

<p>Now I can import that clone to a K8s namespace such as “solidbackup”.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">cat </span>pvc-basic-import-for-sb.yml 
<span class="nt">---</span>
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: sb-test
  namespace: solidbackup
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: iscsi-bronze-xfs

<span class="nv">$ </span>./tridentctl import volume solidfire_192.168.103.34 sb-test <span class="nt">-f</span> pvc-basic-import-for-sb.yml <span class="nt">--no-manage</span> <span class="nt">-n</span> trident
+------------------------------------------+---------+---------------+----------+--------------------------------------+--------+---------+
|                   NAME                   |  SIZE   | STORAGE CLASS | PROTOCOL |             BACKEND UUID             | STATE  | MANAGED |
+------------------------------------------+---------+---------------+----------+--------------------------------------+--------+---------+
| pvc-3e284148-e984-447e-b3c1-9080d68a943d | 3.0 GiB | iscsi-bronze  | block    | 246879c3-3738-4893-8174-138e75d0610b | online | <span class="nb">false</span>   |
+------------------------------------------+---------+---------------+----------+--------------------------------------+--------+---------+

<span class="nv">$ </span>kubectl get pvc <span class="nt">-n</span> solidbackup
NAME      STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
sb-test   Bound    pvc-3e284148-e984-447e-b3c1-9080d68a943d   3Gi        RWO            iscsi-bronze   24s

</code></pre></div></div>

<ul>
  <li>Create a dummy pod (I just used NGINX because it was handy, but it shouldn’t be something that shares data), attach it to the imported volume and annotate (abbreviated):</li>
</ul>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Name:         sb-pod
Namespace:    solidbackup
Priority:     0
Node:         k2/192.168.1.19
Start Time:   Tue, 15 Mar 2022 09:34:49 +0000
Labels:       &lt;none&gt;
Annotations:  backup.velero.io/backup-volumes: sb-test
...
Containers:
  sb-container:
    Container ID:   docker://9bbbf296ff8612b8d318f44aab29946f48c228e20fff6b308e0efc1784c66b3e
    Image:          nginx
    ...
    Mounts:
      /usr/share/nginx/html from sb-test <span class="o">(</span>rw<span class="o">)</span>
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-sgbhv <span class="o">(</span>ro<span class="o">)</span>
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  sb-test:
    Type:       PersistentVolumeClaim <span class="o">(</span>a reference to a PersistentVolumeClaim <span class="k">in </span>the same namespace<span class="o">)</span>
    ClaimName:  sb-test
    ReadOnly:   <span class="nb">false</span>
</code></pre></div></div>

<ul>
  <li>Create a Velero backup schedule for the clones to backup applications and data to StorageGRID S3</li>
</ul>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>velero backup create sb-demo <span class="nt">--include-namespaces</span> solidbackup
Backup request <span class="s2">"sb-demo"</span> submitted successfully.
Run <span class="sb">`</span>velero backup describe sb-demo<span class="sb">`</span> or <span class="sb">`</span>velero backup logs sb-demo<span class="sb">`</span> <span class="k">for </span>more details.

<span class="nv">$ </span>velero backup get 
NAME                 STATUS      ERRORS   WARNINGS   CREATED                         EXPIRES   STORAGE LOCATION   SELECTOR
elasticsearch-demo   Completed   0        1          2022-03-15 06:24:26 +0000 UTC   29d       default            &lt;none&gt;
sb-demo              Completed   0        1          2022-03-15 09:10:53 +0000 UTC   29d       default            &lt;none&gt;
</code></pre></div></div>

<p>The above backup command clearly wasn’t scheduled because I didn’t want to have a schedule, but a schedule could be created like this:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>velero schedule create sb-demo-daily <span class="nt">--include-namespaces</span> sb-demo <span class="nt">--schedule</span><span class="o">=</span><span class="s2">"0 1 * * *"</span>
</code></pre></div></div>

<p>Either way, that pod and persistent volume would be backed up. From detailed job description:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Resource List:
  ...
  v1/PersistentVolume:
    - pvc-3e284148-e984-447e-b3c1-9080d68a943d
  v1/PersistentVolumeClaim:
    - solidbackup/sb-test
  ...
Restic Backups:
  Completed:
    solidbackup/sb-pod: sb-test
</code></pre></div></div>

<p>So both a backup and restore completed successfully. Unlike the first Elasticsearch demo that was created from scratch and didn’t have a lot of data in it, this volume used to be attached to Elasticsearch that ran for hours contributing to growth of used space in the Velero backup bucket.</p>

<p><img src="/assets/images/velero-restic-solidfire-storagegrid-solidbackup-restic-data-00.png" alt="Velero Restic-backed bucket with 420 MB of data" /></p>

<p>Restic-related data directory for the job was populated with directories created by Restic:</p>

<p><img src="/assets/images/velero-restic-solidfire-storagegrid-solidbackup-restic-data-01.png" alt="Velero Restic backup data directory" /></p>

<p>Individual files from the compressed Elasticsearch data (we can’t see the original files because of Restic compression) is what drove backup bucket utilization up:</p>

<p><img src="/assets/images/velero-restic-solidfire-storagegrid-solidbackup-restic-data-02.png" alt="Velero Restic backup files" /></p>

<p>Something I hadn’t noticed last year is that restore jobs are logged in a restores “subdirectory” of the Velero bucket, here solidfire-velero/restores/sb-demo-20220315095143/:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>2022-03-15 17:55:46 CST] 1.9KiB restore-sb-demo-20220315095143-logs.gz
<span class="o">[</span>2022-03-15 17:55:46 CST]   244B restore-sb-demo-20220315095143-results.gz
</code></pre></div></div>

<p>Restore workflow wouldn’t be more complex than with original solidbackup. As solidbackup README.md says:</p>

<ul>
  <li>The first line of defense is volume snapshots (of the original volume)</li>
  <li>The second is recreate the volume from clone assigned to K8s, and assign it to the original Linux/KVM/Docker account</li>
  <li>The third is to restore data to a PV with Velero/Restic (similar to these screenshots right above), and then make a clone while assigning clone ownership to the Linux/KVM/Docker storage account</li>
</ul>

<p>Some of the benefits this would have compared to the all-in-one approach originally taken by solidbackup:</p>

<ul>
  <li>solidbackup users can get unified backup and restore for KVM, K8s, Docker with Velero - we just need to create and import clones to a Kubernetes cluster with Trident CSI</li>
  <li>solidsync jobs could run in Kubernetes, getting a better protection for SolidFire API credentials</li>
  <li>backup jobs would be Velero-based, giving you improved security and full auditing (use Kubernetes namespaces for clones from different teams, and per-backup-storage-location credentials to address security issues, and use ELK for auditing and logging)</li>
  <li>solidbackup cannot scale (it’s constrained by single VM performance), but Velero on Kubernetes can - we get better performance via scale-out (automatically add new worker nodes if you need to run more backup jobs at the same time)</li>
  <li>get self-service data protection with RBAC from both the CLI and UI, albeit a more complex workflow than what you get with commercial data protection software</li>
</ul>

<p>In short, this approach is the same as the original solidbackup approach, but moves everything - from the handling of secrets over logging to monitoring and auditing to Velero on Kubernetes, and provides a way to scale out backup and restore performance.</p>

<p>Only the first step (cloning of volumes in backup configuration file) needs to be done outside of Velero, but that is SolidFire API-specific and would have to be done anyway to make non-K8s volumes accessible to Velero. Current cloning script (solidsync) could run out of a Kubernetes container to create a fully Kubernetes-based backup solution for SolidFire. The only “cost” is extra utilization of metadata capacity on SolidFire, so as long as you’re below 40% of maximum metadata capacity (90% of clusters likely are), this approach can be useful.</p>

<p>Let’s not forget that any volume cloning and other API calls are registered in SolidFire API logs which can be sent to ELK for auditing, so that volume owners can always review what happened in each step, both before the clones were assigned to K8s (solidsync) and after (Velero/Restic). Everything can be logged and audited.</p>

<h2 id="summary">Summary</h2>

<p>Non-CSI Velero V1.8 feels more mature (documentation, behavior) and user-friendly than the version I tested over a year ago and has additional features compared to Velero V1.5 tested back then.</p>

<p>While the backup to S3 feature is almost identical to features from the Velero-with-StorageGRID post I did a while ago, now Velero can use per-back-end credentials which is interesting to general users but also niche use cases like solidbackup. It would be interesting to try StorageGRID’s Object Lock policy with Velero backups, but I couldn’t try that on the StorageGRID cluster used in this demo.</p>

<p>I would prefer to see Velero’s <a href="https://velero.io/docs/v1.8/csi/">CSI support</a> become GA so that it can be used with CSI-native snapshots in Trident CSI to cover additional use cases.</p>

<p>Users with enterprise-grade data protection requirements should still consider a commercial solution such as NetApp Astra Control or other.</p>

      </article>
    </div>
    
      <div class="categories">
    <span><p>Categories:</p>
    
    
      <a href="
      /categories/#kubernetes">kubernetes</a>
      &nbsp; 
    
      <a href="
      /categories/#storage">storage</a>
       
    
  </span>
</div>
    

    
      <div class="related" data-pagefind-ignore>

    <h4>Possibly related - use live search at the top to find other content</h4>
    
    
    
    
    
    
    
        
        
    
        
    
        
          <div class = "related-posts">
          <h5><a href="/2024/03/18/storagegrid-storage-node-filesystem-block-size.html">• Filesystem block size used by NetApp StorageGRID</a></h5>
          </div>
          
          
        
    
      
    
        
        
    
        
    
        
          <div class = "related-posts">
          <h5><a href="/2024/03/18/storagegrid-nlb-http-logs.html">• Analyze HTTP logs from NetApp StorageGRID gateway nodes</a></h5>
          </div>
          
          
        
    
      
    
        
        
    
        
    
        
          <div class = "related-posts">
          <h5><a href="/2024/03/17/monitoring-notifications-eseries-santricity-media-scan-progress.html">• Monitor progress and notify of E-Series media scan events</a></h5>
          </div>
          
          
        
    
      
    
        
        
    
        
    
        
          <div class = "related-posts">
          <h5><a href="/2024/02/29/ubuntu-2404-lts-with-netapp-solidfire.html">• Ubuntu 24.04 LTS with NetApp SolidFire</a></h5>
          </div>
          
          
        
    
      
    
        
        
    
        
    
        
          <div class = "related-posts">
          <h5><a href="/2024/02/28/incus-zfs-netapp-eseries.html">• ZFS with NetApp E-Series</a></h5>
          </div>
          
          
            
    
    </div>

    

    
  </div><footer class= "footer">
    <p>2024-03-19 03:17 </p>
    <p>Copyright © 2024 scaleoutSean. Content is released under the CC BY license. Design: Alessio Franceschi</p>
</footer>

</body>
</html>
